\documentclass{article}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{ bbold }
\usepackage{dsfont}
\usepackage{centernot}
\usepackage[a4paper,top=2.5cm,bottom=2.5cm,left=2cm,right=2cm,marginparwidth=1.75cm]{geometry}
\usepackage{parskip}


% for theorems/lemmas/defs/etc
%-------------------------------
\usepackage[english]{babel}
\usepackage[sort&compress,square,numbers]{natbib}
\bibliographystyle{unsrtnat}

\usepackage[dvipsnames]{xcolor}

\renewcommand\qedsymbol{$\blacksquare$}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\newtheorem{proposition}{Proposition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{corollary1}{Corollary}[definition]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{remark}
\newtheorem*{remark}{\textbf{Remark}}  % if don't want bold face for remark remove textbf

\theoremstyle{remark}
\newtheorem*{remarks}{\textbf{Remarks}}

%-------------------------------

\DeclareMathOperator{\E}{\mathds{E}}
\DeclareMathOperator{\Var}{\mathrm{Var}}
\DeclareMathOperator{\prob}{\mathbb{P}}

\pagestyle{fancy}
\fancyhf{}
\rhead{October 2020}
% \lhead{CID: 00955392}
\cfoot{\thepage}

\usepackage[hidelinks]{hyperref}
\hypersetup{colorlinks=false}

\begin{document}
\setlength\parskip{10pt}
\setlength\parindent{20pt}

\noindent We now focus on the following time-dependent PDE:
\begin{align}
        \mathcal{L}u(x,t) := \partial_{t}u(x,t) -\nabla\cdot(a(x)\nabla u(x,t))&=f(x,t), \hspace{0.3cm} x\in\Omega, \hspace{0.15cm} t\in[0,T] \\
        u(x,t) &= 0, \hspace{1.15cm} x\in\partial\Omega, \hspace{0.15cm} t\in[0,T] \\
        u(x,0) &= u_{0}(x), \hspace{0.5cm} x\in\Omega
\end{align}
where the function $f\sim\mathcal{N}(\bar{f},K)$. The solution $u$ is thus also random.

\noindent We will set up a prior on the solution $u$ to the above problem. To do so we first let $v_{h}\in S_{h}$ be some approximation of the initial condition $u_{0}(x)$ in the FEM space $S_{h}$. To be more specific we will assume that $v_{h}(x)=\Phi(x)^{*}\boldsymbol{\gamma}:=\sum_{i=1}^{J}\phi_{i}(x)\gamma_{i}$. Note that $\Phi(x):=(\phi_{1}(x),\dots,\phi_{J}(x))^{T}$. We take the prior on $u$ to be:
\begin{equation}
    u\sim\mathcal{N}(m_{0},V_{0})
\end{equation}
where $m_{0}(x,t):=v_{h}(x)=\Phi(x)^{*}\boldsymbol{\gamma}$ ($m_{0}$ is constant in time). The prior covariance operator $V_{0}$ is defined as:
\begin{equation}
    (V_{0}g)(x,t)=\int_{\Omega}\int_{0}^{T}k_{x,t,y,s}^{(0)}g(y,s)\mathrm{d}s\mathrm{d}y
\end{equation}
where $k_{x,t,y,s}^{(0)}$ is defined as follows:
\begin{equation}
    k_{x,t,y,s}^{(0)}:=\sum_{i=1}^{J}\lambda_{j}\phi_{j}(x)\phi_{j}(y)k^{(0)}(t,s)
\end{equation}

\noindent We now introduce a uniform time grid:
\begin{equation*}
    t_{n}=n\delta, \hspace{0.2cm} n=0,1,\dots,N
\end{equation*}
where $\delta$ is the spacing between consecutive times and $N=\frac{T}{\delta}$ (\textit{assume that} $N$ \textit{is an integer}). The time kernel $k^{(0)}(t,s)$ will be taken to be:
\begin{equation}
  \label{time_kernel_0}
  k^{(0)}(t,s) := \sum_{i=0}^{N-1}l^{(i)}(t)l^{(i)}(s)
\end{equation}

\noindent Where the functions $\{l^{(i)}\}_{i=0}^{N-1}$ are defined as follows:

\begin{equation}
  l^{(i)}(t) = \left\{\begin{array}{cc}
    (t-t_{i})\mathbb{1}_{(t_{i},t_{i+1}]}(t)+\delta\mathbb{1}_{(t_{i+1},t_{N}]}(t), & i=0,\dots,N-2 \\
    (t-t_{N-1})\mathbb{1}_{(t_{N-1},t_{N}]}, & i=N-1
  \end{array}\right.
\end{equation}
i.e. for $i=0,\dots,N-2$ we have:
\begin{equation}
  l^{(i)}(t) = \left\{\begin{array}{cc}
                        0, & t\leq t_{i} \\
                        t-t_{i}, & t_{i} < t \leq t_{i+1} \\
                        \delta, & t > t_{i+1}
                     \end{array}\right.
\end{equation}
while for for $i=N-1$ we have:
\begin{equation}
  l^{(i)}(t) = l^{(N-1)}(t) =  \left\{\begin{array}{cc}
                        0, & t\leq t_{N-1} \\
                        t-t_{i}, & t > t_{N-1}
                     \end{array}\right.
\end{equation}
\textit{note: we are working only with times in the interval} $[0,T]$ \textit{here.}

\noindent We now introduce the following information operators $\mathcal{I}_{s}:=(I_{1}(s),\dots,I_{J}(s))^{T}$ where:
\begin{equation}
    I_{i}(s)g:=\int_{\Omega}\phi_{i}(x)g(x,s)\mathrm{d}x
\end{equation}

\noindent To update our belief in the distribution of $u$ we will condition on the following events: $\mathcal{I}_{t_{i}}\mathcal{L}u=\mathcal{I}_{t_{i}}f=:F^{(i)}$ sequentially for $i=1,\dots,N$. Letting $\tilde{A}_{t}:=\mathcal{I}_{t}\mathcal{L}$ we seek, for a fixed realisation of $f$ (and hence of the $\{F^{(i)}\}$), the following conditional distributions:
\begin{equation}
    u|\{\tilde{A}_{t_{1}}u=F^{(1)},\dots,\tilde{A}_{t_{p}}u=F^{(p)},f\}\sim\mathcal{N}(m_{p},V_{p})
\end{equation}
for $p\in\{1,\dots,N\}$. We make the following claim:
\begin{proposition}
    \label{proposition_1}
    With the prior specified as above we have that $m_{p}$ and $V_{p}$ are given as follows:
    \begin{align}
        m_{p}(x,t) &:= \Phi(x)^{*}\boldsymbol{\gamma} + \sum_{i=1}^{p}l^{(i-1)}(t)\Phi(x)^{*}\boldsymbol{c}^{(i)} \\
        (V_{p}g)(x,t) &:= \int_{\Omega}\int_{0}^{T}k^{(p)}_{x,t,y,s}g(y,s)\mathrm{d}s\mathrm{d}y \\
        \boldsymbol{c}^{(i)} &:= Q^{-1}\left[F^{(i)}-A\boldsymbol{\gamma}_{i-1}\right] \text{ for } i=1,\dots,p \\
        k^{(p)}_{x,t,y,s} &:= \sum_{j=1}^{J}\lambda_{j}\phi_{j}(x)\phi_{j}(y)k^{(p)}(t,s) \\
        k^{(p)}(t,s) &:= \sum_{i=p}^{N-1}l^{(i)}(t)l^{(i)}(s)
    \end{align}
    and where the $\{\boldsymbol{\gamma}_{i}\}_{i=0}^{p}$ are defined recursively by:
    \begin{align}
        \boldsymbol{\gamma}_{0} &:= \boldsymbol{\gamma}, \\
        \boldsymbol{\gamma}_{i} &:= Q^{-1}\left[M\boldsymbol{\gamma}_{i-1}+\delta F^{(i)}\right] \text{ for } i\geq 1
    \end{align}
    The matrices $M$ and $A$ are the Galerkin Mass and Stiffness matrices respectively, i.e. $M_{ij}:=\int_{\Omega}\phi_{i}(x)\phi_{j}(x)\mathrm{d}x$ and $A_{ij}:=\int_{\Omega}a(x)\nabla\phi_{i}(x)\cdot\nabla\phi_{j}(x)\mathrm{d}x$. The matrix $Q:=(M+\delta A)$. Further, we have that evaluating the conditional mean $m_{p}$ at time $t_p$ yields the following:
    \begin{equation}
        m_{p}(x,t_p) = \Phi(x)^{*}\boldsymbol{\gamma}_{p}
    \end{equation}
    Thus, we can see that this choice of prior yields (for a fixed realisation of $f$) what the classical Backward-Euler Galerkin method yields.
\end{proposition}

\noindent \textit{Proof:} We proceed via proof by induction.

\noindent For $p=1$ it follows that the distribution of $u|\{\tilde{A}_{\delta}u=F^{(1)},f\}$ is Gaussian $\mathcal{N}(m_{1},V_{1})$ by considering the following joint distribution:
\begin{equation*}
    \left(\begin{array}{c}u \\ \tilde{A}_{\delta} u\end{array}\right)=\left(\begin{array}{c}I \\ \tilde{A}_{\delta}\end{array}\right) u \sim \mathcal{N}\left(\left(\begin{array}{c}m_{o} \\ \tilde{A}_{\delta}m_{0}\end{array}\right),\left(\begin{array}{cc}V_{0} & V_{0} \tilde{A}_{\delta}^{*} \\ \tilde{A}_{\delta}V_{0} & \tilde{A}_{\delta}V_{0} \tilde{A}_{\delta}^{*}\end{array}\right)\right)
\end{equation*}
It follows that the conditional distribution is Gaussian and the mean and covariance are given by:
\begin{align}
    \label{mean_update_1}
    m_{1}&=m_{0}+V_{0}\tilde{A}_{\delta}^{*}(\tilde{A}_{\delta}V_{0}\tilde{A}_{\delta}^{*})^{-1}(F^{(1)}-\tilde{A}_{\delta}m_{0}) \\
    \label{cov_update_1}
    V_{1}&=V_{0}-V_{0}\tilde{A}_{\delta}^{*}(\tilde{A}_{\delta}V_{0}\tilde{A}_{\delta}^{*})^{-1}\tilde{A}_{\delta}V_{0}
\end{align}
\noindent To make progress we must now start computing various terms needed for our mean and covariance update rules. We start with $V_{0}\tilde{A}_{\delta}^{*}$. We have:
\begin{equation*}
    V_{0}\tilde{A}_{\delta}^{*}=V_{0}\mathcal{L}^{*}(I_{1}(\delta)^{*},\dots,I_{J}(\delta)^{*})
\end{equation*}
We can thus see that we need to be able to compute terms of the form $V_{0}\mathcal{L}^{*}I_{i}(\delta)^{*}=V_{0}(I_{i}(\delta)\mathcal{L})^{*}$. Now since the operator $I_{i}(\delta)\mathcal{L}$ takes in a function on $\Omega\times[0,T]$ and outputs a real number its adjoint should take in a real number and output a function on $\Omega\times[0,T]$. This adjoint should satisfy the following relation:
\begin{equation}
    \alpha(I_{i}(\delta)\mathcal{L}g)=\int_{\Omega}\int_{0}^{T}((I_{i}(\delta)\mathcal{L})^{*}\alpha)(x,t)g(x,t)\mathrm{d}t\mathrm{d}x \hspace{0.15cm} \forall{g}, \hspace{0.15cm} \forall\alpha\in\mathbb{R}
\end{equation}
Using this we can now compute:
\begin{align*}
    (V_{0}(I_{i}(\delta)\mathcal{L})^{*}\alpha)(x,s) &= \int_{\Omega}\int_{0}^{T}k_{x,s,y,w}^{(0)}((I_{i}(\delta)\mathcal{L})^{*}\alpha)(y,w)\mathrm{d}w\mathrm{d}y \\
    &=\alpha (I_{i}(\delta)\mathcal{L}k_{x,s,\boldsymbol{\cdot},\boldsymbol{\cdot}}^{(0)}) \\
    &=\alpha\int_{\Omega}\phi_{i}(y)(\mathcal{L}k_{x,s,\boldsymbol{\cdot},\boldsymbol{\cdot}}^{(0)})(y,\delta)\mathrm{d}y
\end{align*}

\noindent We now work out $(\mathcal{L}k_{x,s,\boldsymbol{\cdot},\boldsymbol{\cdot}}^{(0)})(y,\delta)$ taking care to remember that $x,s$ are fixed and so $\mathcal{L}$ acts on the variables $y,\delta$:
\begin{align*}
    (\mathcal{L}k_{x,s,\boldsymbol{\cdot},\boldsymbol{\cdot}}^{(0)})(y,\delta) &= \partial_{2}k^{(0)}(s,\delta)\sum_{j=1}^{J}\lambda_{j}\phi_{j}(x)\phi_{j}(y)-k^{(0)}(s,\delta)\nabla_{y}\cdot\left(a(y)\nabla_{y}\sum_{j=1}^{J}(\lambda_{j}\phi_{j}(x)\phi_{j}(y)\right) \\
    &=\partial_{2}k^{(0)}(s,\delta)\sum_{j=1}^{J}\lambda_{j}\phi_{j}(x)\phi_{j}(y)-k^{(0)}(s,\delta)\sum_{j=1}^{J}\lambda_{j}\phi_{j}(x)\nabla_{y}\cdot\left(a(y)\nabla_{y}\phi_{j}(y)\right)
\end{align*}
So we can now compute:
\begin{align*}
    (V_{0}(I_{i}(\delta)\mathcal{L})^{*}\alpha)(x,s) &= \alpha\int_{\Omega}\phi_{i}(y)\partial_{2}k^{(0)}(s,\delta)\sum_{j=1}^{J}\lambda_{j}\phi_{j}(x)\phi_{j}(y)\mathrm{d}y-\alpha\int_{\Omega}\phi_{i}(y)k^{(0)}(s,\delta)\sum_{j=1}^{J}\lambda_{j}\phi_{j}(x)\nabla_{y}\cdot\left(a(y)\nabla_{y}\phi_{j}(y)\right)\mathrm{d}y \\
    &=\alpha\partial_{2}k^{(0)}(s,\delta)\sum_{j=1}^{J}\lambda_{j}\phi_{j}(x)M_{ji}+\alpha k^{(0)}(s,\delta)\sum_{j=1}^{J}\lambda_{j}\phi_{j}(x)A_{ji}
\end{align*}

\noindent Using this result we can deduce that:
\begin{equation}
    \label{vector_formula_1}
    (V_{0}\tilde{A}_{\delta}^{*}\boldsymbol{v})(x,s) = \partial_{2}k^{(0)}(s,\delta)\Phi(x)^{*}\Lambda M\boldsymbol{v} + k^{(0)}(s,\delta)\Phi(x)^{*}\Lambda A \boldsymbol{v}
\end{equation}
for any $\boldsymbol{v}\in\mathbb{R}^{J}$, where $\Lambda=\operatorname{diag}\{\lambda_{i}\}_{i=1}^{J}$.

\noindent For our time kernel we can deduce:
\begin{align}
    \partial_{1}k^{(0)}(t,s) &= \sum_{i=0}^{N-1}l^{(i)\prime}(t)l^{(i)}(s) \\
    \partial_{2}k^{(0)}(t,s) &= \sum_{i=0}^{N-1}l^{(i)}(t)l^{(i)\prime}(s) \\
    \partial_{1}\partial_{2}k^{(0)}(t,s) &= \sum_{i=0}^{N-1}l^{(i)\prime}(t)l^{(i)\prime}(s) \\
\end{align}

\noindent We also have:
\begin{equation}
    l^{(i)\prime}(t)=\mathbb{1}_{(t_{i},t_{i+1}]}(t)
\end{equation}
for all $i=0,\dots,N-1$. (\textit{Note: this includes even the case of } $i=N-1$.) Noting that the kernel $k^{(0)}$ is symmetric we have:
\begin{align*}
    k^{(0)}(s,\delta) &= k^{(0)}(\delta,s) \\
    &= \sum_{i=0}^{N-1}l^{(i)}(\delta)l^{(i)}(s) \\
    &= \sum_{i=0}^{N-1}\delta \delta_{i,0}l^{(i)}(s) \\
    &= \delta l^{(0)}(s)
\end{align*}
and
\begin{align*}
    \partial_{2}k^{(0)}(s,\delta) &= \partial_{1}k^{(0)}(\delta,s) \\
    &=\sum_{i=0}^{N-1}l^{(i)\prime}(\delta)l^{(i)}(s) \\
    &=\sum_{i=0}^{N-1}\delta_{i,0}l^{(i)}(s) \\
    &=l^{(0)}(s)
\end{align*}
where we have used the following properties of the functions $\{l^{(i)}\}$ which can easily be shown:
\begin{align}
    \label{helpful_identity_1}
    l^{(i)}(t_j) &= \delta\cdot\delta_{i,j-1} \\
    \label{helpful_identity_2}
    l^{(i)\prime}(t_j) &= \delta_{i,j-1}
\end{align}
for $i=0,\dots,N-1$ and $j=1,\dots,N$.

\noindent We can now simplify (\ref{vector_formula_1}) to:
\begin{align}
    (V_{0}\tilde{A}_{\delta}^{*}\boldsymbol{v})(x,s) &= l^{(0)}(s)\Phi(x)^{*}\Lambda M\boldsymbol{v} + \delta l^{(0)}(s)\Phi(x)^{*}\Lambda A \boldsymbol{v} \nonumber \\
    &= l^{(0)}(s)\Phi(x)^{*}\Lambda(M+\delta A)\boldsymbol{v} \nonumber \\
    \label{vector_formula_simplified_1}
    &= l^{(0)}(s)\Phi(x)^{*}\Lambda Q\boldsymbol{v}
\end{align}

\noindent We now move onto computing:
\begin{align*}
    \tilde{A}_{\delta}V_{0}\tilde{A}_{\delta}^{*}&=\mathcal{I}_{\delta}\mathcal{L}V_{0}\mathcal{L}^{*}\mathcal{I}_{\delta}^{*} \\
    &=\begin{pmatrix}
        I_{1}(\delta) \\
        \vdots \\
        I_{J}(\delta)
    \end{pmatrix}\mathcal{L}V_{0}\mathcal{L}^{*}\begin{pmatrix}
                                                    I_{1}(\delta)^{*} & \dots & I_{J}(\delta)^{*}
                                                \end{pmatrix}
\end{align*}
This operator has $ij$\textit{-th} entry which is given by:
\begin{align*}
    (\tilde{A}_{\delta}V_{0}\tilde{A}_{\delta}^{*})_{ij}\alpha &= I_{i}(\delta)\mathcal{L}V_{0}\mathcal{L}^{*}I_{j}(\delta)^{*}\alpha \\
    &= \int_{\Omega}\phi_{i}(x)\left[(\mathcal{L}V_{0}(I_{j}(\delta)\mathcal{L})^{*}\alpha)(x,\delta)\right]\mathrm{d}x \\
    &=\int_{\Omega}\phi_{i}(x)\bigg[\alpha\partial_{1}\partial_{2}k^{(0)}(\delta,\delta)\sum_{l=1}^{J}\lambda_{l}\phi_{l}(x)M_{lj}+\alpha\partial_{1}k^{(0)}(\delta,\delta)\sum_{l=1}^{J}\lambda_{l}\phi_{l}(x)A_{lj} \\
    &-\alpha\partial_{2}k^{(0)}(\delta,\delta)\sum_{l=1}^{J}\lambda_{l}M_{lj}\nabla\cdot(a(x)\nabla\phi_{l}(x)) - \alpha k^{(0)}(\delta,\delta)\sum_{l=1}^{J}\lambda_{l}A_{lj}\nabla\cdot(a(x)\nabla\phi_{l}(x))\bigg]\mathrm{d}x \\
    &=\alpha\partial_{1}\partial_{2}k^{(0)}(\delta,\delta)\sum_{l=1}^{J}\lambda_{l}M_{il}M_{lj} + \alpha\partial_{1}k^{(0)}(\delta,\delta)\sum_{l=1}^{J}\lambda_{l}M_{il}A_{lj} \\
    &+\alpha\partial_{2}k^{(0)}(\delta,\delta)\sum_{l=1}^{J}\lambda_{l}M_{lj}A_{il} + \alpha k^{(0)}(\delta,\delta)\sum_{l=1}^{J}\lambda_{l}A_{il}A_{lj} \\
    &= \alpha\partial_{1}\partial_{2}k^{(0)}(\delta,\delta)(M\Lambda M)_{ij} + \alpha\partial_{1}k^{(0)}(\delta,\delta)(M\Lambda A)_{ij} + \alpha\partial_{2}k^{(0)}(\delta,\delta)(A\Lambda M)_{ij} + \alpha k^{(0)}(\delta,\delta)(A\Lambda A)_{ij}
\end{align*}
We can thus conclude that $\tilde{A}_{\delta}V_{0}\tilde{A}_{\delta}^{*}$ is the $J\times J$ matrix given by:
\begin{equation}
    \label{cross_term_1}
    \tilde{A}_{\delta}V_{0}\tilde{A}_{\delta}^{*}=\partial_{1}\partial_{2}k^{(0)}(\delta,\delta)M\Lambda M + \partial_{1}k^{(0)}(\delta,\delta)M\Lambda A + \partial_{2}k^{(0)}(\delta,\delta)A\Lambda M +  k^{(0)}(\delta,\delta)A\Lambda A
\end{equation}

\noindent Using our time kernel and its derivatives we can easily deduce:
\begin{align*}
    k^{(0)}(\delta,\delta) &= \delta^2 \\
    \partial_{1}k^{(0)}(\delta,\delta) &= \delta \\
    \partial_{2}k^{(0)}(\delta,\delta) &= \delta \\
    \partial_{1}\partial_{2}k^{(0)}(\delta,\delta) &= 1 \\
\end{align*}
And thus:
\begin{align*}
    \tilde{A}_{\delta}V_{0}\tilde{A}_{\delta}^{*} &= M\Lambda M + \delta M\Lambda A + \delta A\Lambda M +  \delta^{2}A\Lambda A \\
    &= M\Lambda(M+\delta A)+\delta A \Lambda(M+\delta A) \\
    &= (M\Lambda+\delta A \Lambda)(M+\delta A) \\
    &= (M+\delta A)\Lambda(M+\delta A) = Q\Lambda Q
\end{align*}

\noindent We can now make progress with the mean update equation. We first work out the following term using (\ref{vector_formula_simplified_1}):
\begin{align*}
    (V_{0}\tilde{A}_{\delta}^{*}(\tilde{A}_{\delta}V_{0}\tilde{A}_{\delta}^{*})^{-1}F^{(1)})(x,s) &= l^{(0)}(s)\Phi(x)^{*}\Lambda Q (Q\Lambda Q)^{-1}F^{(1)} \\
    &=l^{(0)}(s)\Phi(x)^{*}\Lambda Q Q^{-1}\Lambda^{-1}Q^{-1}F^{(1)} \\
    &=l^{(0)}(s)\Phi(x)^{*}Q^{-1}F^{(1)}
\end{align*}

\noindent For the other term involving $m_0$ in the mean update equation we must first work out $\tilde{A}_{\delta}m_{0}=\mathcal{I}_{\delta}\mathcal{L}m_{0}$. To do this we compute:
\begin{align*}
    (\mathcal{L}m_{0})(x,t)&=\partial_{t}m_{0}-\nabla\cdot(a(x)\nabla m_{0}(x,t)) \\
    &=-\nabla(a(x)\nabla\Phi(x)^{*}\boldsymbol{\gamma}) \\
    &=-\sum_{j=1}^{J}\gamma_{j}\nabla\cdot(a(x)\nabla\phi_{j}(x))
\end{align*}
Thus, the $i$\textit{-th} entry of $\tilde{A}_{\delta}m_{0}$ can be computed as:
\begin{align*}
    (\tilde{A}_{\delta}m_{0})_{i} &= I_{i}(\delta)\mathcal{L}m_{0} \\
    &= \int_{\Omega}\phi_{i}(x)\left(-\sum_{j=1}^{J}\gamma_{j}\nabla\cdot(a(x)\nabla\phi_{j}(x))\right)\mathrm{d}x \\
    &= \sum_{j=1}^{J}\gamma_{j}A_{ij} = (A\boldsymbol{\gamma})_{i}
\end{align*}
So $\tilde{A}_{\delta}m_{0}=A\boldsymbol{\gamma}$.
We can now compute the second term involving $m_{0}$ as follows (again using (\ref{vector_formula_simplified_1})):
\begin{align*}
    ((V_{0}\tilde{A}_{\delta}^{*}(\tilde{A}_{\delta}V_{0}\tilde{A}_{\delta}^{*})^{-1}\tilde{A}_{\delta})m_{0})(x,s) &= (V_{0}\tilde{A}_{\delta}^{*}(Q\Lambda Q)^{-1}A\boldsymbol{\gamma})(x,s) \\
    &=l^{(0)}(s)\Phi(x)^{*}\Lambda QQ^{-1}\Lambda^{-1}Q^{-1}A\boldsymbol{\gamma}\\
    &=l^{(0)}(s)\Phi(x)^{*}Q^{-1}A\boldsymbol{\gamma}
\end{align*}
Using (\ref{mean_update_1}) we can now compute:
\begin{align*}
    m_1(x,s) &= \Phi(x)^{*}\boldsymbol{\gamma}+l^{(0)}(s)\Phi(x)^{*}Q^{-1}F^{(1)}-l^{(0)}(s)\Phi(x)^{*}Q^{-1}A\boldsymbol{\gamma} \\
    &=\Phi(x)^{*}\boldsymbol{\gamma}+l^{(0)}(s)\Phi(x)^{*}Q^{-1}\left[F^{(1)}-A\boldsymbol{\gamma}\right] \\
    &=\Phi(x)^{*}\boldsymbol{\gamma}+l^{(0)}(s)\Phi(x)^{*}\boldsymbol{c}^{(1)}
\end{align*}
From this we can note:
\begin{align*}
    m_{1}(x,t_{1}) &= \Phi(x)^{*}\boldsymbol{\gamma}+\delta\Phi(x)^{*}\boldsymbol{c}^{(1)} \\
    &= \Phi(x)^{*}\boldsymbol{\gamma}+\delta\Phi(x)^{*}Q^{-1}\left[F^{(1)}-A\boldsymbol{\gamma}\right] \\
    &= \Phi(x)^{*}Q^{-1}\left[Q\boldsymbol{\gamma}+\delta F^{(1)}-\delta A\boldsymbol{\gamma}\right] \\
    &= \Phi(x)^{*}Q^{-1}\left[M\boldsymbol{\gamma}+\delta F^{(1)}\right] \\
    &= \Phi(x)^{*}\boldsymbol{\gamma}_{1}
\end{align*}

\noindent We now move on to computing the covariance $V_1$. To do this we must first work out $\tilde{A}_{\delta}V_{0}$. Computing this involves determining how $I_{j}(\delta)\mathcal{L}V_{0}$ acts on functions $g$ for $j=1,\dots,J$. We have:
\begin{equation*}
    I_{j}(\delta)\mathcal{L}V_{0}g=\int_{\Omega}\phi_{j}(x)(\mathcal{L}V_{0}g)(x,\delta)\mathrm{d}x
\end{equation*}

\noindent Now recalling that $V_{0}g(x,\delta)=\int_{\Omega}\int_{0}^{T}k_{x,\delta,y,s}^{(0)}g(y,s)\mathrm{d}s\mathrm{d}y$ we deduce:
\begin{align*}
    (\mathcal{L}V_{0}g)(x,\delta) &= \int_{\Omega}\int_{0}^{T}(\mathcal{L}k_{\boldsymbol{\cdot},\boldsymbol{\cdot},y,s}^{(0)})(x,\delta)g(y,s)\mathrm{d}s\mathrm{d}y \\
    &= \int_{\Omega}\int_{0}^{T}\left(\partial_{1}k^{(0)}(\delta,s)\sum_{i=1}^{J}\lambda_{i}\phi_{i}(x)\phi_{i}(y)-k^{(0)}(\delta,s)\sum_{i=1}^{J}\lambda_{i}\nabla_{x}\cdot(a(x)\nabla_{x}\phi_{i}(x))\phi_{i}(y)\right)g(y,s)\mathrm{d}s\mathrm{d}y
\end{align*}

\noindent We can now perform the integration to obtain:
\begin{align*}
    I_{j}(\delta)\mathcal{L}V_{0}g &= \int_{\Omega}\phi_{j}(x)\left(\int_{\Omega}\int_{0}^{T}\left(\partial_{1}k^{(0)}(\delta,s)\sum_{i=1}^{J}\lambda_{i}\phi_{i}(x)\phi_{i}(y)-k^{(0)}(\delta,s)\sum_{i=1}^{J}\lambda_{i}\nabla_{x}\cdot(a(x)\nabla_{x}\phi_{i}(x))\phi_{i}(y)\right)g(y,s)\mathrm{d}s\mathrm{d}y\right)\mathrm{d}x \\
    &= \int_{\Omega}\int_{0}^{T}\left(\partial_{1}k^{(0)}(\delta,s)\sum_{i=1}^{J}\lambda_{i}M_{ij}\phi_{i}(y)g(y,s)+k^{(0)}(\delta,s)\sum_{i=1}^{J}\lambda_{i}A_{ij}\phi_{i}(y)g(y,s)\right)\mathrm{d}s\mathrm{d}y \\
    &= \sum_{i=1}^{J}\lambda_{i}\int_{\Omega}\int_{0}^{T}(\partial_{1}k^{(0)}(\delta,s)M_{ij}+k^{(0)}(\delta,s)A_{ij})\phi_{i}(y)g(y,s)\mathrm{d}s\mathrm{d}y \\
    &= \sum_{i=1}^{J}\lambda_{i}\left[\int_{0}^{T}M_{ij}\partial_{1}k^{(0)}(\delta,s)(I_{i}(s)g)\mathrm{d}s+\int_{0}^{T}A_{ij}k^{(0)}(\delta,s)(I_{i}(s)g)\mathrm{d}s\right] \\
    &= \sum_{i=1}^{J}\lambda_{i}\left[M_{ij}\left(\int_{0}^{T}\partial_{1}k^{(0)}(\delta,s)(\mathcal{I}_{s}g)\mathrm{d}s\right)_{i}+A_{ij}\left(\int_{0}^{T}k^{(0)}(\delta,s)(\mathcal{I}_{s}g)\mathrm{d}s\right)_{i}\right] \\
    &= \left(M\Lambda\int_{0}^{T}\partial_{1}k^{(0)}(\delta,s)(\mathcal{I}_{s}g)\mathrm{d}s+A\Lambda\int_{0}^{T}k^{(0)}(\delta,s)(\mathcal{I}_{s}g)\mathrm{d}s\right)_{j}
\end{align*}

\noindent Thus we can deduce:
\begin{equation}
    \tilde{A}_{\delta}V_{0}g=M\Lambda\int_{0}^{T}\partial_{1}k^{(0)}(\delta,s)(\mathcal{I}_{s}g)\mathrm{d}s + A\Lambda\int_{0}^{T}k^{(0)}(\delta,s)(\mathcal{I}_{s}g)\mathrm{d}s
\end{equation}

\noindent We now utilise the specific form of the time kernel to simplify this to:
\begin{align}
    \tilde{A}_{\delta}V_{0}g &= M\Lambda\int_{0}^{T}l^{(0)}(s)(\mathcal{I}_{s}g)\mathrm{d}s + \delta A\Lambda\int_{0}^{T}l^{(0)}(s)(\mathcal{I}_{s}g)\mathrm{d}s \\
    &= (M+\delta A)\Lambda\int_{0}^{T}l^{(0)}(s)(\mathcal{I}_{s}g)\mathrm{d}s \\
    &= Q\Lambda\boldsymbol{\nu}_{g}^{(0)}
\end{align}
where $\boldsymbol{\nu}_{g}^{(i)}:=\int_{0}^{T}l^{(i)}(s)(\mathcal{I}_{s}g)\mathrm{d}s$ for $i=0,\dots,N-1$.

\noindent We can use this to now compute:
\begin{align*}
    (V_{0}\tilde{A}_{\delta}^{*}(\tilde{A}_{\delta}V_{0}\tilde{A}_{\delta}^{*})^{-1}\tilde{A}_{\delta}V_{0}g)(x,t) &=
    (V_{0}\tilde{A}_{\delta}^{*}(Q\Lambda Q)^{-1}Q\Lambda\boldsymbol{\nu}_{g}^{(0)})(x,t) \\
    &=l^{(0)}(t)\Phi(x)^{*}\Lambda Q Q^{-1}\Lambda^{-1}Q^{-1}Q\Lambda\boldsymbol{\nu}_{g}^{(0)} \\
    &=l^{(0)}(t)\Phi(x)^{*}\Lambda\boldsymbol{\nu}_{g}^{(0)}
\end{align*}

\noindent One can also easily show that the action of $V_{0}$ on functions can be rewritten as follows:
\begin{equation}
    (V_{0}g)(x,t) = \sum_{i=0}^{N-1}l^{(i)}(t)\Phi(x)^{*}\Lambda\boldsymbol{\nu}_{g}^{(i)}
\end{equation}
Thus, we can conclude using (\ref{cov_update_1}) that:
\begin{align}
    (V_{1}g)(x,t) &= \sum_{i=1}^{N-1}l^{(i)}(t)\Phi(x)^{*}\Lambda\boldsymbol{\nu}_{g}^{(i)} \nonumber \\
    &=\int_{\Omega}\int_{0}^{T}k^{(1)}_{x,t,y,s}g(y,s)\mathrm{d}s\mathrm{d}y
\end{align}
where:
\begin{align}
    k^{(1)}_{x,t,y,s} &:= \sum_{j=1}^{J}\lambda_{j}\phi_{j}(x)\phi_{j}(y)k^{(1)}(t,s) \\
    k^{(1)}(t,s) &:= \sum_{i=1}^{N-1}l^{(i)}(t)l^{(i)}(s)
\end{align}
We thus see that the result holds for $p=1$. We now proceed with the inductive step. Assume the result holds for some $p<N$. As we did in the proof of the case $p=1$ we can easily deduce that
\begin{equation}
    u|\{\tilde{A}_{t_1}u=F^{(1)},\dots,\tilde{A}_{t_{p+1}}u=F^{(p+1)},f\}\sim\mathcal{N}(m_{p+1},V_{p+1})
\end{equation}
where we have the update equations:
\begin{align}
    \label{mean_update_p}
    m_{p+1}&=m_{p}+V_{p}\tilde{A}_{t_{p+1}}^{*}(\tilde{A}_{t_{p+1}}V_{p}\tilde{A}_{t_{p+1}}^{*})^{-1}(F^{(p+1)}-\tilde{A}_{t_{p+1}}m_{p}) \\
    \label{cov_update_p}
    V_{p+1}&=V_{p}-V_{p}\tilde{A}_{t_{p+1}}^{*}(\tilde{A}_{t_{p+1}}V_{p}\tilde{A}_{t_{p+1}}^{*})^{-1}\tilde{A}_{t_{p+1}}V_{p}
\end{align}
From the time kernel $k^{(p)}(t,s)$ we can work out:
\begin{align}
    \partial_{1}k^{(p)}(t,s) &= \sum_{i=p}^{N-1}l^{(i)\prime}(t)l^{(i)}(s) \\
    \partial_{2}k^{(p)}(t,s) &= \sum_{i=p}^{N-1}l^{(i)}(t)l^{(i)\prime}(s) \\
    \partial_{1}\partial_{2}k^{(p)}(t,s) &= \sum_{i=p}^{N-1}l^{(i)\prime}(t)l^{(i)\prime}(s) \\
\end{align}
Using (\ref{helpful_identity_1}) and (\ref{helpful_identity_2}) we have
\begin{align}
    k^{(p)}(s,t_{p+1}) &= \delta l^{(p)}(s) \\
    \partial_{2}k^{(p)}(s,t_{p+1}) &= l^{(p)}(s)
\end{align}
These results will help with figuring out the analogue of (\ref{vector_formula_simplified_1}). Since $V_p$ is of the same form as $V_0$ we can deduce:
\begin{align}
    (V_{p}\tilde{A}_{t_{p+1}}^{*}\boldsymbol{v})(x,s) &= \partial_{2}k^{(p)}(s,t_{p+1})\Phi(x)^{*}\Lambda M\boldsymbol{v} + k^{(p)}(s,t_{p+1})\Phi(x)^{*}\Lambda A\boldsymbol{v} \\
    &= l^{(p)}(s)\Phi(x)^{*}\Lambda M\boldsymbol{v} + \delta l^{(p)}(s)\Phi(x)^{*}\Lambda A \boldsymbol{v} \\
    \label{vector_formula_simplified_p}
    &=l^{(p)}(s)\Phi(x)^{*}\Lambda Q\boldsymbol{v}
\end{align}
Using (\ref{helpful_identity_1}) and (\ref{helpful_identity_2}) we can also conclude the following:
\begin{align*}
    k^{(p)}(t_{p+1},t_{p+1}) &= \delta^2 \\
    \partial_{1}k^{(p)}(t_{p+1},t_{p+1}) &= \delta \\
    \partial_{2}k^{(p)}(t_{p+1},t_{p+1}) &= \delta \\
    \partial_{1}\partial_{2}k^{(p)}(t_{p+1},t_{p+1}) &= 1 \\
\end{align*}
Thus, just as in the proof for $p=1$ we have:
\begin{align*}
    \tilde{A}_{t_{p+1}}V_{p}\tilde{A}_{t_{p+1}}^{*}&=\partial_{1}\partial_{2}k^{(p)}(t_{p+1},t_{p+1})M\Lambda M + \partial_{1}k^{(p)}(t_{p+1},t_{p+1})M\Lambda A + \partial_{2}k^{(p)}(t_{p+1},t_{p+1})A\Lambda M +  k^{(p)}(\delta,\delta)A\Lambda A \nonumber \\
    &= Q\Lambda Q
\end{align*}
We can now compute:
\begin{align*}
    \left(V_{p}\tilde{A}_{t_{p+1}}^{*}(\tilde{A}_{t_{p+1}}V_{p}\tilde{A}_{t_{p+1}}^{*})^{-1}F^{(p+1)}\right)(x,s) &= l^{(p)}(s)\Phi(x)^{*}\Lambda Q (Q\Lambda Q)^{-1}F^{(p+1)} \\
    &= l^{(p)}(s)\Phi(x)^{*}\Lambda QQ^{-1}\Lambda^{-1}Q^{-1}F^{(p+1)} \\
    &= l^{(p)}(s)\Phi(x)^{*}Q^{-1}F^{(p+1)}
\end{align*}
In order to finish with the mean update we must now compute $\tilde{A}_{t_{p+1}}m_{p}=\mathcal{I}_{t_{p+1}}\mathcal{L}m_{p}$. Recalling that
\begin{equation*}
    m_{p}(x,s)=\Phi(x)^{*}\boldsymbol{\gamma}+\sum_{i=1}^{p}l^{(i-1)}(s)\Phi(x)^{*}\boldsymbol{c}^{(i)}
\end{equation*}
we can work out:
\begin{align*}
    (\mathcal{L}m_{p})(x,s) &= \sum_{i=1}^{p}l^{(i-1)\prime}(s)\Phi(x)^{*}\boldsymbol{c}^{(i)} - \sum_{j=1}^{J}\left(\boldsymbol{\gamma}+\sum_{i=1}^{p}l^{(i-1)}(s)\boldsymbol{c}^{(i)}\right)_{j}\nabla\cdot(a(x)\nabla\phi_{j}(x))
\end{align*}
Using this, together with (\ref{helpful_identity_1}) and (\ref{helpful_identity_2}), we can work out:
\begin{align*}
    (\tilde{A}_{t_{p+1}}m_{p})_{k} &= I_{k}(t_{p+1})\mathcal{L}m_{p} \\
    &= \int_{\Omega}\phi_{k}(x)(\mathcal{L}m_{p})(x,t_{p+1})\mathrm{d}x \\
    &=-\sum_{j=1}^{J}\left(\boldsymbol{\gamma}+\sum_{i=1}^{p}\delta\boldsymbol{c}^{(i)}\right)_{j}\int_{\Omega}\phi_{k}(x)\nabla\cdot(a(x)\nabla\phi_{j}(x))\mathrm{d}x \\
    &=\sum_{j=1}^{J}\left(\boldsymbol{\gamma}+\sum_{i=1}^{p}\delta\boldsymbol{c}^{(i)}\right)_{j}A_{kj} \\
    &= \left[A\left(\boldsymbol{\gamma}+\sum_{i=1}^{p}\delta\boldsymbol{c}^{(i)}\right)\right]_{k}
\end{align*}
i.e. we have:
\begin{equation}
    \tilde{A}_{t_{p+1}}m_{p}=A\left(\boldsymbol{\gamma}+\sum_{i=1}^{p}\delta\boldsymbol{c}^{(i)}\right)
\end{equation}
We now claim that $\boldsymbol{\gamma}+\sum_{i=1}^{p}\delta\boldsymbol{c}^{(i)}=\boldsymbol{\gamma}_{p}$. This can be proven by induction. It is true for $p=1$ (see proof that $m_1(x,t_1)=\Phi(x)^{*}\boldsymbol{\gamma}_1$). Assume it holds for $p$. For $p+1$ we have:
\begin{align*}
    \boldsymbol{\gamma}+\sum_{i=1}^{p+1}\delta\boldsymbol{c}^{(i)} &= \boldsymbol{\gamma}+\sum_{i=1}^{p}\delta\boldsymbol{c}^{(i)} + \delta\boldsymbol{c}^{(p+1)} \\
    &= \boldsymbol{\gamma}_{p}+\delta\boldsymbol{c}^{(p+1)} \\
    &= \boldsymbol{\gamma}_{p}+\delta Q^{-1}\left[F^{(p+1)}-A\boldsymbol{\gamma}_{p}\right] \\
    &= Q^{-1}\left[\left(Q-\delta A\right)\boldsymbol{\gamma}_{p}+\delta F^{(p+1)}\right] \\
    &= Q^{-1}\left[M\boldsymbol{\gamma}_p+\delta F^{(p+1)}\right] \\
    &= \boldsymbol{\gamma}_{p+1}
\end{align*}
as claimed. Thus, $\tilde{A}_{t_{p+1}}m_{p}=A\boldsymbol{\gamma}_{p}$ and we can now compute:
\begin{align*}
    \left(V_{p}\tilde{A}_{t_{p+1}}^{*}(\tilde{A}_{t_{p+1}}V_{p}\tilde{A}_{t_{p+1}}^{*})^{-1}\tilde{A}_{t_{p+1}}m_p\right)(x,s) &= l^{(p)}(s)\Phi(x)^{*}\Lambda Q (Q\Lambda Q)^{-1}A\boldsymbol{\gamma}_{p} \\
    &= l^{(p)}(s)\Phi(x)^{*}\Lambda QQ^{-1}\Lambda^{-1}Q^{-1}A\boldsymbol{\gamma}_{p} \\
    &= l^{(p)}(s)\Phi(x)^{*}Q^{-1}A\boldsymbol{\gamma}_{p}
\end{align*}
It now follows from (\ref{mean_update_p}) that we have:
\begin{align*}
    m_{p+1}(x,s) &= \Phi(x)^{*}\boldsymbol{\gamma}+\sum_{i=1}^{p}l^{(i-1)}(s)\Phi(x)^{*}\boldsymbol{c}^{(i)} + l^{(p)}(s)\Phi(x)^{*}Q^{-1}F^{(p+1)}-l^{(p)}(s)\Phi(x)^{*}Q^{-1}A\boldsymbol{\gamma}_{p} \\
    &= \Phi(x)^{*}\boldsymbol{\gamma}+\sum_{i=1}^{p}l^{(i-1)}(s)\Phi(x)^{*}\boldsymbol{c}^{(i)} + l^{(p)}(s)\Phi(x)^{*}Q^{-1}\left[F^{(p+1)}-A\boldsymbol{\gamma}_{p}\right] \\
    &= \Phi(x)^{*}\boldsymbol{\gamma}+\sum_{i=1}^{p}l^{(i-1)}(s)\Phi(x)^{*}\boldsymbol{c}^{(i)} + l^{(p)}(s)\Phi(x)^{*}\boldsymbol{c}^{(p+1)} \\
    &= \Phi(x)^{*}\boldsymbol{\gamma}+\sum_{i=1}^{p+1}l^{(i-1)}(s)\Phi(x)^{*}\boldsymbol{c}^{(i)}
\end{align*}

\noindent Having obtained this formula we can now evaluate the mean at $s=t_{p+1}$ to deduce:
\begin{equation*}
    m_{p+1}(x,t_{p+1}) = \Phi(x)^{*}\left[\boldsymbol{\gamma}+\sum_{i=1}^{p+1}\delta\boldsymbol{c^{(i)}}\right] = \Phi(x)^{*}\boldsymbol{\gamma}_{p+1}
\end{equation*}
by the above claim.

\noindent We now move on to the covariance update (\ref{cov_update_p}). The piece missing before we can proceed is the computation of $\tilde{A}_{t_{p+1}}V_{p}$. Following the proof of the case $p=1$ we can write:
\begin{align}
    \tilde{A}_{t_{p+1}}V_{p}g &= M\Lambda\int_{0}^{T}\partial_{1}k^{(p)}(t_{p+1},s)(\mathcal{I}_{s}g)\mathrm{d}s + A\Lambda\int_{0}^{T}k^{(p)}(t_{p+1},s)(\mathcal{I}_{s}g)\mathrm{d}s \\
    &= M\Lambda\int_{0}^{T}l^{(p)}(s)(\mathcal{I}_{s}g)\mathrm{d}s + \delta A\Lambda\int_{0}^{T}l^{(p)}(s)(\mathcal{I}_{s}g)\mathrm{d}s \\
    &= (M+\delta A)\Lambda\int_{0}^{T}l^{(p)}(s)(\mathcal{I}_{s}g)\mathrm{d}s \\
    &= Q\Lambda\boldsymbol{\nu}_{g}^{(p)}
\end{align}
And thus, we can compute:
\begin{align*}
    (V_{p}\tilde{A}_{t_{p+1}}^{*}(\tilde{A}_{t_{p+1}}V_{p}\tilde{A}_{t_{p+1}}^{*})^{-1}\tilde{A}_{t_{p+1}}V_{p}g)(x,t) &=
    (V_{p}\tilde{A}_{t_{p+1}}^{*}(Q\Lambda Q)^{-1}Q\Lambda\boldsymbol{\nu}_{g}^{(p)})(x,t) \\
    &=l^{(p)}(t)\Phi(x)^{*}\Lambda Q Q^{-1}\Lambda^{-1}Q^{-1}Q\Lambda\boldsymbol{\nu}_{g}^{(p)} \\
    &=l^{(p)}(t)\Phi(x)^{*}\Lambda\boldsymbol{\nu}_{g}^{(p)}
\end{align*}
Thus, using (\ref{cov_update_p}) we can deduce:
\begin{align*}
    (V_{p+1}g)(x,t) &= \sum_{i=p+1}^{N-1}l^{(i)}(t)\Phi(x)^{*}\Lambda\boldsymbol{\nu}_{g}^{(i)} \\
    &= \int_{\Omega}\int_{0}^{T}k^{(p+1)}_{x,t,y,s}g(y,s)\mathrm{d}s\mathrm{d}y
\end{align*}
where:
\begin{align*}
    k^{(p+1)}_{x,t,y,s} &:= \sum_{j=1}^{J}\lambda_{j}\phi_{j}(x)\phi_{j}(y)k^{(p+1)}(t,s) \\
    k^{(p+1)}(t,s) &:= \sum_{i=p+1}^{N-1}l^{(i)}(t)l^{(i)}(s)
\end{align*}
Thus, the result is true for $p+1$. So by induction it is true for $p\in\{1,\dots,N\}$. \qedsymbol

\noindent Proposition \ref{proposition_1} implies that conditioning at all $N$ time points will yield the following degenerate Gaussian distribution:
\begin{equation}
    u|\{\tilde{A}_{t_1}u=F^{(1)},\dots,\tilde{A}_{t_N}u=F^{(N)},f\}\sim\mathcal{N}(m_{N},0)=\delta_{m_N}
\end{equation}
since $V_N=0$. This is a Dirac point mass located at the function:
\begin{equation}
    m_{N}(x,t)=\Phi(x)^{*}\boldsymbol{\gamma}+\sum_{i=1}^{N}l^{(i-1)}(t)\Phi(x)^{*}\boldsymbol{c}^{(i)}
\end{equation}
We now want to perform the marginalisation over the RHS noise term $f$. In order to do this it will help to rewrite $m_N$ as $m_N=Lf+c$ where $L$ is a bounded linear operator acting on $f$ and $c$ is a determinstic function (not depending on $f$). In order to rewrite $m_N$ in this form it will be useful to first prove the following result:
\begin{lemma}
    \label{lemma_1}
    The vectors $\{\boldsymbol{\gamma}_{i}\}$ satisfy the following:
    \begin{equation}
        \label{f-dependenceOfgammas}
        \boldsymbol{\gamma}_{i}=(Q^{-1}M)^{i}\boldsymbol{\gamma}+\delta Q^{-1}\sum_{j=1}^{i}(MQ^{-1})^{i-j}F^{(j)} \hspace{0.3cm}\text{for } i\geq 1
    \end{equation}
\end{lemma}
\noindent \textit{Proof:} We proceed by induction. For $i=1$ we have:
\begin{align*}
    \boldsymbol{\gamma}_{1} &= Q^{-1}\left[M\boldsymbol{\gamma}+\delta F^{(1)}\right] \\
    &= (Q^{-1}M)\boldsymbol{\gamma} + \delta Q^{-1}F^{(1)} \\
    &= (Q^{-1}M)\boldsymbol{\gamma} + \delta Q^{-1}\sum_{j=1}^{1}(MQ^{-1})^{1-j}F^{(j)}
\end{align*}
so the result is true for $i=1$. Assume it is true for $i$. For $i+1$ we can use the recursive defintion of the $\{\boldsymbol{\gamma}_i\}$ to compute:
\begin{align*}
    \boldsymbol{\gamma}_{i+1} &= Q^{-1}\left[M\boldsymbol{\gamma}_{i}+\delta F^{(i+1)}\right] \\
    &= Q^{-1}M\left[(Q^{-1}M)^{i}\boldsymbol{\gamma}+\delta Q^{-1}\sum_{j=1}^{i}(MQ^{-1})^{i-j}F^{(j)}\right] + \delta Q^{-1}F^{(i+1)} \\
    &= (Q^{-1}M)^{i+1}\boldsymbol{\gamma}+\delta Q^{-1}\left[F^{(i+1)}+MQ^{-1}\sum_{j=1}^{i}(MQ^{-1})^{i-j}F^{(j)}\right] \\
    &= (Q^{-1}M)^{i+1}\boldsymbol{\gamma} + \delta Q^{-1}\left[F^{(i+1)}+\sum_{j=1}^{i}(MQ^{-1})^{i+1-j}F^{(j)}\right] \\
    &= (Q^{-1}M)^{i+1}\boldsymbol{\gamma} + \delta Q^{-1}\sum_{j=1}^{i+1}(MQ^{-1})^{(i+1)-j}F^{(j)}
\end{align*}
so the result is true for $i+1$. Thus, the result holds for $i\geq 1$ by induction. \qedsymbol

\noindent Using Lemma \ref{lemma_1} we can now rewrite $m_N$ as follows:
\begin{align*}
    m_{N}(x,t) &= \Phi(x)^{*}\boldsymbol{\gamma}+\sum_{i=1}^{N}l^{(i-1)}(t)\Phi(x)^{*}\boldsymbol{c}^{(i)} \\
    &= \Phi(x)^{*}\boldsymbol{\gamma}+\sum_{i=1}^{N}l^{(i-1)}(t)\Phi(x)^{*}Q^{-1}\left[F^{(i)}-A\boldsymbol{\gamma}_{i-1}\right] \\
    % &= \Phi(x)^{*}\boldsymbol{\gamma}+\sum_{i=1}^{N}l^{(i-1)}(t)\Phi(x)^{*}Q^{-1}F^{(i)} \\
    &= \Phi(x)^{*}\boldsymbol{\gamma} + \sum_{i=1}^{N}l^{(i-1)}(t)\Phi(x)^{*}Q^{-1}F^{(i)}-\sum_{i=1}^{N}l^{(i-1)}(t)\Phi(x)^{*}Q^{-1}A\boldsymbol{\gamma}_{i-1} \\
    &= \Phi(x)^{*}\boldsymbol{\gamma}+\left(\sum_{i=1}^{N}l^{(i-1)}(t)\Phi(x)^{*}Q^{-1}\mathcal{I}_{t_i}\right)f-l^{(0)}(t)\Phi(x)^{*}Q^{-1}A\boldsymbol{\gamma}-\sum_{i=2}^{N}l^{(i-1)}(t)\Phi(x)^{*}Q^{-1}A\boldsymbol{\gamma}_{i-1} \\
    &= \Phi(x)^{*}\left[I-l^{(0)}(t)Q^{-1}A\right]\boldsymbol{\gamma} + \left(\sum_{i=1}^{N}l^{(i-1)}(t)\Phi(x)^{*}Q^{-1}\mathcal{I}_{t_i}\right)f - \sum_{i=1}^{N-1}l^{(i)}(t)\Phi(x)^{*}Q^{-1}A\boldsymbol{\gamma}_{i} \\
    &= \Phi(x)^{*}\left[I-l^{(0)}(t)Q^{-1}A\right]\boldsymbol{\gamma} + \left(\sum_{i=1}^{N}l^{(i-1)}(t)\Phi(x)^{*}Q^{-1}\mathcal{I}_{t_i}\right)f \\
    &- \sum_{i=1}^{N-1}l^{(i)}(t)\Phi(x)^{*}Q^{-1}A\left((Q^{-1}M)^{i}\boldsymbol{\gamma}+\delta Q^{-1}\sum_{j=1}^{i}(MQ^{-1})^{i-j}F^{(j)}\right) \\
    &= \Phi(x)^{*}\left[I-l^{(0)}(t)Q^{-1}A\right]\boldsymbol{\gamma} + \left(\sum_{i=1}^{N}l^{(i-1)}(t)\Phi(x)^{*}Q^{-1}\mathcal{I}_{t_i}\right)f-\sum_{i=1}^{N-1}l^{(i)}(t)\Phi(x)^{*}Q^{-1}A(Q^{-1}M)^{i}\boldsymbol{\gamma} \\
    &-\delta\sum_{i=1}^{N-1}l^{(i)}(t)\Phi(x)^{*}Q^{-1}AQ^{-1}\sum_{j=1}^{i}(MQ^{-1})^{i-j}F^{(j)} \\
    &= \Phi(x)^{*}\left[I-l^{(0)}(t)Q^{-1}A-\sum_{i=1}^{N-1}l^{(i)}(t)Q^{-1}A(Q^{-1}M)^{i}\right]\boldsymbol{\gamma} \\
    &+ \left[\sum_{i=1}^{N}l^{(i-1)}(t)\Phi(x)^{*}Q^{-1}\mathcal{I}_{t_i}-\delta\sum_{i=1}^{N-1}l^{(i)}(t)\Phi(x)^{*}Q^{-1}AQ^{-1}\sum_{j=1}^{i}(MQ^{-1})^{i-j}\mathcal{I}_{t_j}\right]f \\
    &= \Phi(x)^{*}\left[I-\sum_{i=0}^{N-1}l^{(i)}(t)Q^{-1}A(Q^{-1}M)^{i}\right]\boldsymbol{\gamma} \\
    &+ \left[l^{(0)}(t)\Phi(x)^{*}Q^{-1}\mathcal{I}_{t_1}+\sum_{i=2}^{N}l^{(i-1)}(t)\Phi(x)^{*}Q^{-1}\mathcal{I}_{t_i}-\delta\sum_{i=1}^{N-1}l^{(i)}(t)\Phi(x)^{*}Q^{-1}AQ^{-1}\sum_{j=1}^{i}(MQ^{-1})^{i-j}\mathcal{I}_{t_j}\right]f \\
    &= \Phi(x)^{*}\left[I-\sum_{i=0}^{N-1}l^{(i)}(t)Q^{-1}A(Q^{-1}M)^{i}\right]\boldsymbol{\gamma} \\
    &+\left[l^{(0)}(t)\Phi(x)^{*}Q^{-1}\mathcal{I}_{t_1}+\sum_{i=1}^{N-1}l^{(i)}(t)\Phi(x)^{*}Q^{-1}\mathcal{I}_{t_{i+1}}-\delta\sum_{i=1}^{N-1}l^{(i)}(t)\Phi(x)^{*}Q^{-1}AQ^{-1}\sum_{j=1}^{i}(MQ^{-1})^{i-j}\mathcal{I}_{t_j}\right]f \\
    &= \Phi(x)^{*}\left[I-\sum_{i=0}^{N-1}l^{(i)}(t)Q^{-1}A(Q^{-1}M)^{i}\right]\boldsymbol{\gamma} \\
    &+\left[l^{(0)}(t)\Phi(x)^{*}Q^{-1}\mathcal{I}_{t_1}+\sum_{i=1}^{N-1}l^{(i)}(t)\Phi(x)^{*}Q^{-1}\left(\mathcal{I}_{t_{i+1}}-\delta AQ^{-1}\sum_{j=1}^{i}(MQ^{-1})^{i-j}\mathcal{I}_{t_j}\right)\right]f \\
    &= \Phi(x)^{*}\left[I-\sum_{i=0}^{N-1}l^{(i)}(t)Q^{-1}A(Q^{-1}M)^{i}\right]\boldsymbol{\gamma} + \left[\sum_{i=0}^{N-1}l^{(i)}(t)\Phi(x)^{*}Q^{-1}\left(\mathcal{I}_{t_{i+1}}-\delta AQ^{-1}\sum_{j=1}^{i}(MQ^{-1})^{i-j}\mathcal{I}_{t_j}\right)\right]f \\
    &= c(x,t)+(Lf)(x,t)
\end{align*}
\textit{Note: we have used the convention that a sum from } $j=1$ \textit{to} $j=0$ \textit{is considered to be empty, i.e.} $0$. In the above we have defined:
\begin{align}
    \label{constant_part}
    c(x,t) &= \Phi(x)^{*}\left[I-\sum_{i=0}^{N-1}l^{(i)}(t)Q^{-1}A(Q^{-1}M)^{i}\right]\boldsymbol{\gamma} \\
    \label{operator_on_f_part}
    (Lf)(x,t) &= \Phi(x)^{*}\left[\sum_{i=0}^{N-1}l^{(i)}(t)Q^{-1}\left(\mathcal{I}_{t_{i+1}}-\delta AQ^{-1}\sum_{j=1}^{i}(MQ^{-1})^{i-j}\mathcal{I}_{t_j}\right)\right]f
\end{align}

\noindent We will now marginalise over $f$ in order to obtain the averaged conditional distribution. To do this we will need the following Lemma (which we prove below): \vspace{10pt}
\begin{lemma}
    Let $f\sim\mathcal{N}(\bar{f},K)$ where we assume that this Gaussian measure is on a Hilbert space of functions $\mathcal{H}_{1}\subset\mathbb{R}^{\mathcal{X}}$. Suppose that for a fixed realisation of $f$ we have
    \begin{equation*}
        y|f\sim\mathcal{N}(Lf+c,V)
    \end{equation*}
    where $L$ is a bounded linear operator from $\mathcal{H}_{1}$ to another Hilbert space $\mathcal{H}_{2}\subset\mathbb{R}^{\mathcal{X}}$ (so $y$ lies in $\mathcal{H}_{2}$), $c$ is a deterministic function and the covariance opertor $V$ does not depend on $f$. Then marginalizing over $f$ yields:
    \begin{equation*}
        y\sim\mathcal{N}(L\bar{f}+c,LKL^{*}+V)
    \end{equation*}
    as the averaged distribution of $y$.
\end{lemma}
% \noindent \textit{Proof:} To determine this marginal distribution we must compute the expectation of an arbitrary bounded cylindrical function, i.e. we must compute:
% \begin{equation*}
%     \int\int\psi(y^{N})\delta_{\mathcal{L}f}(\mathrm{d}y)\mu_{\bar{f},K}(\mathrm{d}f)
% \end{equation*}
% where $y^{N}=Py$ and $P:\mathcal{H}_{2}\rightarrow\mathbb{R}^{N}$ is the bounded linear operator given by $Ph:=(h(x_1),\dots,h(x_N))^{T}$ where the $x_{i}\in\mathcal{X}$ for $i=1,\dots,N$. Note that $u^{N}|\{f\}\sim\delta_{P\mathcal{L}f}$. We also have that $Y:=P\mathcal{L}f\sim\mathcal{N}(P\mathcal{L}\bar{f},P\mathcal{L}K\mathcal{L}^{*}P^{*})$. Using these facts we can write:
% \begin{align*}
%     \int\int\psi(y^{N})\delta_{\mathcal{L}f}(\mathrm{d}y)\mu_{\bar{f},K}(\mathrm{d}f) &= \int\int\psi(y^N)\delta_{P\mathcal{L}f}(dy^{N})\mu_{\bar{f},K}(\mathrm{d}f) \\
%     &= \int\int\psi(y^{N})\delta_{Y}(dy^N)\mu_{\bar{f},K}(\mathrm{d}f) \\
%     &=\int\int\psi(y^{N})\delta_{Y}(dy^N)\mu_{P\mathcal{L}\bar{f},P\mathcal{L}K\mathcal{L}^{*}P^{*}}(\mathrm{d}Y) \\
%     &=\int\psi(y^{N})\left(\int\delta_{Y}(dy^N)\mu_{P\mathcal{L}\bar{f},P\mathcal{L}K\mathcal{L}^{*}P^{*}}(\mathrm{d}Y)\right) \\
%     &=\int\psi(y^{N})\mu_{P\mathcal{L}\bar{f},P\mathcal{L}K\mathcal{L}^{*}P^{*}}(\mathrm{d}y^{N})
% \end{align*}
% We thus obtain the expectation of $\psi(y^N)$ with respect to the multivariate Gaussian distribution:
% \begin{equation*}
%     \mathcal{N}(P\mathcal{L}\bar{f},P\mathcal{L}K\mathcal{L}^{*}P^{*})
% \end{equation*}
% We thus have:
% \begin{equation*}
%     y^{N}=Py\sim\mathcal{N}(P\mathcal{L}\bar{f},P\mathcal{L}K\mathcal{L}^{*}P^{*})
% \end{equation*}
% From this we can conclude that $y$ is distributed as $\mathcal{N}(\mathcal{L}\bar{f},\mathcal{L}K\mathcal{L}^{*})$. Indeed, we have that $\mathcal{L}\bar{f}\in\mathcal{H}_{2}$ is in the right space and also $\mathcal{L}K\mathcal{L}^{*}$ is a trace-class operator from $\mathcal{H}_{2}\rightarrow\mathcal{H}_{2}$ (since $\mathcal{L}$ and its adjoint are both bounded linear operators and $K$ is trace-class). We thus have that $\mathcal{N}(\mathcal{L}\bar{f},\mathcal{L}K\mathcal{L}^{*})$ is a valid Gaussian distribution. To show that we indeed have this as the distribution of $y$ we can proceed by contradiction: suppose that $y\sim\mathcal{N}(m,V)$.
\noindent\textit{Proof:} The fact that $y|f\sim\mathcal{N}(Lf+c,V)$ is equivalent to saying that:
\begin{equation*}
    y=Lf+c+\tilde{y}
\end{equation*}
where $\tilde{y}\sim\mathcal{N}(0,V)$ is independent of $f$. Thus we have:
\begin{equation*}
    \begin{pmatrix}
        f \\
        \tilde{y}
    \end{pmatrix}=\mathcal{N}\left(\begin{pmatrix}
                                \bar{f} \\
                                0
                            \end{pmatrix},\begin{pmatrix}
                                            K & 0 \\
                                            0 & V
                                          \end{pmatrix}\right)
\end{equation*}
Since we can write:
\begin{equation*}
    y=\begin{pmatrix}
        L & 1
    \end{pmatrix}\begin{pmatrix}
                    f \\
                    \tilde{y}
                 \end{pmatrix}+c
\end{equation*}
we deduce:
\begin{equation*}
    y\sim\mathcal{N}\left(L\bar{f}+c, \begin{pmatrix}
        L & 1
    \end{pmatrix}\begin{pmatrix}
                    K & 0 \\
                    0 & V
                \end{pmatrix}\begin{pmatrix}
                    L^{*} \\
                    1
                \end{pmatrix}\right)=\mathcal{N}(L\bar{f}+c,LKL^{*}+V)
\end{equation*}
as required. \qedsymbol

\noindent We can now perform the marginalisation over $f$ noting that $V_N=0$ does not depend on $f$ to obtain:
\begin{equation}
    \int u|\{\tilde{A}_{t_1}u=F^{(1)},\dots,\tilde{A}_{t_N}u=F^{(N)},f\}\mathrm{d}f\sim\mathcal{N}(L\bar{f}+c,LKL^{*})
\end{equation}
with $L$ and $c$ given by equations (\ref{constant_part}) and (\ref{operator_on_f_part}) respectively. \textbf{\textit{Note: We must still prove that $L$ is bounded and work out its adjoint $L^{*}$.}}

\end{document}
