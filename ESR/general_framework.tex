We now introduce the general framework we will be considering. This will be split into several parts. We first outline the general type of problem we will be considering in this report. Having done this we will then aim to describe how two priors for the solution to this problem will be formed and then discuss how further inference will be carried out when observational data from the physical system is available.


\subsection{Spatial Boundary Value Problems}

We will consider PDEs which do not have any time dependence in this report. Future work will deal with the case of time dependent PDEs. Let $\Omega$ be a domain with boundary $\partial\Omega$ and let $\mathcal{L}$ be a suitable linear differential operator. We will focus on the following Dirichlet problem:
\begin{equation}
\label{DirichletProb}
    \left\{
    \begin{array}{cc}
        \mathcal{L}u=f & \text{on } \Omega  \\
         u = g  & \text{ on } \partial\Omega
    \end{array}
    \right.
\end{equation}
where the functions $f,g$ may be noisy, typically Gaussian. The operator $\mathcal{L}$ may also be random. For simplicity we will take $g=0$ and will assume that $\mathcal{L}$ is deterministic. We will also now assume that $f$ is Gaussian, i.e. $f\sim\mathcal{N}(\bar{f},K)$. We seek the solution in some appropriate separable Hilbert space of functions $\mathcal{H}$. We now discuss how the two priors over the solution space are formed. The first prior will essentially come from the true solution of the PDE while the second will be the ``output" of a probabilistic numerical method\footnote{Note that in our discussion of probabilistic numerical methods we referred to the output as being a posterior distribution over the solution of the problem; here we are viewing the output as what we will set our prior to be for further inference.} for solving the problem (\ref{DirichletProb}). As such we will initially place an appropriate prior on the solution $u$ which encapsulates our prior belief on the solution (such as its smoothness) before utilising detailed knowledge of the specific PDE. We will then consider using an approximation to obtain a posterior distribution which will then become our prior for further inference. We start with utilising the true solution:

\subsection{Prior from the true solution}
\label{prior_true_solution}

Formally we have $u=\mathcal{L}^{-1}f$, where $\mathcal{L}^{-1}$ is the solution operator (Green's function) for the problem (\ref{DirichletProb}). Since $f$ is random we see that the solution is also random; in particular it is the push-forward of $f$ by the \textbf{linear} operator $\mathcal{L}^{-1}$. Provided that the linear operator $\mathcal{L}^{-1}$ is bounded it follows from the theory of Gaussian measures (see Proposition 1.18 of \textcolor{blue}{\cite{da2006introduction}}) that $u$ is also Gaussian and in particular has the following distribution:
\begin{equation}
    \label{true_prior}
    u\sim\mathcal{N}(\mathcal{L}^{-1}\bar{f},\mathcal{L}^{-1}K(\mathcal{L}^{-1})^{*})
\end{equation}
This will be one of the two priors we will consider. One should note that this prior indeed utilises the true solution to the PDE. However, for most PDEs it will be intractable to actually compute this prior due to the fact that $\mathcal{L}^{-1}$ is inaccessible. This leads us instead to consider a prior based on a probabilistic numerical method for solving (\ref{DirichletProb}):

\subsection{Prior from an approximation}

We will now consider placing an initial Gaussian prior on the solution $u$, $u\sim\mathcal{N}(0,V)$. The covariance operator $V$ will be chosen so as to satisfy the following assumption:

\noindent \textbf{Assumption 1} $V$ is controlled so that $u$ lies almost surely in some appropriate subspace $\mathcal{U}\subset\mathcal{H}$ where the linear operator $\mathcal{L}$ restricted to this subspace is bounded\footnote{$\mathcal{L}$, being a differential operator, is in general an unbounded operator when viewed on the whole function space $\mathcal{H}$.}, i.e. $\mathcal{L}:\mathcal{U}\rightarrow\mathcal{U}^{\prime}$ is bounded where $\mathcal{U}\subset\mathcal{H}\subset{\mathcal{U}^{\prime}}$.
\vspace{10pt}

\begin{remark}
    We shall assume here that the subspace $\mathcal{U}$ is reflexive so that $\mathcal{U}^{\prime\prime}$ can be identified with $\mathcal{U}$.
    $\mathbin{\blacklozenge}$
\end{remark}

Having placed this prior on $u$ we will now consider conditioning on the ``observation" of a suitable information operator in order to form a posterior distribution which will be taken to be our new prior. This information operator will involve a discretization $\mathcal{F}_{h}$ of the function space $\mathcal{H}$, where $h$ is the mesh size. In particular, we will consider the following information operators $I_{j}:\mathcal{U}^{\prime}\rightarrow\mathbb{R}$ defined by:
\begin{equation*}
    I_{j}\boldsymbol{\cdot} = \langle\boldsymbol{\cdot},\phi_{j}\rangle
\end{equation*}
where $\langle\boldsymbol{\cdot},\boldsymbol{\cdot}\rangle$ is the duality pairing between $\mathcal{U}$ and $\mathcal{U}^{\prime}$ and where the $\phi_{j}\in\mathcal{F}_{h}$ for $j=1,\dots,J$. Note that $J$ is inversely proportional to the mesh size $h$, in particular if $\Omega\subset\mathbb{R}^{2}$ then $J\propto 1/h^2$. It is also important to emphasise that $I_{j}\in\mathcal{U}^{\prime\prime}$ and so they are bounded linear operators.
\vspace{10pt}

\begin{remark}
    Think of the $\{\phi_{j}\}$ as a basis for finite element spaces.
    $\mathbin{\blacklozenge}$
\end{remark}
Let $\mathcal{I}$ be the concatenation of these information operators, i.e. $\mathcal{I}=(I_1,\dots,I_J)^{T}$. We will refer to $\mathcal{I}$ as the information operator. The probabilistic numerical method will involve conditioning on the ``observation" of this information operator. To be more specific consider $\mathcal{I}$ acting on the PDE $\mathcal{L}u=f$. We have,
\begin{equation*}
    \mathcal{I}\mathcal{L}u=\mathcal{I}f=:F
\end{equation*}
where both $\mathcal{I}\mathcal{L}u$ and $F$ are vectors in $\mathbb{R}^{J}$. Since, in theory, we know the properties of the forcing term $f$ we have access to $F$. Our second prior will thus be the distribution of $u$ conditional on ``observing" $\mathcal{I}\mathcal{L}u=F$. In order to work out what this distribution is we will first consider the joint distribution of $(u,\mathcal{I}\mathcal{L}u)^{T}$. It is here that Assumption 1 is necessary, since we will utilise the fact that, under our prior, $u$ lies almost surely in a subspace where the restriction of $\mathcal{L}$ is bounded. This implies that the joint distribution is as follows:
\begin{equation}
    \label{jointInfoDist}
    \begin{pmatrix}
    u \\
    \mathcal{I}\mathcal{L}u
    \end{pmatrix} =
    \begin{pmatrix}
    I \\ \mathcal{I}\mathcal{L}
    \end{pmatrix} u \sim \mathcal{N}\left(
    \begin{pmatrix}
    0 \\
    \mathbf{0}
    \end{pmatrix},
    \begin{pmatrix}
    V & V\mathcal{L}^{*}\mathcal{I}^{*} \\
    \mathcal{I}\mathcal{L}V & \mathcal{I}\mathcal{L}V\mathcal{L}^{*}\mathcal{I}^{*}
    \end{pmatrix}
    \right)
\end{equation}
In order to progress we will now consider that we fix a realisation of $f$ (and so the corresponding $F$ is also fixed). Doing so allows us to perform the conditioning step (see the \textcolor{blue}{\nameref{appendix}} for a full justification of this conditioning step) yielding the Bayesian update:
\begin{equation}
    \label{conditionalDistnFixed_f}
    u|\{\mathcal{I}\mathcal{L}u=F,f\}\sim\mathcal{N}(a,\Sigma)=:\mu_{a,\Sigma}
\end{equation}
where,
\begin{align}
    \label{post_mean_before_averaging}
    a &= V\mathcal{L}^{*}\mathcal{I}^{*}(\mathcal{I}\mathcal{L}V\mathcal{L}^{*}\mathcal{I}^{*})^{-1}F \\
    \label{post_var_before_averaging}
    \Sigma &= V - V\mathcal{L}^{*}\mathcal{I}^{*}(\mathcal{I}\mathcal{L}V\mathcal{L}^{*}\mathcal{I}^{*})^{-1}\mathcal{I}\mathcal{L}V
\end{align}
\begin{remark}
We use the notation $\mu_{m,A}$ to denote a Gaussian measure (in arbitrary dimensions) with mean $m$ and covariance $A$. The space (and hence the dimension) should be clear from the domain of the covariance operator.
$\mathbin{\blacklozenge}$
\end{remark}

We will now seek to marginalize over $f$ in (\ref{conditionalDistnFixed_f}) so as to obtain the ``average" conditional distribution over all possible values of the forcing. This will give us our second prior. We state in the proposition below the result of performing this marginalization: \vspace{10pt}

\begin{proposition}
    \label{first_prop}
    Let $u$ satisfy problem (\ref{DirichletProb}) and suppose that we place a prior $u\sim\mathcal{N}(0,V)$ where $V$ satisfies Assumption 1. Also suppose that $g=0$ and that the forcing $f$ is distributed as $f\sim\mathcal{N}(\bar{f},K)$. Then the conditional distribution $u|\{\mathcal{I}\mathcal{L}u=F\}$ is given by:
    \begin{equation}
        \label{approx_prior}
        u|\{\mathcal{I}\mathcal{L}u=F\}\sim\mathcal{N}(Q\bar{F},\Sigma+QK_{\mathcal{I}}Q^{*})
    \end{equation}
    where we have $\bar{F}:=\mathcal{I}\bar{f}$, $K_{\mathcal{I}}:=\mathcal{I}K\mathcal{I}^{*}$ and $Q:=V\mathcal{L}^{*}\mathcal{I}^{*}(\mathcal{I}\mathcal{L}V\mathcal{L}^{*}\mathcal{I}^{*})^{-1}$.
\end{proposition}

\noindent \textit{Proof:}
In order to perform this marginalization over $f$ we first note:
\begin{equation}
    f\sim\mathcal{N}(\bar{f},K)=:\mu_{\bar{f},K} \implies \mathcal{I}f=F\sim\mathcal{N}(\mathcal{I}\bar{f},\mathcal{I}K\mathcal{I}^{*})=:\mu_{\bar{F},K_{\mathcal{I}}}
\end{equation}
where $\bar{F}:=\mathcal{I}\bar{f}$ and $K_{\mathcal{I}}:=\mathcal{I}K\mathcal{I}^{*}$. In order to specify the ``average" conditional distribution it suffices to compute the expectation of arbitrary bounded cylindrical test functions $\psi(u^{N}):=\psi(u(x_1),\dots,u(x_{N}))$ since the $\sigma-$algebra generated by cylinder sets coincides with the Borel $\sigma-$algebra (see for instance Theorem 2.1.1 in \textcolor{blue}{\cite{lunardi2015infinite}}). We must thus compute:
\begin{equation}
    \int\int\psi(u^{N})\mu_{a,\Sigma}(\mathrm{d}u)\mu_{\bar{f},K}(\mathrm{d}f)
\end{equation}
Note that $u^{N}=Pu$ where the bounded linear operator $P:\mathcal{H}\rightarrow\mathbb{R}^{N}$ is defined by $Ph:=(h(x_1),\dots,h(x_N))^T$ for any function $h$. Thus, $u^{N}$ is multivariate normal, i.e., $u^{N}\sim\mathcal{N}(Pa,P\Sigma P^{*})=:\mu_{Pa,\Sigma_{N}}$ where $\Sigma_{N}:=P\Sigma P^{*}$ is the $N\times N$ covariance matrix of $u^{N}$. We thus have:
\begin{equation}
    \int\int\psi(u^{N})\mu_{a,\Sigma}(\mathrm{d}u)\mu_{\bar{f},K}(\mathrm{d}f) = \int\int\psi(u^{N})\mu_{Pa,\Sigma_N}(\mathrm{d}u^{N})\mu_{\bar{f},K}(\mathrm{d}f)
\end{equation}
We note that $Pa=PQF$ where $Q:= V\mathcal{L}^{*}\mathcal{I}^{*}(\mathcal{I}\mathcal{L}V\mathcal{L}^{*}\mathcal{I}^{*})^{-1}$. Since the conditional distribution of $u$, (\ref{conditionalDistnFixed_f}), and hence of $u^{N}$ only depends on $f$ through $F\in\mathbb{R}^{J}$ we can thus write:
\begin{equation}
    \int\int\psi(u^{N})\mu_{Pa,\Sigma_N}(\mathrm{d}u^{N})\mu_{\bar{f},K}(\mathrm{d}f)=\int\int\psi(u^{N})\mu_{PQF,\Sigma_N}(\mathrm{d}u^{N})\mu_{\bar{F},K_{\mathcal{I}}}(\mathrm{d}F)
\end{equation}
Both measures in the above integral are now multivariate normal and so we can utilise the well-known formula for multidimensional Gaussian integrals to conclude that our integral becomes:
\begin{equation}
    \label{psi_integral_one}
    \int\psi(u^{N})\mu_{h^{N},\Sigma_{\mathcal{I}}}(\mathrm{d}u^{N})
\end{equation}
where,
\begin{align}
    h^{N}&:=\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PQB^{-1}K_{\mathcal{I}}^{-1}\bar{F}=PQ\bar{F} \\
    \Sigma_{\mathcal{I}}&:=P(\Sigma+QK_{\mathcal{I}}Q^{*})P^{*} \\
    B &:= \left(Q^{*}P^{*}\Sigma_{N}^{-1}PQ+K_{\mathcal{I}}^{-1}\right)
\end{align}
The details of this computation are left to the \textcolor{blue}{\nameref{appendix}}. From this we can see that we have obtained the expectation of $\psi$ w.r.t. a multivariate Gaussian with mean and covariance given by $h^{N}$ and $\Sigma_{\mathcal{I}}$. Thus, we can conclude that ``averaging" over $f$ gives the following Gaussian posterior:
\begin{equation}
    u|\{\mathcal{I}\mathcal{L}u=F\}\sim\mathcal{N}\left(Q\bar{F},\Sigma+QK_{\mathcal{I}}Q^{*}\right)
\end{equation}
\qedsymbol

This posterior will be taken to be our second prior for further inference. One should note that this prior does indeed arise from an approximation to the solution of the PDE as it involves the information operator $\mathcal{I}$ which contains information on only a finite number of FE basis functions ($J$ of them to be precise). The hope is that as the mesh size $h$ decreases to $0$ (and so $J\rightarrow\infty$) this distribution will become more and more like the true prior (\ref{true_prior}). In Section \textcolor{blue}{\ref{elliptic_bv_prob}} we will obtain, for a specific example, an upperbound on the distance between these two priors in terms of $h$ and we will see that the two priors do indeed agree in the limit $h\rightarrow 0$.

It is also worth pointing out that the approximate prior (\ref{approx_prior}) contains details of $V,\mathcal{I},\bar{f},K$ i.e. it contains information from all of: the original prior on $u$, the information operator and the statistical properties of the forcing term.

\subsection{Incorporating Observational Data}
\label{incorporating_obs_data}

We now have two prior distributions for the solution $u$ to our boundary value problem (\ref{DirichletProb}), $\nu_{i}=\mathcal{N}(m_{i},\Sigma_{i})$ for $i=1,2$. The first is the prior from the true solution to (\ref{DirichletProb}) which has mean and covariance given by:
\begin{align}
    m_{1} &= \mathcal{L}^{-1}\bar{f} \\
    \Sigma_{1} &= \mathcal{L}^{-1}K(\mathcal{L}^{-1})^{*}
\end{align}
The second is the prior from using an approximation to the solution of (\ref{DirichletProb}) and this has mean and covariance given by:
\begin{align}
    m_{2} &= Q\bar{F} \\
    \Sigma_{2} &= \Sigma + QK_{\mathcal{I}}Q^{*}
\end{align}
with $Q$ as in Proposition \textcolor{blue}{\ref{first_prop}}.

We will now also assume that we have observational data, coming from sensors say, which give us noisy observations of the value of $u$ at some points $y_{1},\dots,y_{s}$ in $\Omega$. We wish to update our belief in the distribution of $u$ using this sensor data. In theory we can use either of the $\nu_{i}$ as prior distributions. However, as mentioned previously the true prior will often be intractable and as such we will be forced to use the second approximate prior. Our goal will thus be to investigate how different the two resulting posteriors are. This analysis will be carried out in the next section where we will consider a particular PDE and a particular choice of $V$ which will correspond to a statistical version of FEM. For now we will outline the computation of the posterior distributions for the general framework under consideration.

Let $S:\mathcal{H}\rightarrow\mathbb{R}^{s}$ be the operator which maps a function $h\in\mathcal{H}$ to $(h(y_1),\dots,h(y_s))^{T}$ in $\mathbb{R}^{s}$. Assuming that the sensors make observations at each $y_j$ with a normally distributed error we have that the likelihood for our sensor readings, $\mathbf{v}$, is:
\begin{equation}
  \label{likelihood}
  \mathbf{v}|u \sim \mathcal{N}(Su,\epsilon^{2}I)
\end{equation}
The goal is to now find the posterior $u|\mathbf{v}$ when our prior is $u\sim\nu_{i}\equiv\mathcal{N}(m_{i},\Sigma_{i}), \hspace{0.2cm} i=1,2$. In order to do so we will first find the joint distribution of $(u,\mathbf{v})^{T}$. To do this we note that our assumption that $\mathbf{v}|u$ is distributed according to (\ref{likelihood}) is equivalent to the assertion that:
\begin{equation*}
  \mathbf{v}=Su+\boldsymbol{\delta}
\end{equation*}
where $\boldsymbol{\delta}\sim\mathcal{N}(\mathbf{0},\epsilon^{2}I)$ is independent of $u$. From this we can see that the joint distribution of $(u,\boldsymbol{\delta})^{T}$ is:
\begin{equation*}
  \begin{pmatrix}
    u \\
    \boldsymbol{\delta}
  \end{pmatrix}\sim\mathcal{N}\left(\begin{pmatrix}
                                    m_{i} \\
                                    \mathbf{0}
                                    \end{pmatrix},
                                    \begin{pmatrix}
                                      \Sigma_{i} & 0 \\
                                      0 & \epsilon^{2}I
                                    \end{pmatrix}
                              \right)
\end{equation*}
We now note that $(u,\mathbf{v})$ can be expressed as:
\begin{equation*}
  \begin{pmatrix}
    u \\
    \mathbf{v}
  \end{pmatrix}=\begin{pmatrix}
                  I & 0 \\
                  S & I
                \end{pmatrix}\begin{pmatrix}
                              u \\
                              \boldsymbol{\delta}
                             \end{pmatrix} +
                             \begin{pmatrix}
                                0 \\ \mathbf{0}
                             \end{pmatrix}
\end{equation*}
Since this is just a linear transformation of $(u,\boldsymbol{\delta})^{T}$ we have that the joint distribution of $(u,\mathbf{v})^{T}$ is given by:
\begin{equation}
  \label{jointDist}
  \begin{pmatrix}
    u \\
    \mathbf{v}
  \end{pmatrix}\sim\mathcal{N}\left(
                              \begin{pmatrix}
                                m_{i} \\
                                Sm_{i}
                              \end{pmatrix},
                              \begin{pmatrix}
                                \Sigma_{i} & \Sigma_{i} S^{*} \\
                                S\Sigma_{i} & \epsilon^{2}I+S\Sigma_{i} S^{*}
                              \end{pmatrix}
                              \right)
\end{equation}
From this it is simply a matter of conditioning to obtain the posterior distribution of $u|\mathbf{v}$. We thus obtain:
\begin{equation}
  \label{posterior}
  u|\mathbf{v}\sim\mathcal{N}(m^{(i)}_{u|\mathbf{v}},\Sigma^{(i)}_{u|\mathbf{v}})
\end{equation}
where,
\begin{align}
  \label{posterior_mean}
  m^{(i)}_{u|\mathbf{v}}&:=m_i+\Sigma_{i}S^{*}(\epsilon^{2}I+S\Sigma_{i}S^{*})^{-1}(\mathbf{v}-Sm_{i}) \\
  \label{posterior_covariance}
  \Sigma^{(i)}_{u|\mathbf{v}}&:=\Sigma_{i}-\Sigma_{i}S^{*}(\epsilon^{2}I+S\Sigma_{i}S^{*})^{-1}S\Sigma_{i}
\end{align}

The next step in the analysis will be to quantify how different these two posteriors are by utilizing the Wasserstein Distance. In order to do this we must first investigate the distance between the priors $\nu_{i}$. This will be done in the next section where we look at a particular PDE and a particular choice of $V$. We note that the presentation here has been quite general and that with a little bit of work it will even be possible to generalise this procedure to PDEs which have time dependence.
