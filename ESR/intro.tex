
Mathematical models of physical systems are often expressed in terms of Partial Differential Equations (PDEs). In such models uncertainty is often introduced, either through a lack of knowledge of the parameters of the system or via inherent randomness in the system itself. Furthermore, these mathematical models and the associated computer simulations are often simplifications of the actual system leading to possible model misspecification \textcolor{blue}{\cite{girolami2019statistical}}. The numerical algorithms used to simulate such models also induce uncertainty \textcolor{blue}{\cite{conrad2017statistical}}. For example, it is often the case that a numerical method involves a finite-dimensional approximation of the unknown function. The computer simulations of these models can also often be very expensive computationally, and as such, these models are rarely used alone in the modelling procedure. Instead, observational data from the actual physical system via measurements is often incorporated as well. Data from measurements is now becoming increasingly available in almost every area of engineering and science, and failure to consider either the data or the model is clearly suboptimal. The issue of combining knowledge from both the model and data is thus of utmost importance and is often referred to as \textbf{data assimilation}, especially when the underlying mathematical model is a potentially stochastic dynamical system and the data may be time-ordered \textcolor{blue}{\cite{law2015data}}.

The Bayesian formulation of PDE-based models allows one to naturally incorporate all these sources of uncertainty (while still being able to identify the relevant source) and forces one to deal with modelling issues in a clear and precise manner. It allows measurement data to be considered in order to calibrate and tune mathematical models. Moreover, it allows a full characterization of all possible solutions to be found, together with their relative probabilities \textcolor{blue}{\cite{stuart2010inverse}}.

The statistical formulation of PDE-based models is necessary in many applications in order to handle in a precise manner the uncertainty present in the model. It allows this uncertainty to be propagated forward and gives clear answers for how much trust can be placed in the conclusions of the model. The finite element method (FEM) \textcolor{blue}{\cite{strang1973analysis}} is one of the most widely used methods for numerically approximating the solution of PDEs modelling natural and physical systems. Since FEM is an integral part of the study of many physical systems it is essential that we are able to fully quantify the uncertainty that using FEM introduces to our models and simulations.

In the recent paper ``The Statistical Finite Element Method" \textcolor{blue}{\citeauthor{girolami2019statistical}} introduce a novel unifying approach which provides a fully statistical FEM in which both the finite element model and observational data are combined into a coherent inferential framework. This approach allows observational data to provide us with data adjusted FEM solutions. In particular, this paper considers a large class of linear PDEs and assumes that we have incomplete knowledge of the forcing term. The uncertainty resulting from this lack of knowledge is then formally accounted for by modelling the forcing as being random, having an appropriately defined Gaussian process (GP) distribution. The approximation of the linear PDE using the FE method then yields a multivariate Gaussian distribution on the resulting finite dimensional approximation. This probabilistic representation of the FE method is then used to condition the model on sensor data, providing a systematic methodology by which one can statistically update the Galerkin FEM solution.

Our work aims to provide more detailed error analysis explicitly quantifying the extent by which the distributions obtained using the ``true" solution and the FEM solution differ. Once this is investigated we will then explore how this propagates through to any further inference. To be more specific, we will consider placing two different priors for any further inference. The first prior we will consider will be the distribution obtained using the true solution operator of the PDE. The second prior will be that obtained using the FEM solution. In the case of Gaussian forcing, and under suitable conditions which will be specified, these two priors are both Gaussian measures on appropriate function spaces. We will aim to investigate how these priors differ by utilising the Wasserstein distance between probability measures. In particular, we will obtain an upper-bound for this distance by utilising a connection between the Wasserstein distance and the Procrustes Metric on covariance operators. Having investigated this discrepancy between the priors we will then investigate how this translates to a discrepancy in the subsequent posteriors obtained using observational data. Our treatment aims to follow the guiding principle of ``\textit{avoiding discretization until the the last possible moment}" \textcolor{blue}{\cite{stuart2010inverse}}. This principle is a very powerful one used throughout numerical analysis, and we will aim to highlight its importance in our work.

The remainder of the report is structured as follows. Section \textcolor{blue}{\ref{lit_review}} provides a concise account of the most relevant background material for the project together with a brief survey of the literature behind the topic of this work. In Section \textcolor{blue}{\ref{general_framework}} we introduce the general framework we will be considering. We then apply the general framework in the context of FEM for an elliptic boundary value problem in Section \textcolor{blue}{\ref{elliptic_bv_prob}} and then to a parabolic problem in Section \textcolor{blue}{\ref{parabolic_prob}}. Section \textcolor{blue}{\ref{conclusion}} finally discusses in more general terms the aims of our research before outlining several ideas for future work.
