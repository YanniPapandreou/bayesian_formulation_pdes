Our project is focused on providing detailed analysis of the uncertainty introduced by utilising numerical approximations for solving potentially noisy PDEs. The propagation of this error through to further inference is then investigated, when for instance observational data is incorporated. This project lies at the intersection of the fields of data assimilation, data-centric engineering, and Bayesian inference. We now provide a brief overview of the relevant background material for the project.

There is increasing interest in investigating how the language of probabilistic inference can be applied to numerical problems in order to provide a more detailed notion of the uncertainty resulting from numerically approximating an intractable problem \textcolor{blue}{\citep{diaconis1988bayesian,o1992bayesian,skilling1992bayesian,hennig2015probabilistic}}. Numerical algorithms can be viewed as estimation rules for a latent, often intractable quantity given the results of tractable computations. Such algorithms can be considered to perform inference and are thus open to being analysed using the formal framework of probability theory. The field of Probabilistic Numerics (PN) \textcolor{blue}{\citep{probNumericsSite}} involves the study of so called ``probabilistic numerical methods"; these are numerical algorithms which take in a probability distribution over its inputs and give out a probability distribution over its output. Some probabilistic methods can even be shown to coincide with classical ones under certain conditions (references can be found at \textcolor{blue}{\citep{probNumericsSite}}). It is worth pointing out that so far we have only been referring to problems of a deterministic nature and probability theory is used as a means of providing a notion of the uncertainty inherent in using a numerical approximation to the solution of an intractable deterministic problem. In our work we will not restrict attention to purely deterministic problems but instead will consider potentially noisy PDEs.

Much work has already been undertaken in the field of PN into applications to differential equations, especially for ODEs. Classic numerical algorithms for solving initial value problems (IVPs) provide an approximate solution often defined on a grid of time points. This numerical solution is often computed iteratively by collecting information from evaluations of the vector field associated to the system of differential equations. Probabilistic numerical methods instead provide probability measures, as opposed to point estimates, over the space of possible solutions to the IVP. In the PN literature there are two main approaches to solving ODEs which we now briefly outline.

The first approach introduces probability measures to ODE solvers by representing the distribution of all numerically possible trajectories with a set of sample paths using various different methods of computation (see \textcolor{blue}{\citep{chkrebtii2016bayesian,conrad2017statistical,teymur2016probabilistic,lie2019strong,abdulle2020random,teymur2018implicit}}). \textcolor{blue}{\citep{chkrebtii2016bayesian}} draws them from a Gaussian process regression while \textcolor{blue}{\citep{conrad2017statistical,teymur2016probabilistic,lie2019strong,teymur2018implicit}} perturb classical estimates after an integration step with suitably scaled Gaussian noise and \textcolor{blue}{\citep{abdulle2020random}} instead perturbs the classical estimate via choosing a stochastic step size.

The second approach \textcolor{blue}{\citep{schober2014probabilistic,kersting2016active,magnani2017bayesian,schober2019probabilistic,tronarp2019probabilistic,kersting2018convergence}} recasts IVPs as \textbf{stochastic filtering problems}. This method involves assuming \textit{a priori} that the solution of the IVP and a prespecified number of its derivatives follow a Gauss-Markov process that solves a particular stochastic differential equation (SDE). The evaluations of the vector field of the IVP at numerical estimates of the true solution are then regarded as imperfect evaluations of the time derivative of the solution and are thus used as a Bayesian update for the Gauss-Markov process. This approach gives an algorithm very similar in structure to that of the Kalman filter.

More recently another approach has been taken in the PN community towards ODEs. In \textcolor{blue}{\citep{wang2018role}} the authors introduce a novel Bayesian PNM for solving a limited class of ODEs. This method involves the use of a combination of classical Lie group methods, to exploit underlying symmetries in the gradient field of the ODE, and non-parametric regression in a transformed solution space for the ODE. This method provides a positive answer to the question of whether there exist true exact Bayesian PNM as defined in \textcolor{blue}{\citep{cockayne2019bayesian}}. Such a Bayesian approach to PNM holds several advantages. For a discussion of the benefits one should consult the above two references.

There are some existing numerical methods for dealing with stochastic PDEs. In chapter 9 of ``An Introduction to Computational Stochastic PDEs" \textcolor{blue}{\citep{lord2014introduction}} the authors analyse several different methods for dealing with elliptic PDEs with random data. In particular, the following (random) elliptic boundary-value problem (BVP) on a domain $D\subset\mathbb{R}^d$ is considered:
\begin{align*}
-\nabla \cdot(a(x) \nabla u(x)) &=f(x), \hspace{0.25cm} \forall x \in D \\
u(x) &=g(x), \hspace{0.25cm} \forall x \in \partial D
\end{align*}
where $\{a(x)|x\in D\}$ and $\{f(x)|x\in D\}$ are second-order random fields. The authors consider several methods for dealing with such a BVP. To start they first consider a variational formulation on $D$ and show that under suitable assumptions on the diffusion coefficients there is a unique solution to the variational formulation almost surely. A Galerkin FE approximation is then established for this formulation.
The FEM is then combined with the Monte Carlo method to yield what the authors call the ``Monte Carlo Finite Element Method" (MCFEM) which can be used to estimate the expectation and variance of $u(x)$. This method essentially involves drawing \textit{iid} samples from the random fields in the BVP and then applying the FEM element to the resulting elliptic BVPs.

Following this a variational formulation on $D\times\Omega$ is instead considered, where $\Omega$ is the underlying sample space for the probability space where the random fields live. The associated weak form is not a convenient starting point for Galerkin approximation as it involves taking expectations with respect to the abstract set $\Omega$ and the associated probability measure.
This leads the authors to instead consider that the noise arises from a finite number of random-variables (i.e. the random fields are so called \textit{finite-dimensional noise}). Doing so yields an equivalent weak form on $D\times\Gamma$ where $\Gamma$ is the range of the finite-dimensional noise. Having done this a Stochastic Galerkin FEM is developed to approximate the solution to this new weak form. Both a semi-discrete and fully-discrete version are considered (discretization can now occur in two spaces). After analysing this method the authors finally consider a stochastic collocation FEM which combines collocation on the range of the finite-dimensional noise and FEM approximations on $D$.

It should be pointed out that these methods are quite computationally expensive and become infeasible when the dimension (of both the deterministic and random spaces) increases past 4. Some work has been done on sparse deterministic-stochastic tensor Galerkin finite element methods (sparse sGFEMS) \textcolor{blue}{\citep{bieri2010sparse}}. This method aims to reduce computational complexity by using hierarchic sequences of finite-dimensional approximation spaces to yield sparse tensor product spaces. Sparse tensor discretizations of high-dimensional parametric and stochastic PDEs have also been considered in \textcolor{blue}{\citep{schwab2011sparse}}. Here the authors consider tensorized operator equations for spatial and temporal $k$-point correlation functions of the random solutions to sPDEs. Sparse tensor products of hierarchical (multi-level) discretizations in physical space and time are shown here to converge at rates independent of the dimension of the parameter space. This paper also presents a convergence analysis of multi-level Monte Carlo discretizations of PDEs with random coefficients.

Probabilistic numerical methods for PDEs are more uncommon. However, some methods do exist which we now briefly outline. \textcolor{blue}{\citep{conrad2017statistical}} includes some discussion of how to randomise FEM basis functions in a similar spirit to how they randomise the numerical flow map for ODE solvers, while in \textcolor{blue}{\citep{cockayne2016probabilistic}} a probabilistic numerical method to solve the strong formulation of a PDE (as opposed to the weak form considered in \textcolor{blue}{\citep{conrad2017statistical}}) is proposed. This method begins with a prior distribution over the solution space of the PDE which is then restricted to a subset of the solution space by utilising information about the true solution of the PDE. This can be viewed as imposing the governing equations of the PDE at a finite number of locations in the domain of interest. The choice of where to impose the equations is what constitutes the discretization of the PDE and this restriction yields the posterior distribution on the solution. Another approach is taken in \textcolor{blue}{\citep{chkrebtii2016bayesian}} where the authors deal with parabolic PDEs in a similar manner to how they deal with ODEs. In particular, they define a probability measure over multivariate tragectories, as well as on derivatives with respect to spatial inputs, to model prior belief about the unknown solution and partial derivatives of the parabolic BVP. Then a similar updating scheme is used which correpsonds to a generalisation of the procedure the authors take for ODEs.

We now give a brief account of additional background material necessary for this project. In particular, it was necessary to learn about methods to approximate solutions of partial differential equations, specifically parabolic and elliptic PDEs. We focused on Finite Element based approaches and sought to find a Bayesian formulation of the conditions which give rise to FEM approximations. As such a proper understanding of Finite Element Methods was necessary; the two main references we consulted for this were \textcolor{blue}{\citep{lord2014introduction}} and \textcolor{blue}{\citep{larsson2008partial}}.

A thorough understanding of Gaussian processes/measures was also required. It is well known that we cannot have a Lebesgue measure on an infinite-dimensional space; Gaussian measures provide a widely used substitute \textcolor{blue}{\citep{da2006introduction}}. Since solutions to PDEs lie in infinite-dimensional function spaces Gaussian measures will prove very useful for our purposes in this project. GPs can be viewed as an infinite-dimensional extension of classical normal random variables. GPs have several advantages \textcolor{blue}{\citep{raissi2017machine}}:
\begin{itemize}
    \item they provide a flexible way to model prior belief over function spaces
    \item computations involving them are often analytically tractable
    \item they provide a fully probabilistic work-flow which returns robust posterior variance estimates; allowing uncertainty to be quantified in a natural way
\end{itemize}
There are two main views on how to work with Gaussian processes, one focuses on the covariance function of the GP \textcolor{blue}{\citep{Rasmussen06gaussianprocesses}} whereas the other focuses on the associated covariance operator \textcolor{blue}{\citep{da2006introduction,lifshits2012lectures,lunardi2015infinite}}. Both viewpoints will prove useful in the sequel, though in general it is slightly cumbersome to switch between the two. The main references we followed for GPs include \textcolor{blue}{\citep{da2006introduction,Rasmussen06gaussianprocesses,lifshits2012lectures,lunardi2015infinite}}.

Ideas from the data assimilation and filtering literature were also very useful. Data assimilation involves the combination of two sources of information:
\begin{itemize}
    \item a mathematical model of the physical system or a numerical implementation of this model
    \item observations of the system, typically corrupted by noise
\end{itemize}
The objective of data assimilation \textcolor{blue}{\citep{law2015data}} is to combine these two sources of information in order to obtain a more accurate and complete estimate of the system's true state. Doing so will often allow more accurate predictions of the system's future trajectory and can also lead to more accurate uncertainty quantification. Data assimilation methods are usually Bayesian, since the current knowledge of the state of the system can be thought of as a prior and the incorporation of the model dynamics and observations can be considered to be the ``data" with which we condition on to obtain a posterior. Often it is desirable for \textit{real time} data assimilation as well as for reasonable computational costs and as such there are two main ideas behind filtering/data assimilation:
\begin{itemize}
    \item knowledge about the posterior should be built up sequentially
    \item the unknown state should be split into parts and knowledge should be built up for each of these parts sequentially
\end{itemize}
The first point seeks to improve overall efficiency and the second helps to reduce the dimensionality of each computational problem. The main references consulted for data assimilation/filtering were \textcolor{blue}{\citep{law2015data,sullivan2015introduction,sarkka2013bayesian}}. Another reference which proved invaluable was \textcolor{blue}{\citep{stuart2010inverse}} which provided much insight into how to properly perform Bayesian Inference in the function space setting.
