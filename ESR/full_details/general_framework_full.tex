We now introduce the general framework we will be considering. This will be split into two parts. In the first we shall be considering spatial problems without any time dependence, while in the second part we will consider problems with time dependence. We aim to describe how the two priors will be formed and then discuss how further inference will be carried out when observational data from the physical system is available.


\subsection{Spatial Boundary Value Problems}

We first consider PDEs which do not have any time dependence. Let $\Omega$ be a domain with boundary $\partial\Omega$ and let $\mathcal{L}$ be a suitable linear differential operator. We will focus on the following Dirichlet problem:
\begin{equation}
\label{DirichletProb}
    \left\{
    \begin{array}{cc}
        \mathcal{L}u=f & \text{on } \Omega  \\
         u = g  & \text{ on } \partial\Omega
    \end{array}
    \right.
\end{equation}
where the functions $f,g$ may be noisy, typically Gaussian. The operator $\mathcal{L}$ may also be random. For simplicity we will take $g=0$ and will assume that $\mathcal{L}$ is deterministic. We will also now assume that $f$ is Gaussian, i.e. $f\sim\mathcal{N}(\bar{f},K)$. We seek the solution in some appropriate Hilbert space of functions $\mathcal{H}$. We now discuss how the two priors over the solution space are formed. The first prior will essentially come from the true solution of the PDE while the second will be the ``output" of a probabilistic numerical method\footnote{Note that in our discussion of probabilistic numerical methods we referred to the output as being a posterior distribution over the solution of the problem; here we are viewing the output as what we will set our prior to be for further inference.} for solving the problem (\ref{DirichletProb}). As such we will initially place an appropriate prior on the solution $u$ which encapsulates our prior belief on the solution (such as its smoothness) before utilising detailed knowledge of the specific PDE. We will then consider using an approximation to obtain a posterior distribution which will then become our prior for further inference. We start with utilising the true solution:

\subsubsection{Prior from the true solution}

Formally we have $u=\mathcal{L}^{-1}f$, where $\mathcal{L}^{-1}$ is the solution operator (Green's function) for the problem (\ref{DirichletProb}). Since $f$ is random we see that the solution is also random; in particular it is the push-forward of $f$ by the \textbf{linear} operator $\mathcal{L}^{-1}$. Provided that the linear operator $\mathcal{L}^{-1}$ is bounded it follows from the theory of Gaussian measures (see Proposition 1.18 of \textcolor{blue}{\cite{da2006introduction}}) that $u$ is also Gaussian and in particular has the following distribution:
\begin{equation}
    \label{true_prior}
    u\sim\mathcal{N}(\mathcal{L}^{-1}\bar{f},\mathcal{L}^{-1}K(\mathcal{L}^{-1})^{*})
\end{equation}
This will be one of the two priors we will consider. One should note that this prior indeed utilises the true solution to the PDE. However, for most PDEs it will be intractable to actually compute this prior due to the fact that $\mathcal{L}^{-1}$ is inaccessible. This leads us instead to consider a prior based on a probabilistic numerical method for solving (\ref{DirichletProb}):

\subsubsection{Prior from an approximation}

We will now consider placing an initial Gaussian prior on the solution $u$, $u\sim\mathcal{N}(0,V)$. The covariance operator $V$ will be chosen so as to satisfy the following assumption:

\noindent \textbf{Assumption 1} $V$ is controlled so that $u$ lies almost surely in some appropriate subspace $\mathcal{U}\subset{H}$ where the linear operator $\mathcal{L}$ restricted to this subspace is bounded\footnote{$\mathcal{L}$, being a differential operator, is in general an unbounded operator when viewed on the whole function space $\mathcal{H}$.}, i.e. $\mathcal{L}:\mathcal{U}\rightarrow\mathcal{U}^{\prime}$ is bounded where $\mathcal{U}\subset\mathcal{H}\subset{\mathcal{U}^{\prime}}$.
\begin{remark}
    We shall assume here that the subspace $\mathcal{U}$ is reflexive so that $\mathcal{U}^{\prime\prime}$ can be identified with $\mathcal{U}$.
\end{remark}

Having placed this prior on $u$ we will now consider conditioning on the ``observation" of a suitable information operator in order to form a posterior distribution which will be taken to be our new prior. This information operator will involve the following discretization $\mathcal{F}_{h}$ of the function space $\mathcal{H}$, where $h$ is the mesh size. In particular, we will consider the following information operators:
\begin{equation*}
    I_{j}\boldsymbol{\cdot} = \int_{\Omega}\phi_{j}\boldsymbol{\cdot}\mathrm{d}x, \hspace{0.25cm} j=1,\dots,J
\end{equation*}
where $\phi_{j}\in\mathcal{F}_{h}$ for $j=1,\dots,J$. Note that $J$ is inversely proportional to the mesh size $h$, in particular if $\Omega\subset\mathbb{R}^{2}$ then $J\propto 1/h^2$.
\begin{remark}
    Think of the $\{\phi_{j}\}$ as a basis for finite element spaces.
\end{remark}
Let $\mathcal{I}$ be the concatenation of these information operators, i.e. $\mathcal{I}=(I_1,\dots,I_J)^{T}$. We will refer to $\mathcal{I}$ as the information operator. This information operator essentially involves taking the inner product against the finite element basis functions. The probabilistic numerical method will involve conditioning on the ``observation" of this information operator. To be more specific consider $\mathcal{I}$ acting on the PDE $\mathcal{L}u=f$. We have,
\begin{equation*}
    \mathcal{I}\mathcal{L}u=\mathcal{I}f=:F
\end{equation*}
where both $\mathcal{I}\mathcal{L}u$ and $F$ are vectors in $\mathbb{R}^{J}$. Since, in theory, we know the properties of the forcing term $f$ we have access to $F$. Our second prior will thus be the distribution of $u$ conditional on ``observing" $\mathcal{I}\mathcal{L}u=F$. In order to work out what this distribution is we will first consider the joint distribution of $(u,\mathcal{I}\mathcal{L}u)^{T}$. It is here that Assumption 1 is necessary, since we will utilise the fact that, under our prior, $u$ lies almost surely in a subspace where the restriction of $\mathcal{L}$ is bounded. This implies that the joint distribution is as follows:
\begin{equation}
    \label{jointInfoDist}
    \begin{pmatrix}
    u \\
    \mathcal{I}\mathcal{L}u
    \end{pmatrix} =
    \begin{pmatrix}
    I \\ \mathcal{I}\mathcal{L}
    \end{pmatrix} u \sim \mathcal{N}\left(
    \begin{pmatrix}
    0 \\
    \mathbf{0}_J
    \end{pmatrix},
    \begin{pmatrix}
    V & V\mathcal{L}^{*}\mathcal{I}^{*} \\
    \mathcal{I}\mathcal{L}V & \mathcal{I}\mathcal{L}V\mathcal{L}^{*}\mathcal{I}^{*}
    \end{pmatrix}
    \right)
\end{equation}
In order to progress we will now consider that we fix a realisation of $f$ (and so the corresponding $F$ is also fixed). Doing so allows us to perform the conditioning step (see the \textcolor{blue}{\nameref{appendix}} for a full justification of this conditioning step) yielding the Bayesian update:
\begin{equation}
    \label{conditionalDistnFixed_f}
    u|\{\mathcal{I}\mathcal{L}u=F,f\}\sim\mathcal{N}(a,\Sigma)=:\mu_{a,\Sigma}
\end{equation}
where,
\begin{equation}
    \label{post_mean_before_averaging}
    a = V\mathcal{L}^{*}\mathcal{I}^{*}(\mathcal{I}\mathcal{L}V\mathcal{L}^{*}\mathcal{I}^{*})^{-1}F
\end{equation}
\begin{equation}
    \label{post_var_before_averaging}
    \Sigma = V - V\mathcal{L}^{*}\mathcal{I}^{*}(\mathcal{I}\mathcal{L}V\mathcal{L}^{*}\mathcal{I}^{*})^{-1}\mathcal{I}\mathcal{L}V
\end{equation}

\begin{remark}
We use the notation $\mu_{m,A}$ to denote a Gaussian measure (in arbitrary dimensions) with mean $m$ and covariance $A$. The space (and hence the dimension) should be clear from the domain of the covariance operator.
\end{remark}

We will now seek to marginalize over $f$ in (\ref{conditionalDistnFixed_f}}) so as to obtain the ``average" conditional distribution over all possible values of the forcing. This will give us our second prior. To do so we first note:
\begin{equation}
    f\sim\mathcal{N}(\bar{f},K)=:\mu_{\bar{f},K} \implies \mathcal{I}f=F\sim\mathcal{N}(\mathcal{I}\bar{f},\mathcal{I}K\mathcal{I}^{*})=:\mu_{\bar{F},K_{\mathcal{I}}}
\end{equation}
where $\bar{F}:=\mathcal{I}\bar{f}$ and $K_{\mathcal{I}}:=\mathcal{I}K\mathcal{I}^{*}$. In order to specify the ``average" conditional distribution it suffices to compute the expectation of arbitrary bounded cylindrical test functions $\psi(u^{N}):=\psi(u(x_1),\dots,u(x_{N}))$ since the $\sigma-$algebra generated by cylinder sets coincides with the Borel $\sigma-$algebra (see for instance Theorem 2.1.1 in \textcolor{blue}{\cite{lunardi2015infinite}}). We must thus compute:
\begin{equation}
    \int\int\psi(u^{N})\mu_{a,\Sigma}(\mathrm{d}u)\mu_{\bar{f},K}(\mathrm{d}f)
\end{equation}
Note that $u^{N}=Pu$ where the bounded linear operator $P:\mathcal{H}\rightarrow\mathbb{R}^{N}$ is defined by $Ph:=(h(x_1),\dots,h(x_N))^T$ for any function $h$. Thus, $u^{N}$ is multivariate normal, i.e., $u^{N}\sim\mathcal{N}(Pa,P\Sigma P^{*})=:\mu_{Pa,\Sigma_{N}}$ where $\Sigma_{N}:=P\Sigma P^{*}$ is the $N\times N$ covariance matrix of $u^{N}$. We thus have:
\begin{equation}
    \int\int\psi(u^{N})\mu_{a,\Sigma}(\mathrm{d}u)\mu_{\bar{f},K}(\mathrm{d}f) = \int\int\psi(u^{N})\mu_{Pa,\Sigma_N}(\mathrm{d}u^{N})\mu_{\bar{f},K}(\mathrm{d}f)
\end{equation}
We note that $Pa=PAF$ where $A:= V\mathcal{L}^{*}\mathcal{I}^{*}(\mathcal{I}\mathcal{L}V\mathcal{L}^{*}\mathcal{I}^{*})^{-1}$. Since the conditional distribution of $u$, (\ref{conditionalDistnFixed_f}), and hence of $u^{N}$ only depends on $f$ through $F\in\mathbb{R}^{J}$ we can thus write:
\begin{equation}
    \int\int\psi(u^{N})\mu_{Pa,\Sigma_N}(\mathrm{d}u^{N})\mu_{\bar{f},K}(\mathrm{d}f)=\int\int\psi(u^{N})\mu_{PAF,\Sigma_N}(\mathrm{d}u^{N})\mu_{\bar{F},K_{\mathcal{I}}}(\mathrm{d}F)
\end{equation}
Both measures in the above integral are now multivariate normal and so we have:
\begin{align}
    \int\int&\psi(u^{N})\mu_{Pa,\Sigma_N}(\mathrm{d}u^{N})\mu_{\bar{F},K_{\mathcal{I}}}(\mathrm{d}F)= \nonumber \\ &=\frac{1}{Z_u}\int\int\psi(u^{N})\exp\left(-\frac{1}{2}\left\langle u^{N}-PAF,\Sigma_{N}^{-1}(u^{N}-PAF) \right\rangle\right)\mu_{\bar{F},K_{\mathcal{I}}}(\mathrm{d}F)\mathrm{d}u^{N} \nonumber \\
    &=\frac{1}{Z_{u}Z_{f}}\int\int\psi(u^{N})\exp\left(-\frac{1}{2}\left\langle u^{N}-PAF,\Sigma_{N}^{-1}(u^{N}-PAF)\right\rangle\right)\exp\left(-\frac{1}{2}\left\langle F-\bar{F},K_{\mathcal{I}}^{-1}(F-\bar{F}) \right\rangle\right)\mathrm{d}F\mathrm{d}u^{N}
\end{align}
where the normalization constants are $Z_u:=(2\pi)^{N/2}\det(\Sigma_N)^{1/2}$ and $Z_f:=(2\pi)^{J/2}\det(K_\mathcal{I})^{1/2}$. We now need to compute the integral over $F$. In order to do so we combine the exponents in (11) into a quadratic in $F$ in order to be able to use the well-known formula of a multidimensional Gaussian integral. Going through the algebra we obtain:
\begin{align}
    \frac{1}{Z_{u}Z_{f}}&\int\int\psi(u^{N})\exp\Bigg(-\frac{1}{2}\Big(\left\langle u^{N},\Sigma_{N}^{-1}u^{N} \right\rangle - 2\left\langle \Sigma_{N}^{-1}PAF,u^{N}\right\rangle + \left\langle F,A^{*}P^{*}\Sigma_{N}^{-1}PAF\right\rangle \nonumber \\
    &+ \left\langle F,K_{\mathcal{I}}^{-1}F\right\rangle - 2 \left\langle K_{\mathcal{I}}^{-1}\bar{F},F \right\rangle + \left\langle \bar{F},K_{\mathcal{I}}^{-1}\bar{F}\right\rangle \Big)\Bigg)\mathrm{d}F\mathrm{d}u^{N} = \nonumber \\
    &=\frac{1}{Z_{u}Z_{f}}\int\psi(u^{N})\exp\left(-\frac{1}{2}\left(\left\langle u^{N},\Sigma_{N}^{-1} u^{N} \right\rangle + \left\langle \bar{F},K_{\mathcal{I}}^{-1}\bar{F} \right\rangle\right)\right) \cdot \nonumber \\
    &\left(\int\exp\left(-\frac{1}{2}\left\langle F,BF \right\rangle + \left\langle A^{*}P^{*}\Sigma_{N}^{-1}u^{N} + K_{\mathcal{I}}^{-1}\bar{F},F \right\rangle\right)\mathrm{d}F\right)\mathrm{d}u^{N}
\end{align}
where $B:=\left(A^{*}P^{*}\Sigma_{N}^{-1}PA+K_{\mathcal{I}}^{-1}\right)$. Computing the inner integral over $F$ we thus obtain:
\begin{align}
    \frac{(2\pi)^{J/2}}{Z_{u}Z_{f}\det(B)^{1/2}}\int\psi(u^{N})\exp&\left(\frac{1}{2}\left\langle A^{*}P^{*}\Sigma_{N}^{-1}u^{N} + K_{\mathcal{I}}^{-1}\bar{F}, B^{-1}\left(A^{*}P^{*}\Sigma_{N}^{-1}u^{N} + K_{\mathcal{I}}^{-1}\bar{F}\right) \right\rangle\right. \nonumber \\
    &\left.-\frac{1}{2}\left(\left\langle u^{N}, \Sigma_{N}^{-1}u^{N}\right\rangle + \left\langle \bar{F},K_{\mathcal{I}}^{-1}\bar{F} \right\rangle \right)\right)\mathrm{d}u^{N}
\end{align}
We now focus on the terms in the exponent and simplify these as follows:
\begin{align}
    &\frac{1}{2}\left\langle A^{*}P^{*}\Sigma_{N}^{-1}u^{N} + K_{\mathcal{I}}^{-1}\bar{F}, B^{-1}\left(A^{*}P^{*}\Sigma_{N}^{-1}u^{N} + K_{\mathcal{I}}^{-1}\bar{F}\right) \right\rangle -\frac{1}{2}\left(\left\langle u^{N}, \Sigma_{N}^{-1}u^{N}\right\rangle + \left\langle \bar{F},K_{\mathcal{I}}^{-1}\bar{F} \right\rangle \right) =  \nonumber \\
    &=-\frac{1}{2}\Big( \left\langle u^{N}, \Sigma_{N}^{-1}u^{N}\right\rangle + \left\langle \bar{F}, K_{\mathcal{I}}^{-1}\bar{F} \right\rangle - \left\langle A^{*}P^{*}\Sigma_{N}^{-1}u^{N} + K_{\mathcal{I}}^{-1}\bar{F}, B^{-1}\left(A^{*}P^{*}\Sigma_{N}^{-1}u^{N} + K_{\mathcal{I}}^{-1}\bar{F}\right) \right\rangle \Big) \nonumber \\
    &=-\frac{1}{2}\Big(\left\langle u^{N}, \Sigma_{N}^{-1}u^{N}\right\rangle + \left\langle \bar{F}, K_{\mathcal{I}}^{-1}\bar{F} \right\rangle - \left\langle u^{N},\Sigma_{N}^{-1}PAB^{-1}A^{*}P^{*}\Sigma_{N}^{-1}u^{N} \right\rangle \nonumber \\
    &- 2\left\langle u^{N},\Sigma_{N}^{-1}PAB^{-1}K_{\mathcal{I}}^{-1}\bar{F} \right\rangle - \left\langle \bar{F}, K_{\mathcal{I}}^{-1}B^{-1}K_{\mathcal{I}}^{-1}\bar{F} \right\rangle \Big) \nonumber \\
    &= -\frac{1}{2}\Big( \left\langle u^{N}, (\Sigma_{N}^{-1} - \Sigma_{N}^{-1}PAB^{-1}A^{*}P^{*}\Sigma_{N}^{-1})u^{N} \right\rangle - 2 \left\langle u^{N},\Sigma_{N}^{-1}PAB^{-1}K_{\mathcal{I}}^{-1}\bar{F} \right\rangle + \left\langle \bar{F}, (K_{\mathcal{I}}^{-1}-K_{\mathcal{I}}^{-1}B^{-1}K_{\mathcal{I}}^{-1})\bar{F} \right\rangle \Big) \nonumber \\
    &=-\frac{1}{2}\Big( \left\langle u^{N}, \Sigma_{\mathcal{I}}^{-1}u^{N} \right\rangle - 2 \left\langle u^{N},\Sigma_{N}^{-1}PAB^{-1}K_{\mathcal{I}}^{-1}\bar{F} \right\rangle + \left\langle \bar{F}, (K_{\mathcal{I}}^{-1}-K_{\mathcal{I}}^{-1}B^{-1}K_{\mathcal{I}}^{-1})\bar{F} \right\rangle \Big) \label{exponent}
\end{align}
where $\Sigma_{\mathcal{I}}:=\Sigma_{N} + PAK_{\mathcal{I}}A^{*}P^{*}$. The inverse of $\Sigma_{\mathcal{I}}$ is indeed the coefficient matrix for the quadratic term in $u^{N}$ in (\ref{exponent}). This can be seen by utilizing the Woodbury matrix identity as follows:
\begin{align*}
    \Sigma_{\mathcal{I}}^{-1}&=(\Sigma_{N} + PAK_{\mathcal{I}}A^{*}P^{*})^{-1} \\
    &=\Sigma_{N}^{-1}-\Sigma_{N}^{-1}PA(K_{\mathcal{I}}^{-1}+A^{*}P^{*}\Sigma_{N}^{-1}PA)^{-1}A^{*}P^{*}\Sigma_{N}^{-1} \\
    &=\Sigma_{N}^{-1}-\Sigma_{N}^{-1}PAB^{-1}A^{*}P^{*}\Sigma_{N}^{-1}
\end{align*}
We now complete the square (in terms of $u^{N}$) to obtain:
\begin{align}
    -&\frac{1}{2}\Big(\left\langle u^{N}-h^{N},\Sigma_{\mathcal{I}}^{-1}(u^{N}-h^{N}) \right\rangle + \left\langle \bar{F}, (K_{\mathcal{I}}^{-1}-K_{\mathcal{I}}^{-1}B^{-1}K_{\mathcal{I}}^{-1})\bar{F} \right\rangle - \left\langle \Sigma_{N}^{-1}PAB^{-1}K_{\mathcal{I}}^{-1}\bar{F},\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PAB^{-1}K_{\mathcal{I}}^{-1}\bar{F} \right\rangle \Big) = \nonumber \\
    &=-\frac{1}{2}\Big(\left\langle u^{N}-h^{N},\Sigma_{\mathcal{I}}^{-1}(u^{N}-h^{N}) \right\rangle + \left\langle \bar{F}, (K_{\mathcal{I}}^{-1}-K_{\mathcal{I}}^{-1}B^{-1}K_{\mathcal{I}}^{-1}-K_{\mathcal{I}}^{-1}B^{-1}A^{*}P^{*}\Sigma_{N}^{-1}\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PAB^{-1}K_{\mathcal{I}}^{-1})\bar{F} \right\rangle\Big)
\end{align}
where $h^{N}:=\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PAB^{-1}K_{\mathcal{I}}^{-1}\bar{F}$. We now show that the term quadratic in $\bar{F}$ vanishes by showing that the coefficient matrix of $\bar{F}$ is equal to the zero matrix. Note that this coefficent matrix can be rewritten as:
\begin{align*}
    &K_{\mathcal{I}}^{-1}-K_{\mathcal{I}}^{-1}B^{-1}K_{\mathcal{I}}^{-1}-K_{\mathcal{I}}^{-1}B^{-1}A^{*}P^{*}\Sigma_{N}^{-1}\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PAB^{-1}K_{\mathcal{I}}^{-1} = \\
    &=K_{\mathcal{I}}^{-1}K_{\mathcal{I}}K_{\mathcal{I}}^{-1}-K_{\mathcal{I}}^{-1}B^{-1}K_{\mathcal{I}}^{-1}-K_{\mathcal{I}}^{-1}B^{-1}A^{*}P^{*}\Sigma_{N}^{-1}\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PAB^{-1}K_{\mathcal{I}}^{-1} \\
    &= K_{\mathcal{I}}^{-1}(K_{\mathcal{I}}-B^{-1}-B^{-1}A^{*}P^{*}\Sigma_{N}^{-1}\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PAB^{-1})K_{\mathcal{I}}^{-1} \\
    &=K_{\mathcal{I}}^{-1}B^{-1}(BK_{\mathcal{I}}B-B-A^{*}P^{*}\Sigma_{N}^{-1}\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PA)B^{-1}K_{\mathcal{I}}^{-1}
\end{align*}
and so showing that it is the zero matrix is equivalent to showing that $(BK_{\mathcal{I}}B-B-A^{*}P^{*}\Sigma_{N}^{-1}\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PA)$ is the zero matrix. This can be shown as follows:
\begin{align*}
    &BK_{\mathcal{I}}B-B-A^{*}P^{*}\Sigma_{N}^{-1}\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PA = \\
    &=(A^{*}P^{*}\Sigma_{N}^{-1}PA+K_{\mathcal{I}}^{-1})K_{\mathcal{I}}B-B-A^{*}P^{*}\Sigma_{N}^{-1}\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PA \\
    &=A^{*}P^{*}\Sigma_{N}^{-1}PAK_{\mathcal{I}}B+B-B-A^{*}P^{*}\Sigma_{N}^{-1}\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PA \\
    &=A^{*}P^{*}\Sigma_{N}^{-1}PAK_{\mathcal{I}}(A^{*}P^{*}\Sigma_{N}^{-1}PA+K_{\mathcal{I}}^{-1})-A^{*}P^{*}\Sigma_{N}^{-1}\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PA \\
    &=A^{*}P^{*}\Sigma_{N}^{-1}PAK_{\mathcal{I}}A^{*}P^{*}\Sigma_{N}^{-1}PA + A^{*}P^{*}\Sigma_{N}^{-1}PA - A^{*}P^{*}\Sigma_{N}^{-1}\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PA \\
    &= A^{*}P^{*}\Sigma_{N}^{-1}(PAK_{\mathcal{I}}A^{*}P^{*}+\Sigma_{N}-\Sigma_{\mathcal{I}})\Sigma_{N}^{-1}PA = 0
\end{align*}
where the last equality follows by the definition of $\Sigma_{\mathcal{I}}$. Thus, our integral simplifies to:
\begin{equation}
    \frac{(2\pi)^{J/2}}{Z_{u}Z_{f}\det(B)^{1/2}}\int\psi(u^{N})\exp\Big(-\frac{1}{2}\left\langle u^{N}-h^{N},\Sigma_{\mathcal{I}}^{-1}(u^{N}-h^{N}) \right\rangle\Big)\mathrm{d}u^{N}
\end{equation}
We now focus on simplifying the normalizing constants in front of the integral:
\begin{align*}
    \frac{(2\pi)^{J/2}}{Z_{u}Z_{f}\det(B)^{1/2}}&=\frac{(2\pi)^{J/2}}{(2\pi)^{N/2}\det(\Sigma_N)^{1/2}(2\pi)^{J/2}\det(K_\mathcal{I})^{1/2}\det(B)^{1/2}}= \\
    &=\frac{1}{(2\pi)^{N/2}\det(\Sigma_N)^{1/2}\det(K_\mathcal{I})^{1/2}\det(B)^{1/2}}
\end{align*}
To proceed we note that $\det{B}$ can be rewritten as follows:
\begin{align*}
    \det(B)&=\det(A^{*}P^{*}\Sigma_{N}^{-1}PA + K_{\mathcal{I}}^{-1}) \\
    &=\det(K_{\mathcal{I}}^{-1}(I+K_{\mathcal{I}}A^{*}P^{*}\Sigma_{N}^{-1}PA)) \\
    &=\det(K_{\mathcal{I}}^{-1})\det(I+(K_{\mathcal{I}}A^{*}P^{*})(\Sigma_{N}^{-1}PA))) \\
    &=\det(K_{\mathcal{I}})^{-1}\det(I+\Sigma_{N}^{-1}PAK_{\mathcal{I}}A^{*}P^{*})
\end{align*}
where we have utilized Sylvester's determinant theorem (\textit{note: the identity matrices in the last two lines are of different sizes}). We can now finish up the simplification of the constants outside the integral:
\begin{align*}
    &\frac{1}{(2\pi)^{N/2}\det(\Sigma_N)^{1/2}\det(K_\mathcal{I})^{1/2}\det(B)^{1/2}} \\
    &=\frac{1}{(2\pi)^{N/2}\det(\Sigma_N)^{1/2}\det(K_\mathcal{I})^{1/2}\det(K_{\mathcal{I}})^{-1/2}\det(I_{N}+\Sigma_{N}^{-1}PAK_{\mathcal{I}}A^{*}P^{*})^{1/2}} \\
    &= \frac{1}{(2\pi)^{N/2}\det(\Sigma_{N} + PAK_{\mathcal{I}}A^{*}P^{*})^{1/2}} \\
    &=\frac{1}{(2\pi)^{N/2}\det(\Sigma_{\mathcal{I}})^{1/2}}
\end{align*}
Thus, our integral becomes:
\begin{align}
    \int&\psi(u^{N})\frac{1}{(2\pi)^{N/2}\det(\Sigma_{\mathcal{I}})^{1/2}}\exp\left(-\frac{1}{2}\left\langle u^{N}-h^{N}, \Sigma_{\mathcal{I}}^{-1}(u^{N}-h^{N}) \right\rangle\right)\mathrm{d}u^N = \nonumber \\
    =&\int\psi(u^{N})\mu_{h^{N},\Sigma_{\mathcal{I}}}(\mathrm{d}u^N)
\end{align}
from which we see that we have obtained the expectation of $\psi$ w.r.t. a multivariate Gaussian with mean and covariance given by:
\begin{equation}
    h^{N}:=\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PAB^{-1}K_{\mathcal{I}}^{-1}\bar{F}=PA\bar{F}
\end{equation}
\begin{equation}
    \Sigma_{\mathcal{I}}=P(\Sigma+AK_{\mathcal{I}}A^{*})P^{*}
\end{equation}

\noindent Note that we have simplified the mean $h^N$ of this multivariate Gaussian as follows:
\begin{align*}
    h^N &= \Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PAB^{-1}K_{\mathcal{I}}^{-1}\bar{F} \\
    &=(\Sigma_N+PAK_{\mathcal{I}}A^{*}P^{*})\Sigma_{N}^{-1}PA(K_{\mathcal{I}}B)^{-1}\bar{F} \\
    &=PA(K_{\mathcal{I}}B)^{-1}\bar{F}+PAK_{\mathcal{I}}A^{*}P^{*}\Sigma_{N}^{-1}PA(K_{\mathcal{I}}B)^{-1}\bar{F} \\
    &=PA(I+K_{\mathcal{I}}A^{*}P^{*}\Sigma_{N}^{-1}PA)(K_{\mathcal{I}}B)^{-1}\bar{F} \\
    &=PA(I+K_{\mathcal{I}}A^{*}P^{*}\Sigma_{N}^{-1}PA)(K_{\mathcal{I}}(K_{\mathcal{I}}^{-1}+A^{*}P^{*}\Sigma_{N}^{-1}PA))^{-1}\bar{F}\\
    &=PA(I+K_{\mathcal{I}}A^{*}P^{*}\Sigma_{N}^{-1}PA)(I+K_{\mathcal{I}}A^{*}P^{*}\Sigma_{N}^{-1}PA)^{-1}\bar{F}\\
    &=PA\bar{F}
\end{align*}

\noindent Thus, we conclude that ``averaging" over $f$ gives the following Gaussian posterior:
\begin{equation}
    \label{posteriorDist}
    \mathcal{N}\left(A\bar{F},\Sigma+AK_{\mathcal{I}}A^{*}\right)
\end{equation}
where we recall that the operator $A$ is $A= V\mathcal{L}^{*}\mathcal{I}^{*}(\mathcal{I}\mathcal{L}V\mathcal{L}^{*}\mathcal{I}^{*})^{-1}$.




\clearpage
