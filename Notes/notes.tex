\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{dsfont}
\usepackage{centernot}
\usepackage[a4paper,top=2.5cm,bottom=2.5cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% for theorems/lemmas/defs/etc
%-------------------------------
\usepackage[english]{babel}
\renewcommand\qedsymbol{$\blacksquare$}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\newtheorem{proposition}{Proposition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{corollary1}{Corollary}[definition]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{remark}
\newtheorem*{remark}{\textbf{Remark}}  % if don't want bold face for remark remove textbf
%-------------------------------

\DeclareMathOperator{\E}{\mathds{E}}
\DeclareMathOperator{\Var}{\mathrm{Var}}
\DeclareMathOperator{\prob}{\mathbb{P}}

\title{Gaussian Processes for PDEs}
\author{Yanni Papandreou}
\date{November 2019}

\usepackage{hyperref}
\hypersetup{colorlinks=false}

\begin{document}

\maketitle

\section{Notes 1}

Let $\Omega$ be a domain with boundary $\partial\Omega$ and let $\mathcal{L}$ be a suitable linear differential operator. We consider the following Dirichlet problem:
\begin{equation}
\label{DirichletProb}
    \left\{
    \begin{array}{cc}
        \mathcal{L}u=f & \text{on } \Omega  \\
         u = g  & \text{ on } \partial\Omega
    \end{array}
    \right.
\end{equation}
where the functions $f,g$ may be noisy, typically Gaussian. The operator $\mathcal{L}$ may also be random. For simplicity we will take $g=0$ and will assume that $\mathcal{L}$ is deterministic. We seek the solution in some appropriate Hilbert space of functions $\mathcal{H}$. Formally we have $u=\mathcal{L}^{-1}f$.

\begin{remark}
If $f$ is Gaussian, i.e. $f\sim\mathcal{N}(\bar{f},K)$, then $(u,f)$ is joint Gaussian:
\begin{equation}
\label{trueJoint}
    \begin{pmatrix}
        f \\
        u
    \end{pmatrix} \sim \mathcal{N}\left(
    \begin{pmatrix}
    \bar{f} \\
    \mathcal{L}^{-1}\bar{f}
    \end{pmatrix},
    \begin{pmatrix}
    K & K(\mathcal{L}^{-1})^{*} \\
    \mathcal{L}^{-1}K & \mathcal{L}^{-1}K(\mathcal{L}^{-1})^{*}
    \end{pmatrix}
    \right)
\end{equation}
provided that $\mathcal{L}^{-1}$ is bounded.
\end{remark}

Let us consider placing a Gaussian prior on $u$, $u\sim\mathcal{N}(0,V)$, where we control $V$ so that $u$ is almost surely in some appropriate subspace $\mathcal{U}\subset\mathcal{H}$ where the linear operator $\mathcal{L}$ restricted to this subspace is bounded. I.e. $\mathcal{L}:\mathcal{U}\rightarrow \mathcal{U}^{\prime}$ is bounded where $\mathcal{U}\subset\mathcal{H}\subset\mathcal{U}^{\prime}$.
\begin{remark}
    We shall assume here that the subspace $\mathcal{U}$ is reflexive so that $\mathcal{U}^{\prime\prime}$ can be identified with $\mathcal{U}$.
\end{remark}

Now consider the following information operators:
\begin{equation*}
    I_{j}\cdot=\int_{\Omega}\psi_{j}\cdot\mathrm{d}x, \hspace{0.5cm} j=1,\dots,J
\end{equation*}
where $\psi_{j}\in\mathcal{F}_{h}$ for $j=1,\dots,J$ where $\mathcal{F}_{h}$ is some discretisation of the function space $\mathcal{H}$ (and $h$ is the mesh size).
\begin{remark}
Think of the $\{\psi_{j}\}$ as a basis for the finite element spaces.
\end{remark}

Let $\mathcal{I}=(I_1,\dots,I_J)^T$. We have,
\begin{equation*}
    \mathcal{I}\mathcal{L}u=(I_{1}\mathcal{L}u,\dots,I_{J}\mathcal{L}u)\in\mathbb{R}^{J}
\end{equation*}
and under our assumptions on $V$ we have that,
\begin{equation}
    \label{jointInfoDist}
    \begin{pmatrix}
    u \\
    \mathcal{I}\mathcal{L}u
    \end{pmatrix} =
    \begin{pmatrix}
    I \\ \mathcal{I}\mathcal{L}
    \end{pmatrix} u \sim \mathcal{N}\left(
    \begin{pmatrix}
    0 \\
    \mathbf{0}_J
    \end{pmatrix},
    \begin{pmatrix}
    V & V\mathcal{L}^{*}\mathcal{I}^{*} \\
    \mathcal{I}\mathcal{L}V & \mathcal{I}\mathcal{L}V\mathcal{L}^{*}\mathcal{I}^{*}
    \end{pmatrix}
    \right)
\end{equation}

Under the assumption that (\ref{DirichletProb}) holds we have $\mathcal{I}\mathcal{L}u=\mathcal{I}f=:F$. Assuming now that we fix a realisation of $f$ (and so $F$ is fixed) we have by conditioning (see Notes 3 for a justification of why this is valid) that:
\begin{equation}
    \label{conditionalDistnFixed_f}
    u|\{\mathcal{I}\mathcal{L}u=F,f\}\sim\mathcal{N}(a,\Sigma)=:\mu_{a,\Sigma}
\end{equation}
where,
\begin{equation}
    \label{post_mean_before_averaging}
    a = V\mathcal{L}^{*}\mathcal{I}^{*}(\mathcal{I}\mathcal{L}V\mathcal{L}^{*}\mathcal{I}^{*})^{-1}F
\end{equation}
\begin{equation}
    \label{post_var_before_averaging}
    \Sigma = V - V\mathcal{L}^{*}\mathcal{I}^{*}(\mathcal{I}\mathcal{L}V\mathcal{L}^{*}\mathcal{I}^{*})^{-1}\mathcal{I}\mathcal{L}V
\end{equation}

\begin{remark}
We use the notation $\mu_{m,A}$ to denote a Gaussian measure (in arbitrary dimensions) with mean $m$ and covariance $A$. The space (and hence the dimension) should be clear from the domain of the covariance operator.
\end{remark}

We will now assume that $f$ is distributed as $f\sim\mathcal{N}(\bar{f},K)$ and we will seek to marginalize over $f$ in (\ref{conditionalDistnFixed_f}) so as to obtain the ``average" conditional distribution over all possible values of $f$. To do so we first note:
\begin{equation}
    f\sim\mathcal{N}(\bar{f},K)=:\mu_{\bar{f},K} \implies \mathcal{I}f=F\sim\mathcal{N}(\mathcal{I}\bar{f},\mathcal{I}K\mathcal{I}^{*})=:\mu_{\bar{F},K_{\mathcal{I}}}
\end{equation}
where $\bar{F}:=\mathcal{I}\bar{f}$ and $K_{\mathcal{I}}:=\mathcal{I}K\mathcal{I}^{*}$. In order to specify the ``average" conditional distribution it suffices to compute the expectation of arbitrary bounded cylindrical test functions $\phi(u^{N}):=\phi(u(x_1),\dots,u(x_{N}))$ since the $\sigma-$algebra generated by cylinder sets coincides with the Borel $\sigma-$algebra (see for instance Theorem 2.1.1 in \cite{lunardi2015infinite}). We must thus compute:
\begin{equation}
    \int\int\phi(u^{N})\mu_{a,\Sigma}(\mathrm{d}u)\mu_{\bar{f},K}(\mathrm{d}f)
\end{equation}
Note that $u^{N}=Pu$ where the bounded linear operator $P:\mathcal{H}\rightarrow\mathbb{R}^{N}$ is defined by $Ph:=(h(x_1),\dots,h(x_N))^T$ for any function $h$. Thus, $u^{N}$ is multivariate normal, i.e., $u^{N}\sim\mathcal{N}(Pa,P\Sigma P^{*})=:\mu_{Pa,\Sigma_{N}}$ where $\Sigma_{N}:=P\Sigma P^{*}$ is the $N\times N$ covariance matrix of $u^{N}$. We thus have:
\begin{equation}
    \int\int\phi(u^{N})\mu_{a,\Sigma}(\mathrm{d}u)\mu_{\bar{f},K}(\mathrm{d}f) = \int\int\phi(u^{N})\mu_{Pa,\Sigma_N}(\mathrm{d}u^{N})\mu_{\bar{f},K}(\mathrm{d}f)
\end{equation}
We note that $Pa=PAF$ where $A:= V\mathcal{L}^{*}\mathcal{I}^{*}(\mathcal{I}\mathcal{L}V\mathcal{L}^{*}\mathcal{I}^{*})^{-1}$. Since the conditional distribution of $u$ (\ref{conditionalDistnFixed_f}) and hence of $u^{N}$ only depends on $f$ through $F\in\mathbb{R}^{J}$ we can thus write:
\begin{equation}
    \int\int\phi(u^{N})\mu_{Pa,\Sigma_N}(\mathrm{d}u^{N})\mu_{\bar{f},K}(\mathrm{d}f)=\int\int\phi(u^{N})\mu_{Pa,\Sigma_N}(\mathrm{d}u^{N})\mu_{\bar{F},K_{\mathcal{I}}}(\mathrm{d}F)
\end{equation}
Both measures in the above integral are now multivariate normal and so we have:
\begin{align}
    \int\int&\phi(u^{N})\mu_{Pa,\Sigma_N}(\mathrm{d}u^{N})\mu_{\bar{F},K_{\mathcal{I}}}(\mathrm{d}F)= \nonumber \\ &=\frac{1}{Z_u}\int\int\phi(u^{N})\exp\left(-\frac{1}{2}\left\langle u^{N}-PAF,\Sigma_{N}^{-1}(u^{N}-PAF) \right\rangle\right)\mu_{\bar{F},K_{\mathcal{I}}}(\mathrm{d}F)\mathrm{d}u^{N} \nonumber \\
    &=\frac{1}{Z_{u}Z_{f}}\int\int\phi(u^{N})\exp\left(-\frac{1}{2}\left\langle u^{N}-PAF,\Sigma_{N}^{-1}(u^{N}-PAF)\right\rangle\right)\exp\left(-\frac{1}{2}\left\langle F-\bar{F},K_{\mathcal{I}}^{-1}(F-\bar{F}) \right\rangle\right)\mathrm{d}F\mathrm{d}u^{N}
\end{align}
where the normalization constants are $Z_u:=(2\pi)^{N/2}\det(\Sigma_N)^{1/2}$ and $Z_f:=(2\pi)^{J/2}\det(K_\mathcal{I})^{1/2}$. We now need to compute the integral over $F$. In order to do so we combine the exponents in (11) into a quadratic in $F$ in order to be able to use the well-known formula of a multidimensional Gaussian integral. Going through the algebra we obtain:
\begin{align}
    \frac{1}{Z_{u}Z_{f}}&\int\int\phi(u^{N})\exp\Bigg(-\frac{1}{2}\Big(\left\langle u^{N},\Sigma_{N}^{-1}u^{N} \right\rangle - 2\left\langle \Sigma_{N}^{-1}PAF,u^{N}\right\rangle + \left\langle F,A^{*}P^{*}\Sigma_{N}^{-1}PAF\right\rangle \nonumber \\
    &+ \left\langle F,K_{\mathcal{I}}^{-1}F\right\rangle - 2 \left\langle K_{\mathcal{I}}^{-1}\bar{F},F \right\rangle + \left\langle \bar{F},K_{\mathcal{I}}^{-1}\bar{F}\right\rangle \Big)\Bigg)\mathrm{d}F\mathrm{d}u^{N} = \nonumber \\
    &=\frac{1}{Z_{u}Z_{f}}\int\phi(u^{N})\exp\left(-\frac{1}{2}\left(\left\langle u^{N},\Sigma_{N}^{-1} u^{N} \right\rangle + \left\langle \bar{F},K_{\mathcal{I}}^{-1}\bar{F} \right\rangle\right)\right) \cdot \nonumber \\
    &\left(\int\exp\left(-\frac{1}{2}\left\langle F,BF \right\rangle + \left\langle A^{*}P^{*}\Sigma_{N}^{-1}u^{N} + K_{\mathcal{I}}^{-1}\bar{F},F \right\rangle\right)\mathrm{d}F\right)\mathrm{d}u^{N}
\end{align}
where $B:=\left(A^{*}P^{*}\Sigma_{N}^{-1}PA+K_{\mathcal{I}}^{-1}\right)$. Computing the inner integral over $F$ we thus obtain:
\begin{align}
    \frac{(2\pi)^{J/2}}{Z_{u}Z_{f}\det(B)^{1/2}}\int\phi(u^{N})\exp&\left(\frac{1}{2}\left\langle A^{*}P^{*}\Sigma_{N}^{-1}u^{N} + K_{\mathcal{I}}^{-1}\bar{F}, B^{-1}\left(A^{*}P^{*}\Sigma_{N}^{-1}u^{N} + K_{\mathcal{I}}^{-1}\bar{F}\right) \right\rangle\right. \nonumber \\
    &\left.-\frac{1}{2}\left(\left\langle u^{N}, \Sigma_{N}^{-1}u^{N}\right\rangle + \left\langle \bar{F},K_{\mathcal{I}}^{-1}\bar{F} \right\rangle \right)\right)\mathrm{d}u^{N}
\end{align}
We now focus on the terms in the exponent and simplify these as follows:
\begin{align}
    &\frac{1}{2}\left\langle A^{*}P^{*}\Sigma_{N}^{-1}u^{N} + K_{\mathcal{I}}^{-1}\bar{F}, B^{-1}\left(A^{*}P^{*}\Sigma_{N}^{-1}u^{N} + K_{\mathcal{I}}^{-1}\bar{F}\right) \right\rangle -\frac{1}{2}\left(\left\langle u^{N}, \Sigma_{N}^{-1}u^{N}\right\rangle + \left\langle \bar{F},K_{\mathcal{I}}^{-1}\bar{F} \right\rangle \right) =  \nonumber \\
    &=-\frac{1}{2}\Big( \left\langle u^{N}, \Sigma_{N}^{-1}u^{N}\right\rangle + \left\langle \bar{F}, K_{\mathcal{I}}^{-1}\bar{F} \right\rangle - \left\langle A^{*}P^{*}\Sigma_{N}^{-1}u^{N} + K_{\mathcal{I}}^{-1}\bar{F}, B^{-1}\left(A^{*}P^{*}\Sigma_{N}^{-1}u^{N} + K_{\mathcal{I}}^{-1}\bar{F}\right) \right\rangle \Big) \nonumber \\
    &=-\frac{1}{2}\Big(\left\langle u^{N}, \Sigma_{N}^{-1}u^{N}\right\rangle + \left\langle \bar{F}, K_{\mathcal{I}}^{-1}\bar{F} \right\rangle - \left\langle u^{N},\Sigma_{N}^{-1}PAB^{-1}A^{*}P^{*}\Sigma_{N}^{-1}u^{N} \right\rangle \nonumber \\
    &- 2\left\langle u^{N},\Sigma_{N}^{-1}PAB^{-1}K_{\mathcal{I}}^{-1}\bar{F} \right\rangle - \left\langle \bar{F}, K_{\mathcal{I}}^{-1}B^{-1}K_{\mathcal{I}}^{-1}\bar{F} \right\rangle \Big) \nonumber \\
    &= -\frac{1}{2}\Big( \left\langle u^{N}, (\Sigma_{N}^{-1} - \Sigma_{N}^{-1}PAB^{-1}A^{*}P^{*}\Sigma_{N}^{-1})u^{N} \right\rangle - 2 \left\langle u^{N},\Sigma_{N}^{-1}PAB^{-1}K_{\mathcal{I}}^{-1}\bar{F} \right\rangle + \left\langle \bar{F}, (K_{\mathcal{I}}^{-1}-K_{\mathcal{I}}^{-1}B^{-1}K_{\mathcal{I}}^{-1})\bar{F} \right\rangle \Big) \nonumber \\
    &=-\frac{1}{2}\Big( \left\langle u^{N}, \Sigma_{\mathcal{I}}^{-1}u^{N} \right\rangle - 2 \left\langle u^{N},\Sigma_{N}^{-1}PAB^{-1}K_{\mathcal{I}}^{-1}\bar{F} \right\rangle + \left\langle \bar{F}, (K_{\mathcal{I}}^{-1}-K_{\mathcal{I}}^{-1}B^{-1}K_{\mathcal{I}}^{-1})\bar{F} \right\rangle \Big) \label{exponent}
\end{align}
where $\Sigma_{\mathcal{I}}:=\Sigma_{N} + PAK_{\mathcal{I}}A^{*}P^{*}$. The inverse of $\Sigma_{\mathcal{I}}$ is indeed the coefficient matrix for the quadratic term in $u^{N}$ in (\ref{exponent}). This can be seen by utilizing the Woodbury matrix identity as follows:
\begin{align*}
    \Sigma_{\mathcal{I}}^{-1}&=(\Sigma_{N} + PAK_{\mathcal{I}}A^{*}P^{*})^{-1} \\
    &=\Sigma_{N}^{-1}-\Sigma_{N}^{-1}PA(K_{\mathcal{I}}^{-1}+A^{*}P^{*}\Sigma_{N}^{-1}PA)^{-1}A^{*}P^{*}\Sigma_{N}^{-1} \\
    &=\Sigma_{N}^{-1}-\Sigma_{N}^{-1}PAB^{-1}A^{*}P^{*}\Sigma_{N}^{-1}
\end{align*}
We now complete the square (in terms of $u^{N}$) to obtain:
\begin{align}
    -&\frac{1}{2}\Big(\left\langle u^{N}-h^{N},\Sigma_{\mathcal{I}}^{-1}(u^{N}-h^{N}) \right\rangle + \left\langle \bar{F}, (K_{\mathcal{I}}^{-1}-K_{\mathcal{I}}^{-1}B^{-1}K_{\mathcal{I}}^{-1})\bar{F} \right\rangle - \left\langle \Sigma_{N}^{-1}PAB^{-1}K_{\mathcal{I}}^{-1}\bar{F},\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PAB^{-1}K_{\mathcal{I}}^{-1}\bar{F} \right\rangle \Big) = \nonumber \\
    &=-\frac{1}{2}\Big(\left\langle u^{N}-h^{N},\Sigma_{\mathcal{I}}^{-1}(u^{N}-h^{N}) \right\rangle + \left\langle \bar{F}, (K_{\mathcal{I}}^{-1}-K_{\mathcal{I}}^{-1}B^{-1}K_{\mathcal{I}}^{-1}-K_{\mathcal{I}}^{-1}B^{-1}A^{*}P^{*}\Sigma_{N}^{-1}\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PAB^{-1}K_{\mathcal{I}}^{-1})\bar{F} \right\rangle\Big)
\end{align}
where $h^{N}:=\Sigma_{I}\Sigma_{N}^{-1}PAB^{-1}K_{\mathcal{I}}^{-1}\bar{F}$. We now show that the term quadratic in $\bar{F}$ vanishes by showing that the coefficient matrix of $\bar{F}$ is equal to the zero matrix. Note that this coefficent matrix can be rewritten as:
\begin{align*}
    &K_{\mathcal{I}}^{-1}-K_{\mathcal{I}}^{-1}B^{-1}K_{\mathcal{I}}^{-1}-K_{\mathcal{I}}^{-1}B^{-1}A^{*}P^{*}\Sigma_{N}^{-1}\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PAB^{-1}K_{\mathcal{I}}^{-1} = \\
    &=K_{\mathcal{I}}^{-1}K_{\mathcal{I}}K_{\mathcal{I}}^{-1}-K_{\mathcal{I}}^{-1}B^{-1}K_{\mathcal{I}}^{-1}-K_{\mathcal{I}}^{-1}B^{-1}A^{*}P^{*}\Sigma_{N}^{-1}\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PAB^{-1}K_{\mathcal{I}}^{-1} \\
    &= K_{\mathcal{I}}^{-1}(K_{\mathcal{I}}-B^{-1}-B^{-1}A^{*}P^{*}\Sigma_{N}^{-1}\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PAB^{-1})K_{\mathcal{I}}^{-1} \\
    &=K_{\mathcal{I}}^{-1}B^{-1}(BK_{\mathcal{I}}B-B-A^{*}P^{*}\Sigma_{N}^{-1}\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PA)B^{-1}K_{\mathcal{I}}^{-1}
\end{align*}
and so showing that it is the zero matrix is equivalent to showing that $(BK_{\mathcal{I}}B-B-A^{*}P^{*}\Sigma_{N}^{-1}\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PA)$ is the zero matrix. This can be shown as follows:
\begin{align*}
    &BK_{\mathcal{I}}B-B-A^{*}P^{*}\Sigma_{N}^{-1}\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PA = \\
    &=(A^{*}P^{*}\Sigma_{N}^{-1}PA+K_{\mathcal{I}}^{-1})K_{\mathcal{I}}B-B-A^{*}P^{*}\Sigma_{N}^{-1}\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PA \\
    &=A^{*}P^{*}\Sigma_{N}^{-1}PAK_{\mathcal{I}}B+B-B-A^{*}P^{*}\Sigma_{N}^{-1}\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PA \\
    &=A^{*}P^{*}\Sigma_{N}^{-1}PAK_{\mathcal{I}}(A^{*}P^{*}\Sigma_{N}^{-1}PA+K_{\mathcal{I}}^{-1})-A^{*}P^{*}\Sigma_{N}^{-1}\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PA \\
    &=A^{*}P^{*}\Sigma_{N}^{-1}PAK_{\mathcal{I}}A^{*}P^{*}\Sigma_{N}^{-1}PA + A^{*}P^{*}\Sigma_{N}^{-1}PA - A^{*}P^{*}\Sigma_{N}^{-1}\Sigma_{\mathcal{I}}\Sigma_{N}^{-1}PA \\
    &= A^{*}P^{*}\Sigma_{N}^{-1}(PAK_{\mathcal{I}}A^{*}P^{*}+\Sigma_{N}-\Sigma_{\mathcal{I}})\Sigma_{N}^{-1}PA = 0
\end{align*}
where the last equality follows by the definition of $\Sigma_{\mathcal{I}}$. Thus, our integral simplifies to:
\begin{equation}
    \frac{(2\pi)^{J/2}}{Z_{u}Z_{f}\det(B)^{1/2}}\int\phi(u^{N})\exp\Big(-\frac{1}{2}\left\langle u^{N}-h^{N},\Sigma_{\mathcal{I}}^{-1}(u^{N}-h^{N}) \right\rangle\Big)\mathrm{d}u^{N}
\end{equation}
We now focus on simplifying the normalizing constants infront of the integral:
\begin{align*}
    \frac{(2\pi)^{J/2}}{Z_{u}Z_{f}\det(B)^{1/2}}&=\frac{(2\pi)^{J/2}}{(2\pi)^{N/2}\det(\Sigma_N)^{1/2}(2\pi)^{J/2}\det(K_\mathcal{I})^{1/2}\det(B)^{1/2}}= \\
    &=\frac{1}{(2\pi)^{N/2}\det(\Sigma_N)^{1/2}\det(K_\mathcal{I})^{1/2}\det(B)^{1/2}}
\end{align*}
To proceed we note that $\det{B}$ can be rewritten as follows:
\begin{align*}
    \det(B)&=\det(A^{*}P^{*}\Sigma_{N}^{-1}PA + K_{\mathcal{I}}^{-1}) \\
    &=\det(K_{\mathcal{I}}^{-1}(I+K_{\mathcal{I}}A^{*}P^{*}\Sigma_{N}^{-1}PA)) \\
    &=\det(K_{\mathcal{I}}^{-1})\det(I+(K_{\mathcal{I}}A^{*}P^{*})(\Sigma_{N}^{-1}PA))) \\
    &=\det(K_{\mathcal{I}})^{-1}\det(I+\Sigma_{N}^{-1}PAK_{\mathcal{I}}A^{*}P^{*})
\end{align*}
where we have utilized Sylvester's determinant theorem (\textit{note: the identity matrices in the last two lines are of different sizes}). We can now finish up the simplification of the constants outside the integral:
\begin{align*}
    &\frac{1}{(2\pi)^{N/2}\det(\Sigma_N)^{1/2}\det(K_\mathcal{I})^{1/2}\det(B)^{1/2}} \\
    &=\frac{1}{(2\pi)^{N/2}\det(\Sigma_N)^{1/2}\det(K_\mathcal{I})^{1/2}\det(K_{\mathcal{I}})^{-1/2}\det(I_{N}+\Sigma_{N}^{-1}PAK_{\mathcal{I}}A^{*}P^{*})^{1/2}} \\
    &= \frac{1}{(2\pi)^{N/2}\det(\Sigma_{N} + PAK_{\mathcal{I}}A^{*}P^{*})^{1/2}} \\
    &=\frac{1}{(2\pi)^{N/2}\det(\Sigma_{\mathcal{I}})^{1/2}}
\end{align*}
Thus, our integral becomes:
\begin{align}
    \int&\phi(u^{N})\frac{1}{(2\pi)^{N/2}\det(\Sigma_{\mathcal{I}})^{1/2}}\exp\left(-\frac{1}{2}\left\langle u^{N}-h^{N}, \Sigma_{\mathcal{I}}^{-1}(u^{N}-h^{N}) \right\rangle\right)\mathrm{d}u^N = \nonumber \\
    =&\int\phi(u^{N})\mu_{h^{N},\Sigma_{I}}(\mathrm{d}u^N)
\end{align}
from which we see that we have obtained the expectation of $\phi$ w.r.t. a multivariate Gaussian with mean and covariance given by:
\begin{equation}
    h^{N}:=\Sigma_{I}\Sigma_{N}^{-1}PAB^{-1}K_{\mathcal{I}}^{-1}\bar{F}
\end{equation}
\begin{equation}
    \Sigma_{\mathcal{I}}:=P(\Sigma+AK_{\mathcal{I}}A^{*})P^{*}
\end{equation}
Thus, we conclude that ``averaging" over $f$ gives the following Gaussian posterior:
\begin{equation}
    \label{posteriorDist}
    N\left((\Sigma+AK_{\mathcal{I}}A^{*})P^{*}\Sigma_{N}^{-1}PAB^{-1}K_{\mathcal{I}}^{-1}\bar{F},\Sigma+AK_{\mathcal{I}}A^{*}\right)
\end{equation}

\section{Notes 2}

We will now start focusing on a specific PDE. We will consider here the standard elliptic stationary conductivity problem:
\begin{align}
    \label{standard_conduct}
    \begin{split}
        \mathcal{L}u &:= -\nabla\cdot(a(x)\nabla u(x))=f(x), \hspace{0.3cm} x\in\Omega \\
        u&=0, \hspace{0.3cm} x\in\partial\Omega
    \end{split}
\end{align}
where $f$ is stochastic. In this case, standard energy estimates will give that $\mathcal{L}$ is a bounded operator from $H^1$ to $H^{-1}$ (!!!!CHECK!!!!!). Thus, we seek a solution $u\in H_{0}^{1}(\Omega)$. \\

Once we assume this, we can be more precise about the choice of Gaussian prior on $u$. In particular, we want, at the very least, that the Cameron-Martin space of $\mathcal{N}(0,V)$ lies in $H^1$. Let $\mathcal{A}=-\Delta$ (with Dirichlet or Neumann boundary conditions). $\mathcal{A}$ satisfies Assumptions 2.9(i)-(iii) of \cite{stuart2010inverse}. Further, on page 474 of \cite{stuart2010inverse} it states that for such a choice of $\mathcal{A}$ the spaces $\mathcal{H}^{s}$ in (2.29) are contained in the usual Sobolev spaces $H^s$. Thus, provided we choose $\alpha>1+d/2$, Lemma 6.27 of \cite{stuart2010inverse} gives us that
$u\sim\mathcal{N}(0,V)$, where $V=\mathcal{A}^{-\alpha}$, is in $\mathcal{H}^{s}$ almost surely for any $s\in[0,\alpha-d/2)$. In particular, since we have assumed $\alpha>1+d/2$ we have that $\alpha-d/2>1$ and so $u\sim\mathcal{N}(0,V)$ is in $\mathcal{H}^{1}$ almost surely. This gives us that $u$ is almost surely in $H^1$ since $\mathcal{H}^1\subset H^1$ for our choice of $\mathcal{A}$. \\

We will now connect this to FEM. We start by introducing a choice of information operators. These operators must be consistent with the spaces we are working with. In particular, $I_j$ should lie in $\mathcal{U}^{\prime\prime}=U$. i.e. $I_j$ maps $H^{-1}$ to $\mathbb{R}$. Given a finite element basis $\{\psi_{i}\}$, we can now choose to define $I_j$ as:
\begin{equation}
    \label{info_operator_def}
    I_{j}g := \langle g, \psi_j \rangle
\end{equation}
where $\langle , \rangle$ denotes thes $H^{-1}(\Omega),H_{0}^{1}(\Omega)$ duality pairing. This can be identified with an inner product on $L^{2}$ using the Riesz Representation theorem. It can be shown that for this choice we have:
\begin{equation}
    \label{info_operator_def_on_Lu}
    I_{j}\mathcal{L}u=\int_{\Omega}a(x)\nabla u(x)\cdot\nabla\psi_{j}(x)\mathrm{d}x
\end{equation}

Next we make a particular choice of prior covariance $V$ (\textit{note: this is not related to the choice discussed above and is just to relate the formulation to FEM}). We define $V:H^{-1}\rightarrow H^1$ as follows:
\begin{equation}
    \label{FEM_prior}
    Vu(x) := \sum_{i=1}^{J}\lambda_{i}\psi(x)\int_{\Omega}\psi_{i}(y)u(y)\mathrm{d}y
\end{equation}
In order to proceed it will prove helpful to express $V$ in a different form as follows. We note that we can write:
\begin{equation*}
    Vu(x)=\Phi(x)^{*}\Lambda\int_{\Omega}\Phi(y)u(y)\mathrm{d}y
\end{equation*}
where $\Phi(x):=(\psi_{1}(x),\dots,\psi_{J}(x))^{T}$ is a vector in $\mathbb{R}^{J}$ and $\Lambda:=\text{diag}\{\lambda_{i}\}_{i=1}^{J}$ is a $J\times J$ matrix. We now introduce the following operators, $\{T_i\}_{i=1}^{J}$ defined by:
\begin{equation*}
    T_{i}u:=\int_{\Omega}\psi_{i}(y)u(y)\mathrm{d}y
\end{equation*}
If we now let $T:=(T_1,\dots,T_{J})^{T}$ we have:
\begin{equation*}
    Vu(x)=\Phi(x)^{*}\Lambda Tu
\end{equation*}
Thus,
\begin{equation*}
    V=\Phi^{*}\Lambda T
\end{equation*}
We note that $\Lambda Tu$ is a vector in $R^{J}$ which gives the coefficients of the expansion of the function $Vu$ in terms of the finite element basis. Noting this will enable us to make sense what happens to (\ref{post_mean_before_averaging}) and (\ref{post_var_before_averaging}). Before we go through this calculation we first introduce the following operators: $G_j:=I_{j}\mathcal{L}:H^{1}\rightarrow\mathbb{R}$ for $j=1,\dots,J$. We also let $G=(G_1,\dots,G_J)^{T}$ so that $G=\mathcal{I}\mathcal{L}$. Looking at the mean we have:
\begin{align*}
    a &= V\mathcal{I}^{*}\mathcal{L}^{*}(\mathcal{I}\mathcal{L}V\mathcal{L}^{*}\mathcal{I}^{*})^{-1}F \\
    &= VG^{*}(GVG^{*})^{-1}F
\end{align*}
Now, $G$ is a map from $H^{1}$ to $\mathbb{R}^{J}$ and so $G^{*}$ is a map from $\mathbb{R}^{J}$ to $H^{-1}$. Thus, $GVG^{*}$ is a map from $\mathbb{R}^{J}$ to $\mathbb{R}^{J}$, i.e. a $J\times J$ matrix. We first compute this matrix. We have $GV=G\Phi^{*}\Lambda T$. We observe that:
\begin{equation*}
    G\Phi^{*}=\begin{pmatrix}
                G_1 \\
                \vdots \\
                G_{J}
              \end{pmatrix} \begin{pmatrix}
                                \psi_1 & \hdots & \psi_{J}
                            \end{pmatrix}
\end{equation*}
is a $J\times J$ matrix. The $ij$\textit{th} entry of this matrix is:
\begin{align*}
    (G\Phi^{*})_{ij}&=G_{i}\psi_{j} \\
    &=I_{i}\mathcal{L}\psi_{j} \\
    &=\int_{\Omega}a(x)\nabla\psi_{i}(x)\cdot\nabla\psi_{j}(x)\mathrm{d}x \\
    &=:M_{ij}
\end{align*}
This matrix $M$ is the standard Galerkin matrix which appears in the finite element method. Continuing we thus have,
\begin{align*}
    &GV=G\Phi^{*}\Lambda T = M\Lambda T \\
    &\implies GVG^{*}=M\Lambda TG^{*}
\end{align*}
To figure out $TG^{*}$ we express it as $TG^{*}=(GT^{*})^{*}$ where the adjoint of $T$ is $T^{*}=(T_1^{*},\dots,T_{J}^{*})$. To figure out the adjoints $T_i^{*}$ we consider for $\alpha\in\mathbb{R}$ and for any function $u$ the following:
\begin{align*}
    \alpha T_{i}u &= \alpha\int_{\Omega}\psi_{i}(y)u(y)\mathrm{d}y \\
    &=\int_{\Omega}\alpha\psi_{i}(y)u(y)\mathrm{d}y
\end{align*}
which suggests that $T_{i}^{*}\alpha=\alpha\psi_{i}$. I.e. $T_{i}^{*}$ is the operator which maps a real number $\alpha$ to the function $\alpha\psi_{i}$. Thus,
\begin{equation*}
    GT^{*}\alpha=\begin{pmatrix}
                G_1 \\
                \vdots \\
                G_{J}
              \end{pmatrix} \begin{pmatrix}
                                T_1^{*} & \hdots & T_{J}^{*}
                            \end{pmatrix}\alpha
\end{equation*}
and this has $ij$\textit{th} entry given by:
\begin{align*}
    (GT^{*}\alpha)_{ij}&=G_{i}T_{j}^{*}\alpha \\
    &=G_{i}(\alpha\psi_{j}) \\
    &=\alpha G_{i}(\psi_{j}) \\
    &=\alpha M_{ij}
\end{align*}
which gives us that $GT^{*}=M$. Thus, $TG^{*}=(GT^{*})^{*}=M^{*}$. We thus have that $GVG^{*}=M\Lambda TG^{*}=M\Lambda M^{*}$ and so we can now finish the simplification of $a$:
\begin{align*}
    a &= VG^{*}(GVG^{*})^{-1}F \\
    &= VG^{*}(M\Lambda M^{*})^{-1}F \\
    &= \Phi^{*}\Lambda TG^{*}(M\Lambda M^{*})^{-1}F \\
    &= \Phi^{*}\Lambda M^{*}(M\Lambda M^{*})^{-1}F \\
\end{align*}
Now using standard finite element arguments, one can identify conditions under which the matrix $M$ is non-singular. We will expand on these conditions in the next section. Under the assumption that these conditions hold everything drastically simplifies and we obtain:
\begin{align*}
    a &= \Phi^{*}\Lambda M^{*}(M\Lambda M^{*})^{-1}F \\
    &= \Phi^{*}\Lambda M^{*}(M^{*})^{-1}\Lambda^{-1}M^{-1}F \\
    &= \Phi^{*}M^{-1}F
\end{align*}
We thus see that when expressed in terms of the finite element basis the mean function has coefficients given by the vector $\hat{a}:=M^{-1}F$ just like FEM gives. We now turn to the covariance (\ref{post_var_before_averaging}) and compute:
\begin{align*}
    \Sigma &= V -V\mathcal{L}^{*}\mathcal{I}^{*}(\mathcal{I}\mathcal{L}V\mathcal{L}^{*}\mathcal{I}^{*})^{-1}\mathcal{I}\mathcal{L}V \\
    &= V - VG^{*}(GVG^{*})^{-1}GV \\
    &= V - \Phi^{*}\Lambda TG^{*}(M\Lambda M^{*})^{-1}GV \\
    &= V - \Phi^{*}\Lambda M^{*}(M^{*})^{-1}\Lambda^{-1}M^{-1}GV \\
    &= V - \Phi^{*}M^{-1}GV \\
    &= V - \Phi^{*}M^{-1}M\Lambda T \\
    &= V - \Phi^{*}\Lambda T \\
    &= V - V = 0
\end{align*}
We can thus see that when we choose our information operators according to (\ref{info_operator_def}) and our prior covariance according to (\ref{FEM_prior}) that the posterior (for a fixed realisation of $F$) collapses to a point measure located at the FEM solution to the PDE. \\

We now move on to average over the distribution of $f$. We note that we cannot just utilize (\ref{posteriorDist}) as in this expression we require the inverse of $\Sigma_{N}$ which under our assumptions is $\Sigma_N=P\Sigma P^{*}=0$ as $\Sigma=0$. We must instead redo the calculation of the expectation of an arbitrary bounded cylindrical test function which we performed in section 1. As before we are interested in computing:

\begin{equation}
    \int\int\phi(u^{N})\mu_{a,\Sigma}(\mathrm{d}u)\mu_{\bar{f},K}(\mathrm{d}f)
\end{equation}
where now $a=\Phi^{*}M^{-1}F$ and $\Sigma=0$. Thus, $\mu_{a,\Sigma}=\delta_{a}$, that is to say that the measure is in fact a Dirac point mass at $a=\Phi^{*}M^{-1}F$. We thus have:
\begin{equation*}
    \int\int\phi(u^{N})\mu_{a,\Sigma}(\mathrm{d}u)\mu_{\bar{f},K}(\mathrm{d}f)=\int\int\phi(u^N)\delta_{\Phi^{*}M^{-1}F}(\mathrm{d}u)\mu_{\bar{f},K}(\mathrm{d}f)
\end{equation*}
Now since the posterior for $u$ for a fixed realisation of $f$ is a point mass we have that the posterior for $u^N=Pu$ is $\delta_{P\Phi^{*}M^{-1}F}$. Thus, we can write:
\begin{equation*}
    \int\int\phi(u^N)\delta_{\Phi^{*}M^{-1}F}(\mathrm{d}u)\mu_{\bar{f},K}(\mathrm{d}f)=\int\int\phi(u^N)\delta_{P\Phi^{*}M^{-1}F}(\mathrm{d}u^N)\mu_{\bar{f},K}(\mathrm{d}f)
\end{equation*}
It is now important to point out that $P\Phi^{*}$ is to be interpreted\footnote{By this we mean that $P\Phi^{*}$ is actually interpreted as $P\otimes\Phi^{*}$} as the following $N\times J$ matrix:
\begin{align*}
    P\Phi^{*}&=P(\psi_{1},\dots,\psi_{J}) \\
    &=(P\psi_{1},\dots,P\psi_{J}) \\
    &=\begin{pmatrix}
        \psi_{1}(x_1) & \dots & \psi_{J}(x_1) \\
        \vdots & & \vdots \\
        \psi_{1}(x_N) & \dots & \psi_{J}(x_N)
      \end{pmatrix}=:\Psi
\end{align*}
i.e. the matrix with $ij$\textit{th} entry $\Psi_{i,j}:=\psi_{j}(x_{i})$ for $i\in\{1,\dots,N\},j\in\{1,\dots,J\}$. We thus have that $Y:=P\Phi^{*}M^{-1}F=\Psi M^{-1}F\sim\mathcal{N}(\Psi M^{-1}\bar{F},\Psi M^{-1}K_{\mathcal{I}}M^{-1}\Psi^{*})$. Together with this and with the fact that the posterior of $u^{N}$ only depends on $f$ through $F$ we can write:
\begin{align*}
    \int\int\phi(u^N)\delta_{P\Phi^{*}M^{-1}F}(\mathrm{d}u^N)\mu_{\bar{f},K}(\mathrm{d}f)&=\int\int\phi(u^N)\delta_{\Psi M^{-1}F}(\mathrm{d}u^{N})\mu_{\bar{F},K_{\mathcal{I}}}(\mathrm{d}F) \\
    &=\int\int\phi(u^N)\delta_{Y}(\mathrm{d}u^N)\mu_{\Psi M^{-1}\bar{F},\Psi M^{-1}K_{\mathcal{I}}M^{-1}\Psi^{*}}(\mathrm{d}Y) \\
    &=\int\phi(u^N)\left(\int\delta_{Y}(\mathrm{d}u^N)\mu_{\Psi M^{-1}\bar{F},\Psi M^{-1}K_{\mathcal{I}}M^{-1}\Psi^{*}}(\mathrm{d}Y)\right) \\
    &=\int\phi(u^N)\mu_{\Psi M^{-1}\bar{F},\Psi M^{-1}K_{\mathcal{I}}M^{-1}\Psi^{*}}(\mathrm{d}u^N)
\end{align*}
which we recognize as a finite dimensional projection of a Gaussian measure. Thus, we conclude that ``averaging" over $f$ gives the following Gaussian posterior under these choices:
\begin{equation}
    \label{average_posterior_FEM_prior}
    \mathcal{N}(\Phi^{*}M^{-1}\bar{F},\Phi^{*}M^{-1}K_{\mathcal{I}}M^{-1}\Phi)
\end{equation}

This Gaussian measure directly links to what is found at the end of section 2 in the Statistical FEM paper.

\section{Notes 3}

We now provide details for some of the claims in Notes 1 and 2 above. \\

To start we provide justification for why it is valid to condition as we did in (\ref{conditionalDistnFixed_f}). The justification for this is essentially given by Theorem 3.3 in \cite{owhadi2015conditioning}. To apply this theorem to our problem we note that our pair $(u,\mathcal{I}\mathcal{L}u)^{T}$ lies in the orthogonal direct sum $\mathcal{H}=\mathcal{H}_1\oplus\mathcal{H}_{2}$ of the separable Hilbert spaces $\mathcal{H}_{1}:=H^{1}(\Omega)\times\{0\}$ and $\mathcal{H}_{2}:=\{0\}\times\mathbb{R}^{J}$. The corresponding Gaussian measure we are dealing with on this $\mathcal{H}$ is given by (\ref{jointInfoDist}). Theorem 3.3 states that the conditional distribution is a Gaussian measure with covariance operator being the short of the covariance operator of the distribution (\ref{jointInfoDist}) to $\mathcal{H}_{2}$. This operator,
\begin{equation}
    \begin{pmatrix}
    V & V\mathcal{L}^{*}\mathcal{I}^{*} \\
    \mathcal{I}\mathcal{L}V & \mathcal{I}\mathcal{L}V\mathcal{L}^{*}\mathcal{I}^{*}
    \end{pmatrix}
\end{equation}
is already written in its $(\mathcal{H}_1,\mathcal{H}_2)$ partition representation. In the section on shorted operators in \cite{owhadi2015conditioning} it is stated that provided the lower right partition of the covariance operator of (\ref{jointInfoDist}) corresponding to the covariance in $\mathcal{H}_2$, i.e. $\mathcal{I}\mathcal{L}V(\mathcal{I}\mathcal{L})^{*}$ is invertible then the shorted operator is given by:
\begin{equation}
    \begin{pmatrix}
        V-V\mathcal{L}^{*}\mathcal{I}^{*}(\mathcal{I}\mathcal{L}V\mathcal{L}^{*}\mathcal{I}^{*})^{-1}\mathcal{I}\mathcal{L}V & 0 \\
        0 & 0
    \end{pmatrix}=\begin{pmatrix}
                    \Sigma & 0 \\
                    0 & 0
                  \end{pmatrix}
\end{equation}
We assume the invertiblity of this partition. We do note however that this part of the partition reduces to the $J\times J$ matrix $M\Lambda M^{*}$ when we choose the FEM prior and FEM information operator. The invertibility of this matrix is then equivalent to the invertibility of the Galerkin Matrix, $M$ (if we assume that the entries of $\Lambda$ are non-zero). \\

We thus see that the conditional distribution (\ref{conditionalDistnFixed_f}) has the correct covariance operator. \\

Theorem 3.3 also goes on to give the mean of the conditional distribution in the case that the covariance operator of the joint distribution is compatible with $\mathcal{H}_{2}$. For our problem this is in fact the case as we now show. We first denote the covariance operator of the joint distribution by:
\begin{equation}
    \begin{pmatrix}
        V & V\mathcal{L}^{*}\mathcal{I}^{*} \\
        \mathcal{I}\mathcal{L}V & \mathcal{I}\mathcal{L}V\mathcal{L}^{*}\mathcal{I}^{*}
    \end{pmatrix}=
    \begin{pmatrix}
        C_{11} & C_{12} \\
        C_{21} & C_{22}
    \end{pmatrix}
\end{equation}
i.e. $C_{11}=V, C_{12}=V\mathcal{L}^{*}\mathcal{I}^{*}, C_{21}=\mathcal{I}\mathcal{L}V, C_{22}=\mathcal{I}\mathcal{L}V\mathcal{L}^{*}\mathcal{I}^{*}$. If we now define the following bounded operator $Q:\mathcal{H}\rightarrow\mathcal{H}$ by

\begin{equation}
    Q = \begin{pmatrix}
            0 & 0 \\
            C_{22}^{-1}C_{21} & I
        \end{pmatrix} = \begin{pmatrix}
                            0 & 0 \\
                            \hat{Q} & I
                        \end{pmatrix}
\end{equation}
where $\hat{Q}:=C_{22}^{-1}C_{21}:\mathcal{H}_{1}\rightarrow\mathcal{H}_{2}$ we can easily check that such an operator $Q$ is an (C-symmetric) oblique projection onto $\mathcal{H}_{2}$. Since $Q$ is not $0$ we have by Theorem 3.3 that the mean of the conditional distribution (\ref{conditionalDistnFixed_f}) is given by $\hat{Q}^{*}F$ where the adjoint is defined by the relation $\left\langle\hat{Q}^{*} h_{2}, h_{1}\right\rangle_{\mathcal{H}_{1}}=\left\langle h_{2}, \hat{Q} h_{1}\right\rangle_{\mathcal{H}_{2}}$ for all $h_1\in\mathcal{H}_1,h_{2}\in\mathcal{H}_{2}$. Since we have that $C_{22}$ and hence $C_{22}^{-1}$ is self-adjoint and further that $C_{12}^{*}=C_{21}$ we can easily deduce that $\hat{Q}^{*}=C_{12}C_{22}^{-1}$ and so the mean is given by
\begin{equation}
    \hat{Q}^{*}F=C_{12}C_{22}^{-1}C_{21}F=V\mathcal{L}^{*}\mathcal{I}^{*}(\mathcal{I}\mathcal{L}V\mathcal{L}^{*}\mathcal{I}^{*})^{-1}F=a
\end{equation}
where $a$ is what we had in (\ref{post_mean_before_averaging}). \\

We can now move on to provide conditions for the Galerkin matrix $M$ defined in Notes 2 to be invertible. A sufficient condition to ensure this invertibility is the following assumption (see Assumption 2.34 in \cite{lord2014introduction}): \\

\noindent \textbf{Assumption} (regularity of coefficients) The diffusion coefficient $a(x)$ satisfies:
\begin{equation}
    \label{regularity_of_coeff}
    0<a_{\text{min}}\leq a(x) \leq a_{\text{max}}<\infty \text{ for almost all } x\in\Omega
\end{equation}
for some real constants $a_{\text{min}},a_{\text{max}}$. In particular $a\in L^{\infty}(\Omega)$. \\

If we assume that this assumption holds then it is a simple exercise to show that the following bilinear form from $H_{0}^{1}(\Omega)\times H_{0}^{1}(\Omega)$ to $\mathbb{R}$ defined by:
\begin{equation}
    \label{bilinear_form}
    \tilde{a}(u,w):=\int_{\Omega}a(x)\nabla u(x)\cdot\nabla w(x)\mathrm{d}x
\end{equation}
defines a norm $|\cdot|_{E}$ on $H_{0}^{1}(\Omega)$ via $|u|_{E}:=\tilde{a}(u,u)^{1/2}$. To show that the Galerkin matrix $M$ is invertible we will show that it is (strictly) positive definite. To this end let $\mathbf{v}\in\mathbb{R}^{J}\backslash\{0\}$ and define $v=\sum_{i=1}^{J}v_{i}\psi_{i}$ which is a function in $H_{0}^{1}(\Omega)$. We now have:
\begin{align*}
    \mathbf{v}^{T}(M)\mathbf{v}&=\sum_{i,j=1}^{J}v_{j}(M_{ij})v_{i} \\
    &=\sum_{i,j=1}^{J}v_{j}\left(\int_{\Omega}a(x)\nabla\psi_{i}(x)\cdot\nabla\psi_{j}(x)\mathrm{d}x\right)v_{i} \\
    &=\sum_{i,j=1}^{J}v_{j}\tilde{a}(\psi_{i},\psi_{j})v_{i}\\
    &=\tilde{a}\left(\sum_{i=1}^{J}v_{i}\psi_{i},\sum_{j=1}^{J}v_{j}\psi_{j}\right) \\
    &=\tilde{a}(v,v) \\
    &=|v|_{E}^{2}>0
\end{align*}
where the strict inequality follows since $\tilde{a}(v,v)=0$ iff $|v|_{E}=0$ iff $v=0$ (since $|\cdot|_{E}$ is a norm on $H_{0}^{1}(\Omega)$) iff $\mathbf{v}=0$ since the $\{\psi_{i}\}$ are linearly independent.

\section{Notes 4}

In Notes 1 the true joint distribution of $(u,f)$ given by (\ref{trueJoint}) gives us that the ``true" distribution of the solution $u$ to the problem (\ref{standard_conduct}) is $\nu_{1}=\mathcal{N}(\mathcal{L}^{-1}\bar{f},\mathcal{L}^{-1}K(\mathcal{L}^{-1})^{*})$. Meanwhile, in Notes 2 we worked out that ``averaged" posterior for $u$ (for the FEM prior) is $\nu_{2}=\mathcal{N}(\Phi^{*}M^{-1}\bar{F},\Phi^{*}M^{-1}K_{\mathcal{I}}M^{-1}\Phi)$ (see (\ref{average_posterior_FEM_prior})). In this section we will consider quantifying how close these two distributions $\nu_{1},\nu_{2}$ are. This will be achieved by obtaining an upperbound for the Wasserstein distance between $\nu_{1},\nu_{2}$ using the connection between the Wasserstein distance between Gaussian measures and the Procrustes Metric on covariance operators (see \cite{masarotto2019procrustes}). We start by first giving the definition of the Wasserstein distance between two probability measures $\mu,\nu$ on $\mathcal{H}$, $W(\mu,\nu)$:
\begin{equation*}
    W^{2}(\mu, \nu)=\inf _{\pi \in \Gamma(\mu, \nu)} \int_{\mathcal{H} \times \mathcal{H}}\|x-y\|^{2} \mathrm{d} \pi(x, y)
\end{equation*}
where $\Gamma(\mu,\nu)$ is the set of couplings of $\mu$ and $\nu$, i.e: \\ $$\Gamma(\mu,\nu)=\{\text{Borel probability measures } \pi \text{ on } \mathcal{H}\times\mathcal{H}|\pi(E\times\mathcal{H})=\mu(E) \text{ and } \pi(\mathcal{H}\times F)=\nu(F) \text{ for all Borel } E,F\subset\mathcal{H}\}$$
When $\mu$ and $\nu$ are both Gaussian measures an explicit expression can be obtained for the Wasserstein distance. Suppose $\mu=\mathcal{N}(m_1,\Sigma_1)$ and $\nu=\mathcal{N}(m_2,\Sigma_2)$. One has,
\begin{equation*}
    W^{2}(\mu, \nu)=\left\|m_{1}-m_{2}\right\|^{2}_{\mathcal{H}}+\operatorname{tr}\left(\Sigma_{1}\right)+\operatorname{tr}\left(\Sigma_{2}\right)-2 \operatorname{tr} \sqrt{\Sigma_{1}^{1 / 2} \Sigma_{2} \Sigma_{1}^{1 / 2}}
\end{equation*}
This formula is true in both the finite and infinite dimensional cases. The term $\operatorname{tr} \sqrt{\Sigma_{1}^{1 / 2} \Sigma_{2} \Sigma_{1}^{1 / 2}}$ is difficult to analyse in our situation. As such we will make use of Proposition 3 from \cite{masarotto2019procrustes} which states:
\begin{proposition}
    The Procrustes distance between two trace-class operators $\Sigma_{1}$ and $\Sigma_{2}$ on $\mathcal{H}$ coincides with the Wasserstein distance between two second-order Gaussian processes $\mathcal{N}(0,\Sigma_{1})$ and $\mathcal{N}(0,\Sigma_2)$ on $\mathcal{H}$,
    $$\Pi(\Sigma_{1},\Sigma_{2}):=\inf_{R:R^{*}R=I}\|\Sigma_{1}^{1/2}-R\Sigma_{2}^{1/2}\|_{2}=W(\mathcal{N}(0,\Sigma_{1}),\mathcal{N}(0,\Sigma_{2}))$$
    where $\|\cdot\|_{2}$ is the Hilbert-Schmidt norm defined by $\|A\|_{2}=\sqrt{\operatorname{tr}(A^{*}A)}$.
\end{proposition}
Using this result one can obtain a simple upperbound on the Wasserstein distance by choosing $R=I$ in the infimum:
\begin{equation*}
    W(\mathcal{N}(0,\Sigma_1),\mathcal{N}(0,\Sigma_2))=\inf_{R:R^{*}R=I}\|\Sigma_{1}^{1/2}-R\Sigma_{2}^{1/2}\|_{2} \leq \|\Sigma_{1}^{1/2}-\Sigma_{2}^{1/2}\|_{2}
\end{equation*}
Now since in our case we have un-centered Gaussian measures $\nu_{1},\nu_{2}$ we must first link the Wasserstein distance between $\nu_1,\nu_2$ to the Wasserstein distance of the centred measures $\nu_{1}^{*},\nu_{2}^{*}$ using a general result mentioned in \cite{cuesta1996lower}:
\begin{equation*}
    W^2(\nu_{1},\nu_{2})=\|m_1-m_2\|^{2}+W^{2}(\nu_{1}^*,\nu_{2}^{*})
\end{equation*}
\textit{Note: here} $m_1,m_2$ \textit{are the means of} $\nu_{1},\nu_{2}$ \textit{respectively}. \\

For our particular case, the norm for the difference in means is the norm on $H_{0}^{1}(\Omega)$: $\|m_1-m_2\|^{2}=|m_1-m_2|_{H^1(\Omega)}^{2}=\|\nabla(m_1-m_2)\|_{L^2(\Omega)}^{2}$ If we now denote the covariance operators of $\nu_{1},\nu_{2}$ as $\Sigma_{1},\Sigma_{2}$ respectively we have:
\begin{equation*}
    W^{2}(\nu_1,\nu_2)=|m_1-m_2|_{H^{1}(\Omega)}^{2}+W^{2}(\mathcal{N}(0,\Sigma_1),\mathcal{N}(0,\Sigma_2))
\end{equation*}
We now go about obtaining an upperbound on each of these two terms. We want to control each term by the FEM mesh size $h$ (which is inversely proportional to the number of finite elements $J$). For brevity we will assume now that $\Omega\subset\mathbb{R}^{2}$ so we have a 2 dimensional problem. The analysis follows in almost exactly the same way for $\mathbb{R}^{d}$. We will assume that $\Omega$ is a convex polygonal domain (the polygonal assumption can easily be relaxed). The convexity assumption gives us that the $H^2(\Omega)$ norm of the variational solution of our PDE is controlled by the $L^2$ norm of the RHS. We now take our FEM mesh to be a triangulation of $\Omega$ with $h$ being the maximum side length of any triangle in the triangulation. We require a further technical assumption that the meshes we consider remain regular in the sense that as we refine the mesh by decreasing $h$ to zero the angles of all the triangles are bounded below independently of $h$. Since $m_1=\mathcal{L}^{-1}\bar{f}$ is the solution to the following elliptic boundary value problem:
\begin{align*}
    \begin{split}
        -\nabla\cdot(a(x)\nabla v(x))&=\bar{f}(x), \hspace{0.3cm} x\in\Omega \\
        v&=0, \hspace{0.3cm} x\in\partial\Omega
    \end{split}
\end{align*}
and since $m_2=\Phi^{*}M^{-1}\bar{F}=\Phi^{*}M^{-1}\mathcal{I}\bar{f}$ is the FEM solution to the variational formulation of the above problem the error analysis of FEM transfers over to allow us to bound the norm of the difference of the means as follows:
\begin{equation}
    \label{bound_on_diff_means}
    |m_1-m_2|_{H^{1}(\Omega)}\leq Ch\|\bar{f}\|_{L^2(\Omega)}
\end{equation}
for some constant $C_{1}>0$ (!!ask about which value to use for this!!)
The assumptions and error analysis is taken from Chapter 5 of \cite{larsson2008partial}. \\

We now move on to getting an upperbound for the second term. Using the link with the Procrustes distance discussed above we have:
\begin{equation*}
    W^2(\mathcal{N}(0,\Sigma_1),\mathcal{N}(0,\Sigma_2))\leq\|\Sigma_{1}^{1/2}-\Sigma_{2}^{1/2}\|_{2}^{2}
\end{equation*}

The RHS of the above is still difficult to deal with so we make use of Lemma 4.1 from \cite{powers1970free} to obtain:
\begin{equation*}
    W^2(\mathcal{N}(0,\Sigma_1),\mathcal{N}(0,\Sigma_2))\leq\|\Sigma_{1}^{1/2}-\Sigma_{2}^{1/2}\|_{2}^{2}\leq\|\Sigma_1-\Sigma_2\|_{1}
\end{equation*}
where $\|\cdot\|_1$ is the trace norm or nuclear norm defined by $\|A\|_{1}=\operatorname{tr}(\sqrt{A^{*}A})$.






\clearpage
\section{Questions}

\noindent \textbf{Question 1:} I still do not see how defining $I_j$ by $I_j:=\langle g, \psi_j \rangle$ in (\ref{info_operator_def}) implies that $I_{j}\mathcal{L}u=-\int_{\Omega}a\nabla u \cdot \nabla\psi_{j}\mathrm{d}x$. The reason I am confused is because of the fact that $I_j$ is a map from $H^{-1}(\Omega)$ to $\mathbb{R}$. I do see that if I assume that $I_{j}$ acts on functions $g$ via:
\begin{equation*}
    I_{j}g=\int_{\Omega}\psi_{j}(x)g(x)\mathrm{d}x
\end{equation*}
then for $g=\mathcal{L}u$ I would have:
\begin{align*}
    I_{j}\mathcal{L}u&=\int_{\Omega}\psi_{j}(x)\nabla\cdot(a(x)\nabla u(x))\mathrm{d}x \\
    &=-\int_{\Omega}a(x)\nabla u(x)\cdot\nabla\psi_{j}(x)\mathrm{d}x + \int_{\partial\Omega}\psi_{j}(x)a(x)\nabla u(x)\cdot\mathbf{n}\mathrm{d}S \\
    &=-\int_{\Omega}a(x)\nabla u(x)\cdot\nabla\psi_{j}(x)\mathrm{d}x
\end{align*}
since the second integral is $0$ as $\psi_j$ is zero on the boundary $\partial\Omega$. This calculation however only makes sense provided the functions $g$ (and so $\mathcal{L}u$) is integrable ($L^2$ would let this work). I have tried to follow the argument using the Riesz Representation theorem in Evans in the section about $H^{-1}$ and apply it to our problem here but I cannot seem to connect the two. \\

\noindent \textbf{Question 2:} In Notes 2 I defined the operators $T_{i}$ by $T_{i}u:=\int_{\Omega}\psi_{i}(y)u(y)\mathrm{d}y$. I then wanted to figure out the adjoint of this operator $T_{i}^{*}$. If I view $T_{i}$ as an operator from $L^{2}(\Omega)$ to $\mathbb{R}$ then the adjoint is clearly given by $T_{i}^{*}\alpha=\alpha \psi_{i}$. My question is basically the following: when we introduced the FEM prior $V$ it was as a map from $H^{-1}$ to $H^{1}$. This suggest that $T_{i}$ is also a map from $H^{-1}\rightarrow\mathbb{R}$. But then wouldn't the adjoint be a map from $\mathbb{R}\rightarrow H_{0}^{1}(\Omega)$ defined by:
\begin{equation*}
    \alpha T_{i}g=\langle T_{i}^{*}\alpha, g \rangle_{H_{0}^{1},H^{-1}}
\end{equation*}
for all $g\in H^{-1}$ and for all $\alpha \in \mathbb{R}$? If this is the case I cannot seem to show that $T_{i}^{*}$ is defined by $T_{i}^{*}\alpha=\alpha \psi_{i}$. \\

\noindent \textbf{Question 3:} Since the particular PDE we are working with has the boundary condition that $u=0$ on $\partial\Omega$ does this mean that everywhere we mention the space $H^1$ we are talking about $H_{0}^{1}(\Omega)$? This is still a separable Hilbert space right?



\bibliography{references.bib}
\bibliographystyle{unsrt}

\end{document}
