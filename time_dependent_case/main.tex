\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{dsfont}
\usepackage{centernot}
\usepackage[a4paper,top=2.5cm,bottom=2.5cm,left=2cm,right=2cm,marginparwidth=1.75cm]{geometry}
\usepackage{parskip}

% for theorems/lemmas/defs/etc
%-------------------------------
\usepackage[english]{babel}
\usepackage[sort&compress,square,numbers]{natbib}
\bibliographystyle{unsrtnat}

\usepackage[dvipsnames]{xcolor}

\renewcommand\qedsymbol{$\blacksquare$}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]

\newtheorem{proposition}{Proposition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{corollary1}{Corollary}[definition]
\newtheorem{lemma}[theorem]{Lemma}

\theoremstyle{remark}
\newtheorem*{remark}{\textbf{Remark}}  % if don't want bold face for remark remove textbf

\theoremstyle{remark}
\newtheorem*{remarks}{\textbf{Remarks}}

%-------------------------------

\DeclareMathOperator{\E}{\mathds{E}}
\DeclareMathOperator{\Var}{\mathrm{Var}}
\DeclareMathOperator{\prob}{\mathbb{P}}

\pagestyle{fancy}
\fancyhf{}
\rhead{June 2020}
% \lhead{CID: 00955392}
\cfoot{\thepage}

\usepackage[hidelinks]{hyperref}
\hypersetup{colorlinks=false}

\begin{document}
\setlength\parskip{10pt}
\setlength\parindent{20pt}

\noindent We now focus on the following time-dependent PDE:
\begin{align}
        \mathcal{L}u(x,t) := \partial_{t}u(x,t) -\nabla\cdot(a(x)\nabla u(x,t))&=f(x,t), \hspace{0.3cm} x\in\Omega, \hspace{0.15cm} t\in[0,T] \\
        u(x,t) &= 0, \hspace{1.15cm} x\in\partial\Omega, \hspace{0.15cm} t\in[0,T] \\
        u(x,0) &= u_{0}(x), \hspace{0.5cm} x\in\Omega
\end{align}

We will now set up a prior on the solution $u$ to the above problem. To do so we first let $v_{h}\in S_{h}$ be some approximation of the initial condition $u_{0}(x)$ in the FEM space $S_{h}$. To be more specific we will assume that $v_{h}(x)=\Phi(x)^{*}\boldsymbol{\gamma}:=\sum_{i=1}^{J}\phi_{i}(x)\gamma_{i}$. Note that $\Phi(x):=(\phi_{1}(x),\dots,\phi_{J}(x))^{T}$. We take the prior on $u$ to be:
\begin{equation}
    u\sim\mathcal{N}(m_{0},V_{0})
\end{equation}
where $m_{0}(x,t):=v_{h}(x)=\Phi^{*}(x)\boldsymbol{\gamma}$ ($m_{0}$ is constant in time). The prior covariance operator $V_{0}$ is defined as follows:
\begin{equation}
    (V_{0}g)(x,t)=\int_{\Omega}\int_{o}^{T}\sum_{i=1}^{J}\lambda_{i}\phi_{i}(x)\phi_{i}(y)k(t,s)g(y,s)\mathrm{d}s\mathrm{d}y =: \int_{\Omega}\int_{0}^{T}k_{ys}^{xt}g(y,s)\mathrm{d}s\mathrm{d}y
\end{equation}
where we have a general kernel $k(t,s)$ for time which will be taken to be a specific function later. We have also used the notation $k_{ys}^{xt}:=\sum_{i=1}^{J}\lambda_{i}\phi_{i}(x)\phi_{i}(y)k(t,s)$ to make it clear which variables are held fixed and which we integrate against.

\noindent We now introduce the following operators $\mathcal{I}_{s}:=(I_{1}(s),\dots,I_{J}(s))^{T}$ where:
\begin{equation}
    I_{i}(s)g:=\int_{\Omega}\phi_{i}(x)g(x,s)\mathrm{d}x
\end{equation}

\noindent We now introduce a uniform time grid:
\begin{equation*}
    t_{n}=n\delta, \hspace{0.2cm} n=0,1,\dots,N
\end{equation*}
where $\delta$ is the spacing between consecutive times and $N=\frac{T}{\delta}$ (\textit{assume that} $N$ \textit{is an integer}).

To move from $t=t_{0}=0$ to $t=t_{1}=\delta$ we condition on observing $\mathcal{I}_{\delta}\mathcal{L}u=\mathcal{I}_{\delta}f=:F^{1}$. Let $\tilde{A}_{\delta}:=\mathcal{I}_{\delta}\mathcal{L}$. For a fixed realisation of $f$ (and so of $F^{1}$) we thus seek the following conditional distribution:
\begin{equation}
    u|\{\tilde{A}_{\delta}u=F^{1},f\}\sim\mathcal{N}(m_{1},V_{1})
\end{equation}
That this distribution is itself Gaussian follows from considering the following joint distribution:
\begin{equation*}
    \left(\begin{array}{c}u \\ \tilde{A}_{\delta} u\end{array}\right)=\left(\begin{array}{c}I \\ \tilde{A}_{\delta}\end{array}\right) u \sim \mathcal{N}\left(\left(\begin{array}{c}m_{o} \\ \tilde{A}_{\delta}m_{0}\end{array}\right),\left(\begin{array}{cc}V_{0} & V_{0} \tilde{A}_{\delta}^{*} \\ \tilde{A}_{\delta}V_{0} & \tilde{A}_{\delta}V_{0} \tilde{A}_{\delta}^{*}\end{array}\right)\right)
\end{equation*}
It follows that the conditional distribution is Gaussian and the mean and covariance are given by:
\begin{align}
    m_{1}&=m_{0}+V_{0}\tilde{A}_{\delta}^{*}(\tilde{A}_{\delta}V_{0}\tilde{A}_{\delta}^{*})^{-1}(F^{1}-\tilde{A}_{\delta}m_{0}) \\
    V_{1}&=V_{0}-V_{0}\tilde{A}_{\delta}^{*}(\tilde{A}_{\delta}V_{0}\tilde{A}_{\delta}^{*})^{-1}\tilde{A}_{\delta}V_{0}
\end{align}
We now rewrite the mean update equation as follows:
\begin{equation}
    \label{new_method_update}
    m_{1}=\left(1-V_{0}\tilde{A}_{\delta}^{*}(\tilde{A}_{\delta}V_{0}\tilde{A}_{\delta}^{*})^{-1}\tilde{A}_{\delta}\right)m_{0}+V_{0}\tilde{A}_{\delta}^{*}(\tilde{A}_{\delta}V_{0}\tilde{A}_{\delta}^{*})^{-1}F^{1}
\end{equation}
Written in this form this update equation can now be more easily compared to the backward-Euler Galerkin method update rule. This method involves the following approximations: $U^{n}\approx u(t_n)$ and $U^{n}(x)=\Phi(x)^{*}\boldsymbol{\alpha}^{n}$. The update rule for the vector of coefficients $\boldsymbol{\alpha}^{n}$ is given by:
\begin{equation}
    \boldsymbol{\alpha}^{n}=(M+\delta A)^{-1}M\boldsymbol{\alpha}^{n-1}+\delta(M+\delta A)^{-1}\mathbf{b}^{n}
\end{equation}
where $\mathbf{b}^{n}=\mathcal{I}_{t_n}f=F^{n}$. In order to compare this to our mean update rule we now project this into $S_{h}$ by premultiplying by $\Phi^{*}$:
\begin{equation}
    \label{classical_method_update}
    \Phi^{*}\boldsymbol{\alpha}^{n}=\Phi^{*}(M+\delta A)^{-1}M\boldsymbol{\alpha}^{n-1}+\delta\Phi^{*}(M+\delta A)^{-1}\mathbf{b}^{n}
\end{equation}
In our mean update rule $m_{0}$ plays the role of $\Phi^{*}\boldsymbol{\alpha}^{0}$ and $m_{1}$ plays the role of $\Phi^{*}\alpha^{1}$. In fact, we have $m_{0}=\Phi^{*}\boldsymbol{\gamma}$ and so we can consider $\boldsymbol{\alpha}^{0}=\boldsymbol{\gamma}$. This is exactly the initial condition for the coefficient vector in the backward-Euler Galerkin method. Comparing (\ref{classical_method_update}) with (\ref{new_method_update}) we thus see that we would like to be able to show:
\begin{align}
    \Phi^{*}(M+\delta A)^{-1}M &= \left(1-V_{0}\tilde{A}_{\delta}^{*}(\tilde{A}_{\delta}V_{0}\tilde{A}_{\delta}^{*})^{-1}\tilde{A}_{\delta}\right)\Phi^{*} \\
    \delta\Phi^{*}(M+\delta A)^{-1} &= V_{0}\tilde{A}_{\delta}^{*}(\tilde{A}_{\delta}V_{0}\tilde{A}_{\delta}^{*})^{-1}
\end{align}

To make progress we must now start computing various terms needed for our mean and covariance update rules. We start with $V_{0}\tilde{A}_{\delta}^{*}$. We have:
\begin{equation*}
    V_{0}\tilde{A}_{\delta}^{*}=V_{0}\mathcal{L}^{*}(I_{1}(\delta)^{*},\dots,I_{J}(\delta)^{*})
\end{equation*}
We can thus see that we need to be able to compute terms of form $V_{0}\mathcal{L}^{*}I_{i}^{*}(\delta)=V_{0}(I_{i}(\delta)\mathcal{L})^{*}$. Now since the operator $I_{i}(\delta)\mathcal{L}$ takes in a function on $\Omega\times[0,T]$ and outputs a real number its adjoint should take in a real number and output a function on $\Omega\times[0,T]$. This adjoint should satisfy the following relation:
\begin{equation}
    \alpha(I_{i}(\delta)\mathcal{L}g)=\int_{\Omega}\int_{0}^{T}((I_{i}(\delta)\mathcal{L})^{*}\alpha)(x,t)g(x,t)\mathrm{d}t\mathrm{d}x \hspace{0.15cm} \forall{g}, \hspace{0.15cm} \forall\alpha\in\mathbb{R}
\end{equation}
Using this we can now compute:
\begin{align*}
    (V_{0}(I_{i}(\delta)\mathcal{L})^{*}\alpha)(x,s) &= \int_{\Omega}\int_{0}^{T}k_{yw}^{xs}((I_{i}(\delta)\mathcal{L})^{*}\alpha)(y,w)\mathrm{d}w\mathrm{d}y \\
    &=\alpha (I_{i}(\delta)\mathcal{L}k^{xs}) \\
    &=\alpha\int_{\Omega}\phi_{i}(y)(\mathcal{L}k^{xs})(y,\delta)\mathrm{d}y
\end{align*}

We now work out $(\mathcal{L}k^{xs})(y,\delta)$ taking care to remember that $x,s$ are fixed and so $\mathcal{L}$ acts on the variables $y,\delta$:
\begin{align*}
    (\mathcal{L}k^{xs})(y,\delta) &= \partial_{2}k(s,\delta)\sum_{j=1}^{J}\lambda_{j}\phi_{j}(x)\phi_{j}(y)-k(s,\delta)\nabla_{y}\cdot\left(a(y)\nabla_{y}\sum_{j=1}^{J}(\lambda_{j}\phi_{j}(x)\phi_{j}(y)\right) \\
    &=\partial_{2}k(s,\delta)\sum_{j=1}^{J}\lambda_{j}\phi_{j}(x)\phi_{j}(y)-k(s,\delta)\sum_{j=1}^{J}\lambda_{j}\phi_{j}(x)\nabla_{y}\cdot\left(a(y)\nabla_{y}\phi_{j}(y)\right)
\end{align*}
So we can now compute:
\begin{align*}
    (V_{0}(I_{i}(\delta)\mathcal{L})^{*}\alpha)(x,s) &= \alpha\int_{\Omega}\phi_{i}(y)\partial_{2}k(s,\delta)\sum_{j=1}^{J}\lambda_{j}\phi_{j}(x)\phi_{j}(y)\mathrm{d}y-\alpha\int_{\Omega}\phi_{i}(y)k(s,\delta)\sum_{j=1}^{J}\lambda_{j}\phi_{j}(x)\nabla_{y}\cdot\left(a(y)\nabla_{y}\phi_{j}(y)\right)\mathrm{d}y \\
    &=\alpha\partial_{2}k(s,\delta)\sum_{j=1}^{J}\lambda_{j}\phi_{j}(x)M_{ji}+\alpha k(s,\delta)\sum_{j=1}^{J}\lambda_{j}\phi_{j}(x)A_{ji}
\end{align*}
where $M$ is the Galerkin mass matrix and $A$ is the Galerkin stiffness  matrix, i.e. the matrices with entries given by:
\begin{align}
    M_{ij}&:=\int_{\Omega}\phi_{i}(x)\phi_{j}(x)\mathrm{d}x \\
    A_{ij}&:=\int_{\Omega}a(x)\nabla\phi_{i}(x)\nabla\phi_{j}(x)\mathrm{d}x
\end{align}

Using this result we can deduce that:
\begin{equation}
    \label{vector_formula}
    (V_{0}\tilde{A}_{\delta}^{*}\boldsymbol{v})(x,s) = \partial_{2}k(s,\delta)\Phi(x)^{*}\Lambda M\boldsymbol{v} + k(s,\delta)\Phi(x)^{*}\Lambda A \boldsymbol{v}
\end{equation}
for any $\boldsymbol{v}\in\mathbb{R}^{J}$, where $\Lambda=\operatorname{diag}\{\lambda_{i}\}_{i=1}^{J}$.

We now move onto computing:
\begin{align*}
    \tilde{A}_{\delta}V_{0}\tilde{A}_{\delta}^{*}&=\mathcal{I}_{\delta}\mathcal{L}V_{0}\mathcal{L}^{*}\mathcal{I}_{\delta}^{*} \\
    &=\begin{pmatrix}
        I_{1}(\delta) \\
        \vdots \\
        I_{J}(\delta)
    \end{pmatrix}\mathcal{L}V_{0}\mathcal{L}^{*}\begin{pmatrix}
                                                    I_{1}(\delta)^{*} & \dots & I_{J}(\delta)^{*}
                                                \end{pmatrix}
\end{align*}
This operator has $ij$\textit{-th} entry which is given by:
\begin{align*}
    (\tilde{A}_{\delta}V_{0}\tilde{A}_{\delta}^{*})_{ij}\alpha &= I_{i}(\delta)\mathcal{L}V_{0}\mathcal{L}^{*}I_{j}(\delta)^{*}\alpha \\
    &= \int_{\Omega}\phi_{i}(x)\left[(\mathcal{L}V_{0}(I_{j}(\delta)\mathcal{L})^{*}\alpha)(x,\delta)\right]\mathrm{d}x \\
    &=\int_{\Omega}\phi_{i}(x)\bigg[\alpha\partial_{1}\partial_{2}k(\delta,\delta)\sum_{l=1}^{J}\lambda_{l}\phi_{l}(x)M_{lj}+\alpha\partial_{1}k(\delta,\delta)\sum_{l=1}^{J}\lambda_{l}\phi_{l}(x)A_{lj} \\
    &-\alpha\partial_{2}k(\delta,\delta)\sum_{l=1}^{J}\lambda_{l}M_{lj}\nabla\cdot(a(x)\nabla\phi_{l}(x)) - \alpha k(\delta,\delta)\sum_{l=1}^{J}\lambda_{l}A_{lj}\nabla\cdot(a(x)\nabla\phi_{l}(x))\bigg]\mathrm{d}x \\
    &=\alpha\partial_{1}\partial_{2}k(\delta,\delta)\sum_{l=1}^{J}\lambda_{l}M_{il}M_{lj} + \alpha\partial_{1}k(\delta,\delta)\sum_{l=1}^{J}\lambda_{l}M_{il}A_{lj} \\
    &+\alpha\partial_{2}k(\delta,\delta)\sum_{l=1}^{J}\lambda_{l}M_{lj}A_{il} + \alpha k(\delta,\delta)\sum_{l=1}^{J}\lambda_{l}A_{il}A_{lj} \\
    &= \alpha\partial_{1}\partial_{2}k(\delta,\delta)(M\Lambda M)_{ij} + \alpha\partial_{1}k(\delta,\delta)(M\Lambda A)_{ij} + \alpha\partial_{2}k(\delta,\delta)(A\Lambda M)_{ij} + \alpha k(\delta,\delta)(A\Lambda A)_{ij}
\end{align*}
We can thus conclude that $\tilde{A}_{\delta}V_{0}\tilde{A}_{\delta}^{*}$ is the $J\times J$ matrix given by:
\begin{equation}
    \tilde{A}_{\delta}V_{0}\tilde{A}_{\delta}^{*}=\partial_{1}\partial_{2}k(\delta,\delta)M\Lambda M + \partial_{1}k(\delta,\delta)M\Lambda A + \partial_{2}k(\delta,\delta)A\Lambda M +  k(\delta,\delta)A\Lambda A
\end{equation}

We now will choose a specific kernel $k(s,t)$ for the temporal part of our prior covariance. We will take $k(s,t):=st$. We thus have $\partial_{1}k(s,t)=t$, $\partial_{2}k(s,t)=s$, and $\partial_{1}\partial_{2}k(s,t)=1$. So we have:
\begin{align*}
    \tilde{A}_{\delta}V_{0}\tilde{A}_{\delta}^{*} &= M\Lambda M + \delta M\Lambda A + \delta A\Lambda M +  \delta^{2}A\Lambda A \\
    &= M\Lambda(M+\delta A)+\delta A \Lambda(M+\delta A) \\
    &= (M\Lambda+\delta A \Lambda)(M+\delta A) \\
    &= (M+\delta A)\Lambda(M+\delta A) = Q\Lambda Q
\end{align*}
where we have defined $Q:=M+\delta A$. We can now finish computing $m_{1}$. Evaluate it at $(x,t_{1})=(x,\delta)$:
\begin{equation*}
    m_{1}(x,\delta)=((1-V_{0}\tilde{A}_{\delta}^{*}(\tilde{A}_{\delta}V_{0}\tilde{A}_{\delta}^{*})^{-1}\tilde{A}_{\delta})m_{0})(x,\delta)+(V_{0}\tilde{A}_{\delta}^{*}(\tilde{A}_{\delta}V_{0}\tilde{A}_{\delta}^{*})^{-1}F^{1})(x,\delta)
\end{equation*}
We can now apply (\ref{vector_formula}) to compute the last term:
\begin{align*}
    (V_{0}\tilde{A}_{\delta}^{*}(\tilde{A}_{\delta}V_{0}\tilde{A}_{\delta}^{*})^{-1}F^{1})(x,\delta) &= (V_{0}\tilde{A}_{\delta}^{*}(Q\Lambda Q)^{-1}F^{1})(x,\delta) \\
    &=\partial_{2}k(\delta,\delta)\Phi(x)^{*}\Lambda M (Q\Lambda Q)^{-1}F^{1}+k(\delta,\delta)\Phi(x)^{*}\Lambda A (Q\Lambda Q)^{-1}F^{1} \\
    &= \delta\Phi(x)^{*}\Lambda M(Q\Lambda Q)^{-1}F^{1}+\delta^{2}\Phi(x)^{*}\Lambda A (Q\Lambda Q)^{-1}F^{1} \\
    &=\delta\Phi(x)^{*}\Lambda(M+\delta A)(Q\Lambda Q)^{-1}F^{1} \\
    &=\delta\Phi(x)^{*}\Lambda Q Q^{-1}\Lambda^{-1} Q^{-1}F^{1} \\
    &=\delta\Phi(x)^{*}Q^{-1}F^{1}
\end{align*}
In order to apply (\ref{vector_formula}) to compute the second term involving $m_{0}$ in the update rule we must first compute $\tilde{A}_{\delta}m_{0}=\mathcal{I}_{\delta}\mathcal{L}m_{0}$. To do this we compute:
\begin{align*}
    (\mathcal{L}m_{0})(x,t)&=\partial_{t}m_{0}-\nabla\cdot(a(x)\nabla m_{0}(x,t)) \\
    &=-\nabla(a(x)\nabla\Phi(x)^{*}\boldsymbol{\gamma}) \\
    &=-\sum_{j=1}^{J}\gamma_{j}\nabla\cdot(a(x)\nabla\phi_{j}(x))
\end{align*}
Thus, the $i$\textit{-th} entry of $\tilde{A}_{\delta}m_{0}$ can be computed as:
\begin{align*}
    (\tilde{A}_{\delta}m_{0})_{i} &= I_{i}(\delta)\mathcal{L}m_{0} \\
    &= \int_{\Omega}\phi_{i}(x)\left(-\sum_{j=1}^{J}\gamma_{j}\nabla\cdot(a(x)\nabla\phi_{j}(x))\right)\mathrm{d}x \\
    &= -\sum_{j=1}^{J}\gamma_{j}A_{ij} = (A\boldsymbol{\gamma})_{i}
\end{align*}
So $\tilde{A}_{\delta}m_{0}=A\boldsymbol{\gamma}$.
We can now apply (\ref{vector_formula}) to compute the second term involving $m_{0}$ as follows:
\begin{align*}
    ((V_{0}\tilde{A}_{\delta}^{*}(\tilde{A}_{\delta}V_{0}\tilde{A}_{\delta}^{*})^{-1}\tilde{A}_{\delta})m_{0})(x,\delta) &= (V_{0}\tilde{A}_{\delta}^{*}(Q\Lambda Q)^{-1}A\boldsymbol{\gamma})(x,\delta) \\
    &= \partial_{2}k(\delta,\delta)\Phi(x)^{*}\Lambda M (Q\Lambda Q)^{-1}A\boldsymbol{\gamma} + k(\delta,\delta)\Phi(x)^{*}\Lambda A(Q\Lambda Q)^{-1}A\boldsymbol{\gamma} \\
    &= \delta\Phi(x)^{*}\Lambda M (Q\Lambda Q)^{-1}A\boldsymbol{\gamma} + \delta^{2}\Phi(x)^{*}\Lambda A(Q\Lambda Q)^{-1}A\boldsymbol{\gamma} \\
    &= \delta\Phi(x)^{*}\Lambda (M + \delta A)(Q \Lambda Q)^{-1}A\boldsymbol{\gamma} \\
    &=\delta\Phi(x)^{*}\Lambda Q Q^{-1}\Lambda^{-1}Q^{-1}A\boldsymbol{\gamma} \\
    &=\delta\Phi(x)^{*}Q^{-1}A\boldsymbol{\gamma}
\end{align*}
Thus we can compute:
\begin{align*}
    (1-(V_{0}\tilde{A}_{\delta}^{*}(\tilde{A}_{\delta}V_{0}\tilde{A}_{\delta}^{*})^{-1}\tilde{A}_{\delta})m_{0})(x,\delta)&=\Phi(x)^{*}\boldsymbol{\gamma}-\delta\Phi(x)^{*}Q^{-1}A\boldsymbol{\gamma} \\
    &= \Phi(x)^{*}(I-\delta Q^{-1}A)\boldsymbol{\gamma} \\
    &= \Phi(x)^{*}Q^{-1}(Q-\delta A)\boldsymbol{\gamma} \\
    &= \Phi(x)^{*}Q^{-1}(M+\delta A- \delta A)\boldsymbol{\gamma} \\
    &= \Phi(x)^{*}Q^{-1}M\boldsymbol{\gamma}
\end{align*}

Putting this all together we obtain:
\begin{align*}
    m_{1}(x,t_{1}) = m_{1}(x,\delta) &= \Phi(x)^{*}Q^{-1}M\boldsymbol{\gamma} + \delta\Phi(x)^{*}Q^{-1}F^{1} \\
    &= \Phi(x)^{*}(Q^{-1}M\boldsymbol{\gamma}+\delta Q^{-1}F^{1}) \\
    &= \Phi(x)^{*}((M+\delta A)^{-1}M\boldsymbol{\gamma}+\delta (M+\delta A)^{-1}F^{1})
\end{align*}
Thus we see that performing this mean update and then evaluating at the time $t_{1}=\delta$ we obtain that the coefficients of the $\{\phi_{i}(x)\}_{i=1}^{J}$ changes as follows:
\begin{equation}
    \boldsymbol{\gamma} \longmapsto (M+\delta A)^{-1}M\boldsymbol{\gamma}+\delta (M+\delta A)^{-1}F^{1}
\end{equation}
just like in the update equation for the backward-Euler Galerkin method. \vspace{10pt}
\begin{remark}
    Note that evaluating $m_{1}$ at $(x,s)$ instead of at $(x,\delta)$ yields the following:
    \begin{equation}
        m_{1}(x,s)=\Phi(x)^{*}[(M+\delta A)^{-1}(M+(\delta-s)A)\boldsymbol{\gamma}+s (M+\delta A)^{-1}F^{1}]
    \end{equation}
\end{remark}

We can now move on to computing the covariance $V_{1}$. We start by computing $\tilde{A}_{\delta}V_{0}$. Computing this involves determining how $I_{j}(\delta)\mathcal{L}V_{0}$ acts on functions $g$ for $j=1,\dots,J$. We have:
\begin{equation*}
    I_{j}(\delta)\mathcal{L}V_{0}g=\int_{\Omega}\phi_{j}(x)(\mathcal{L}V_{0}g)(x,\delta)\mathrm{d}x
\end{equation*}

Now recalling that $V_{0}g(x,\delta)=\int_{\Omega}\int_{0}^{T}k_{ys}^{x\delta}g(y,s)\mathrm{d}s\mathrm{d}y$ we deduce:
\begin{align*}
    (\mathcal{L}V_{0}g)(x,\delta) &= \int_{\Omega}\int_{0}^{T}(\mathcal{L}k_{ys})(x,\delta)g(y,s)\mathrm{d}s\mathrm{d}y \\
    &= \int_{\Omega}\int_{0}^{T}\left(\partial_{1}k(\delta,s)\sum_{i=1}^{J}\lambda_{i}\phi_{i}(x)\phi_{i}(y)-k(\delta,s)\sum_{i=1}^{J}\lambda_{i}\nabla_{x}\cdot(a(x)\nabla_{x}\phi_{i}(x))\phi_{i}(y)\right)g(y,s)\mathrm{d}s\mathrm{d}y
\end{align*}

We can now perform the integration to obtain:
\begin{align*}
    I_{j}(\delta)\mathcal{L}V_{0}g &= \int_{\Omega}\phi_{j}(x)\left(\int_{\Omega}\int_{0}^{T}\left(\partial_{1}k(\delta,s)\sum_{i=1}^{J}\lambda_{i}\phi_{i}(x)\phi_{i}(y)-k(\delta,s)\sum_{i=1}^{J}\lambda_{i}\nabla_{x}\cdot(a(x)\nabla_{x}\phi_{i}(x))\phi_{i}(y)\right)g(y,s)\mathrm{d}s\mathrm{d}y\right)\mathrm{d}x \\
    &= \int_{\Omega}\int_{0}^{T}\left(\partial_{1}k(\delta,s)\sum_{i=1}^{J}\lambda_{i}M_{ij}\phi_{i}(y)g(y,s)+k(\delta,s)\sum_{i=1}^{J}\lambda_{i}A_{ij}\phi_{i}(y)g(y,s)\right)\mathrm{d}s\mathrm{d}y \\
    &= \sum_{i=1}^{J}\lambda_{i}\int_{\Omega}\int_{0}^{T}(\partial_{1}k(\delta,s)M_{ij}+k(\delta,s)A_{ij})\phi_{i}(y)g(y,s)\mathrm{d}s\mathrm{d}y \\
    &= \sum_{i=1}^{J}\lambda_{i}\left[\int_{0}^{T}M_{ij}\partial_{1}k(\delta,s)(I_{i}(s)g)\mathrm{d}s+\int_{0}^{T}A_{ij}k(\delta,s)(I_{i}(s)g)\mathrm{d}s\right] \\
    &= \sum_{i=1}^{J}\lambda_{i}\left[M_{ij}\left(\int_{0}^{T}\partial_{1}k(\delta,s)(\mathcal{I}_{s}g)\mathrm{d}s\right)_{i}+A_{ij}\left(\int_{0}^{T}k(\delta,s)(\mathcal{I}_{s}g)\mathrm{d}s\right)_{i}\right] \\
    &= \left(M\Lambda\int_{0}^{T}\partial_{1}k(\delta,s)(\mathcal{I}_{s}g)\mathrm{d}s+A\Lambda\int_{0}^{T}k(\delta,s)(\mathcal{I}_{s}g)\mathrm{d}s\right)_{j}
\end{align*}

Thus we can deduce:
\begin{equation}
    \tilde{A}_{\delta}V_{0}g=M\Lambda\int_{0}^{T}\partial_{1}k(\delta,s)(\mathcal{I}_{s}g)\mathrm{d}s + A\Lambda\int_{0}^{T}k(\delta,s)(\mathcal{I}_{s}g)\mathrm{d}s
\end{equation}

Having worked this out we can now compute the second term in the formula for $V_{1}$ by utilising (\ref{vector_formula}) as follows:
\begin{align*}
    (V_{0}\tilde{A}_{\delta}^{*}(\tilde{A}_{\delta}V_{o}\tilde{A}_{\delta}^{*})^{-1}\tilde{A}_{\delta}V_{0}g)(x,t) &= \partial_{2}k(t,\delta)\Phi(x)^{*}\Lambda M (Q\Lambda Q)^{-1}\left[M\Lambda\int_{0}^{T}\partial_{1}k(\delta,s)(\mathcal{I}_{s}g)\mathrm{d}s + A\Lambda\int_{0}^{T}k(\delta,s)(\mathcal{I}_{s}g)\mathrm{d}s\right] \\
    &+ k(t,\delta)\Phi(x)^{*}\Lambda A (Q\Lambda Q)^{-1} \left[M\Lambda\int_{0}^{T}\partial_{1}k(\delta,s)(\mathcal{I}_{s}g)\mathrm{d}s + A\Lambda\int_{0}^{T}k(\delta,s)(\mathcal{I}_{s}g)\mathrm{d}s\right]
\end{align*}

To proceed we now utilise the specific choice of $k(s,t)$ to work out:
\begin{align*}
    \int_{0}^{T}k(\delta,s)(\mathcal{I}_{s}g)\mathrm{d}s &= \delta\int_{0}^{T}s(\mathcal{I}_{s}g)\mathrm{d}s = \delta\boldsymbol{\nu}_{g} \\
    \int_{0}^{T}\partial_{1}k(\delta,s)(\mathcal{I}_{s}g) &= \int_{0}^{T}s(\mathcal{I}_{s}g)\mathrm{d}s=\boldsymbol{\nu}_{g}
\end{align*}
where $\boldsymbol{\nu}g:=\int_{0}^{T}s(\mathcal{I}_{s}g)\mathrm{d}s$.
Thus we have:
\begin{align*}
    M\Lambda\int_{0}^{T}\partial_{1}k(\delta,s)(\mathcal{I}_{s}g)\mathrm{d}s + A\Lambda\int_{0}^{T}k(\delta,s)(\mathcal{I}_{s}g)\mathrm{d}s
    &= M\Lambda\boldsymbol{\nu}_{g}+\delta A\Lambda\boldsymbol{\nu}_{g} \\
    &=(M+\delta A)\Lambda\boldsymbol{\nu}_{g} \\
    &=Q\Lambda\boldsymbol{\nu}_{g}
\end{align*}

We can now finish the computation of the second term of $V_{1}$:
\begin{align*}
    (V_{0}\tilde{A}_{\delta}^{*}(\tilde{A}_{\delta}V_{o}\tilde{A}_{\delta}^{*})^{-1}\tilde{A}_{\delta}V_{0}g)(x,t) &= t\Phi(x)^{*}\Lambda M(Q\Lambda Q)^{-1}Q\Lambda\boldsymbol{\nu}_{g} + t\delta\Phi(x)^{*}\Lambda A(Q\Lambda Q)^{-1}Q\Lambda\boldsymbol{\nu}_{g} \\
    &= t\Phi(x)^{*}\Lambda(M+\delta A)(Q\Lambda Q)^{-1}Q\Lambda\boldsymbol{\nu}_{g} \\
    &= t\Phi(x)^{*}\Lambda QQ^{-1}\Lambda^{-1}Q^{-1}Q\Lambda\boldsymbol{\nu}_{g} \\
    &= t\Phi(x)^{*}\Lambda\boldsymbol{\nu}_{g} \\
    &= \sum_{i=1}^{J}t\lambda_{i}\phi_{i}(x)(\boldsymbol{\nu}_{g})_{i} \\
    &= \sum_{i=1}^{J}\lambda_{i}t\phi_{i}(x)\int_{0}^{T}s(I_{i}(s)g)\mathrm{d}s \\
    &= \int_{\Omega}\int_{0}^{T}\sum_{i=1}^{J}\lambda_{i}\phi_{i}(x)\phi_{i}(y)tsg(y,s)\mathrm{d}s\mathrm{d}y \\
    &= \int_{\Omega}\int_{0}^{T}\sum_{i=1}^{J}\lambda_{i}\phi_{i}(x)\phi_{i}(y)k(t,s)g(y,s)\mathrm{d}s\mathrm{d}y = (V_{0}g)(x,t)
\end{align*}
Thus, we can conclude: $V_{0}\tilde{A}_{\delta}^{*}(\tilde{A}_{\delta}V_{o}\tilde{A}_{\delta}^{*})^{-1}\tilde{A}_{\delta}V_{0}=V_{0}$ and so:
\begin{equation}
    V_{1}=V_{0}-V_{0}\tilde{A}_{\delta}^{*}(\tilde{A}_{\delta}V_{o}\tilde{A}_{\delta}^{*})^{-1}\tilde{A}_{\delta}V_{0} = 0
\end{equation}

Thus, for a fixed realisation of $f$ we have:
\begin{equation}
    \label{posterior_1}
    u|\{\tilde{A}_{\delta}u=F^{1},f\}\sim\mathcal{N}(m_{1},V_{1})
\end{equation}
where the function $m_{1}$ is given by:
\begin{equation}
    m_{1}(x,s)=\Phi(x)^{*}[(M+\delta A)^{-1}(M+(\delta-s)A)\boldsymbol{\gamma}+s (M+\delta A)^{-1}F^{1}]
\end{equation}
and where:
\begin{equation*}
    V_{1}=0
\end{equation*}

Thus if we evaluate the posterior (\ref{posterior_1}) at the time $t_{1}=\delta$ we will obtain a point mass at the spatial function
$m_{1}(x,\delta)=\Phi(x)^{*}[(M+\delta A)^{-1}M\boldsymbol{\gamma}+\delta (M+\delta A)^{-1}F^{1}]$ just like the backward-Euler Galerkin method yields.

We now have the following distribution (for a fixed realisation of $f$):
\begin{equation}
    \label{dist_time_1_fixed_f}
    u|\{\tilde{A}_{\delta}u=F^{1},f\}\sim\mathcal{N}(m_1,0)=\delta_{m_{1}}
\end{equation}

We will now marginalise (\ref{dist_time_1_fixed_f}) over $f$ in order to obtain the averaged conditional distribution. To do this we will need the following Lemma (which we prove below): \vspace{10pt}
\begin{lemma}
    Let $f\sim\mathcal{N}(\bar{f},K)$ where we assume that this Gaussian measure is on a Hilbert space of functions $\mathcal{H}_{1}\subset\mathbb{R}^{\mathcal{X}}$. Suppose that for a fixed realisation of $f$ we have
    \begin{equation*}
        y|f\sim\mathcal{N}(Lf+c,0)=\delta_{Lf}
    \end{equation*}
    where $L$ is a bounded linear operator from $\mathcal{H}_{1}$ to another Hilbert space $\mathcal{H}_{2}\subset\mathbb{R}^{\mathcal{X}}$ (so $y$ lies in $\mathcal{H}_{2}$) and where $c$ is a deterministic function. Then marginalizing over $f$ yields:
    \begin{equation*}
        y\sim\mathcal{N}(L\bar{f}+c,LKL^{*})
    \end{equation*}
    as the averaged distribution of $y$.
\end{lemma}
% \noindent \textit{Proof:} To determine this marginal distribution we must compute the expectation of an arbitrary bounded cylindrical function, i.e. we must compute:
% \begin{equation*}
%     \int\int\psi(y^{N})\delta_{\mathcal{L}f}(\mathrm{d}y)\mu_{\bar{f},K}(\mathrm{d}f)
% \end{equation*}
% where $y^{N}=Py$ and $P:\mathcal{H}_{2}\rightarrow\mathbb{R}^{N}$ is the bounded linear operator given by $Ph:=(h(x_1),\dots,h(x_N))^{T}$ where the $x_{i}\in\mathcal{X}$ for $i=1,\dots,N$. Note that $u^{N}|\{f\}\sim\delta_{P\mathcal{L}f}$. We also have that $Y:=P\mathcal{L}f\sim\mathcal{N}(P\mathcal{L}\bar{f},P\mathcal{L}K\mathcal{L}^{*}P^{*})$. Using these facts we can write:
% \begin{align*}
%     \int\int\psi(y^{N})\delta_{\mathcal{L}f}(\mathrm{d}y)\mu_{\bar{f},K}(\mathrm{d}f) &= \int\int\psi(y^N)\delta_{P\mathcal{L}f}(dy^{N})\mu_{\bar{f},K}(\mathrm{d}f) \\
%     &= \int\int\psi(y^{N})\delta_{Y}(dy^N)\mu_{\bar{f},K}(\mathrm{d}f) \\
%     &=\int\int\psi(y^{N})\delta_{Y}(dy^N)\mu_{P\mathcal{L}\bar{f},P\mathcal{L}K\mathcal{L}^{*}P^{*}}(\mathrm{d}Y) \\
%     &=\int\psi(y^{N})\left(\int\delta_{Y}(dy^N)\mu_{P\mathcal{L}\bar{f},P\mathcal{L}K\mathcal{L}^{*}P^{*}}(\mathrm{d}Y)\right) \\
%     &=\int\psi(y^{N})\mu_{P\mathcal{L}\bar{f},P\mathcal{L}K\mathcal{L}^{*}P^{*}}(\mathrm{d}y^{N})
% \end{align*}
% We thus obtain the expectation of $\psi(y^N)$ with respect to the multivariate Gaussian distribution:
% \begin{equation*}
%     \mathcal{N}(P\mathcal{L}\bar{f},P\mathcal{L}K\mathcal{L}^{*}P^{*})
% \end{equation*}
% We thus have:
% \begin{equation*}
%     y^{N}=Py\sim\mathcal{N}(P\mathcal{L}\bar{f},P\mathcal{L}K\mathcal{L}^{*}P^{*})
% \end{equation*}
% From this we can conclude that $y$ is distributed as $\mathcal{N}(\mathcal{L}\bar{f},\mathcal{L}K\mathcal{L}^{*})$. Indeed, we have that $\mathcal{L}\bar{f}\in\mathcal{H}_{2}$ is in the right space and also $\mathcal{L}K\mathcal{L}^{*}$ is a trace-class operator from $\mathcal{H}_{2}\rightarrow\mathcal{H}_{2}$ (since $\mathcal{L}$ and its adjoint are both bounded linear operators and $K$ is trace-class). We thus have that $\mathcal{N}(\mathcal{L}\bar{f},\mathcal{L}K\mathcal{L}^{*})$ is a valid Gaussian distribution. To show that we indeed have this as the distribution of $y$ we can proceed by contradiction: suppose that $y\sim\mathcal{N}(m,V)$.
\noindent\textit{Proof:} The fact that $y|f\sim\mathcal{N}(Lf+c,0)$ is equivalent to saying that:
\begin{equation*}
    y=Lf+c+\tilde{y}
\end{equation*}
where $\tilde{y}\sim\mathcal{N}(0,0)(=\delta_{0})$ is independent of $f$. Thus we have:
\begin{equation*}
    \begin{pmatrix}
        f \\
        \tilde{y}
    \end{pmatrix}=\mathcal{N}\left(\begin{pmatrix}
                                \bar{f} \\
                                0
                            \end{pmatrix},\begin{pmatrix}
                                            K & 0 \\
                                            0 & 0
                                          \end{pmatrix}\right)
\end{equation*}
Since we can write:
\begin{equation*}
    y=\begin{pmatrix}
        L & 1
    \end{pmatrix}\begin{pmatrix}
                    f \\
                    \tilde{y}
                 \end{pmatrix}+c
\end{equation*}
we deduce:
\begin{equation*}
    y\sim\mathcal{N}\left(L\bar{f}+c, \begin{pmatrix}
        L & 1
    \end{pmatrix}\begin{pmatrix}
                    K & 0 \\
                    0 & 0
                \end{pmatrix}\begin{pmatrix}
                    L^{*} \\
                    1
                \end{pmatrix}\right)=\mathcal{N}(L\bar{f}+c,LKL^{*})
\end{equation*}
as required. \qedsymbol

We can now perform the marginalisation over $f$ in (\ref{dist_time_1_fixed_f}). We first rewrite $m_1$ as follows:
\begin{align*}
    m_{1}(x,s) &= \Phi(x)^{*}[Q^{-1}(M+(\delta-s)A)\boldsymbol{\gamma}+sQ^{-1}F^{1}] \\
    &= \Phi(x)^{*}Q^{-1}(M+(\delta-s)A)\boldsymbol{\gamma}+s\Phi(x)^{*}Q^{-1}F^{1} \\
    &=c(x,s) + s\Phi(x)^{*}Q^{-1}\mathcal{I}_{\delta}f
\end{align*}
where $c(x,s):=\Phi(x)^{*}Q^{-1}(M+(\delta-s)A)\boldsymbol{\gamma}$ Thus, we have:
\begin{equation*}
    m_1=\Psi^{*}Q^{-1}\mathcal{I}_{\delta}f+c=Lf+c
\end{equation*}
where $\Psi^{*}:\mathbb{R}^{J}\rightarrow L^{2}(\Omega\times[0,T])$ is the linear operator given by $(\Psi^{*}\boldsymbol{v})(x,s):=s\Phi(x)^{*}\boldsymbol{v}$ and $L:=\Psi^{*}Q^{-1}\mathcal{I}_{\delta}$. Now both $\mathcal{I}_{\delta}$ and $Q^{-1}$ are bounded linear operators. $L$ will thus be a bounded linear operator provided $\Psi^{*}$ is bounded. We check this:
\begin{align*}
    \|\Psi^{*}\boldsymbol{v}\|^{2} &= \int_{\Omega}\int_{0}^{T}|(\Psi^{*}\boldsymbol{v})(y,s)|^{2}\mathrm{d}s\mathrm{d}y \\
    &=\int_{\Omega}\int_{0}^{T}s^{2}|\Phi(y)^{*}\boldsymbol{v}|^{2}\mathrm{d}s\mathrm{d}y \\
    &\leq T^{2}\int_{\Omega}\left|\sum_{i=1}^{J}\phi_{i}(y)v_{i}\right|^{2}\mathrm{d}y \\
    &\leq T^{2}\int_{\Omega}\sum_{i=1}^{J}|\phi_{i}(y)|^{2}\sum_{j=1}^{J}|v_{j}|^{2}\mathrm{d}y \\
    &= T^{2}\|\boldsymbol{v}\|^{2}\sum_{i=1}^{J}\|\phi_{i}\|^{2}
\end{align*}
and so $\|\Psi^{*}\boldsymbol{v}\|\leq T\left(\sum_{i=1}^{J}\|\phi_{i}\|^{2}\right)^{1/2}\|\boldsymbol{v}\|$. This implies: $\|\Psi^{*}\|_{\infty}\leq T\left(\sum_{i=1}^{J}\|\phi_{i}\|^{2}\right)<\infty$ and so $\Psi^{*}$ is a bounded linear operator. We can thus use the Lemma given above to conclude that the marginalised distribution is:
\begin{equation*}
    \int u|\{\tilde{A}_{\delta}u=F^{1},f\}\mathrm{d}f\sim\mathcal{N}(L\bar{f}+c,LKL^{*})=\mathcal{N}(\bar{m}_{1},\bar{V}_{1})
\end{equation*}
where we have denoted $\bar{m}_1:=L\bar{f}+c$ and $\bar{V}_{1}:=LKL^{*}$. The mean is thus the following function:
\begin{equation*}
    \bar{m}_{1}(x,s)=\Phi(x)^{*}[Q^{-1}(M+(\delta-s)A)\boldsymbol{\gamma}+sQ^{-1}\bar{F}^{1}]
\end{equation*}
where $\bar{F}^{1}:=\mathcal{I}_{\delta}\bar{f}$. The covariance operator is given by:
\begin{align*}
        \bar{V}_{1} &= \Psi^{*}Q^{-1}\mathcal{I}_{\delta}K\mathcal{I}_{\delta}^{*}Q^{-1}\Psi \\
        &= \Psi^{*}H^{(1)}\Psi
\end{align*}
where $H^{(1)}:=Q^{-1}\mathcal{I}_{\delta}K\mathcal{I}_{\delta}^{*}Q^{-1}$ and $\Psi$ is the adjoint of $\Psi^{*}$ and which takes a function $g$ and gives the following vector: $\Psi g=\int_{0}^{T}s(\mathcal{I}_{s}g)\mathrm{d}s$. Thus, $\bar{V}_{1}$ acts on functions via:
\begin{equation*}
    (\bar{V}_{1}g)(x,t)=\int_{\Omega}\int_{0}^{T}\left(ts\sum_{i=1}^{J}\sum_{j=1}^{J}H^{(1)}_{ij}\phi_{i}(x)\phi_{j}(y)\right)g(y,s)\mathrm{d}s\mathrm{d}y
\end{equation*}

My idea was to continue with this distribution $\mathcal{N}(\bar{m}_1,\bar{V}_1)$ as the new prior and to condition on $A_{2\delta}u=F^{2}$ where $F^2:=\mathcal{I}_{2\delta}f$. This doesn't produce the Backward Euler-Galerkin since the mean function $\bar{m}_1$ is no longer constant in time and also applying $\mathcal{I}_{2\delta}$ will make a matrix $(M+2\delta A)$ appear in the update equation instead of $Q=(M+\delta A)$.

\end{document}
